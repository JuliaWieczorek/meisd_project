{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from deep_translator import GoogleTranslator\n",
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "MAX_LEN = 30\n",
    "#MAX_LEN = min(30, max(df_train['Utterances'].str.split().apply(len).max(), 50))\n",
    "BATCH_SIZE = 16\n",
    "# TRAIN_BATCH_SIZE = 16\n",
    "# VALID_BATCH_SIZE = 16\n",
    "# TEST_BATCH_SIZE = 16\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001 #1e-05\n",
    "THRESHOLD = 0.2 # threshold for the sigmoid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "904ac53922491685",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('MEISD/MEISD_text.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "191da412cb222efc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be38c4ec0e98b8b0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5387eb99bd8c623d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_data_integrity(df):\n",
    "    print(\"=== Weryfikacja integralności danych ===\")\n",
    "\n",
    "    print(\"\\n>>> Sprawdzanie brakujących wartości:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "    print(\"\\n>>> Typy danych:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\n>>> Liczba duplikatów:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Liczba zduplikowanych wierszy: {duplicates}\")\n",
    "\n",
    "    print(\"\\n>>> Zakres wartości w kolumnach liczbowych:\")\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        print(f\"{col}: min = {df[col].min()}, max = {df[col].max()}\")\n",
    "\n",
    "    print(\"\\n>>> Unikalne wartości w kolumnach kluczowych:\")\n",
    "    for col in ['emotion', 'emotion2', 'emotion3']:\n",
    "        if col in df.columns:\n",
    "            print(f\"{col}: {df[col].unique()}\")\n",
    "\n",
    "    print(\"\\nWeryfikacja zakończona.\\n\")\n",
    "\n",
    "\n",
    "# Funkcja do weryfikacji kompletności danych\n",
    "def verify_data_integrity(data):\n",
    "    issues = []\n",
    "\n",
    "    if 'combined_emotions' not in data.columns:\n",
    "        print(\"Column 'combined_emotions' is missing!\")\n",
    "        return\n",
    "\n",
    "    for idx, entry in data.iterrows():\n",
    "        combined_emotions = entry['combined_emotions']\n",
    "\n",
    "        if not combined_emotions:  \n",
    "            issues.append(f\"Entry {idx} has an empty 'combined_emotions' list.\")\n",
    "        elif not isinstance(combined_emotions, list):  \n",
    "            issues.append(f\"Entry {idx} has 'combined_emotions' which is not a list.\")\n",
    "\n",
    "    if issues:\n",
    "        print(\"Data integrity check failed:\")\n",
    "        for issue in issues:\n",
    "            print(issue)\n",
    "    else:\n",
    "        print(\"All entries in 'combined_emotions' are valid!\")\n",
    "\n",
    "check_data_integrity(df_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d502e133144d6890",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "unique_emotions = set(df_data['emotion'].unique()) | set(df_data['emotion2'].unique()) | set(df_data['emotion3'].unique())\n",
    "print(unique_emotions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e9f6b6ecff771d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "emotion_map = {\n",
    "    'neutral': 0, 'Neutral': 0, 'Neutral ': 0, 'neutral ': 0,\n",
    "    'acceptance': 1, 'acceptance ': 1,\n",
    "    'disgust': 2, 'Disgust': 2, ' disgust': 2, '  disgust': 2, 'digust': 2,\n",
    "    'surprise': 3, 'Surprise': 3, ' surprise': 3, 'Surprise ': 3, 'sur': 3,\n",
    "    'joy': 4, 'Joy': 4, 'like': 4,\n",
    "    'sadness': 5, 'Sadness': 5, ' sadness': 5, 'Sadness ': 5, 'sadness ': 5, 'sadnes': 5, 'asadness': 5,\n",
    "    'anger': 6, 'Anger': 6, ' anger': 6, 'ANGER': 6, 'anger1': 6, 'an': 6,\n",
    "    'fear': 8, 'Fear': 8, 'fear ': 8, 'Fear ': 8, 'faer': 8, 'Fera': 8,\n",
    "    'l': None, np.nan: None\n",
    "}\n",
    "\n",
    "data_emotion = pd.DataFrame()\n",
    "data_emotion['Utterances'] = df_data['Utterances']\n",
    "\n",
    "data_emotion['target1'] = df_data['emotion'].map(emotion_map)\n",
    "data_emotion['target2'] = df_data['emotion2'].map(emotion_map)\n",
    "data_emotion['target3'] = df_data['emotion3'].map(emotion_map)\n",
    "\n",
    "\n",
    "def fill_nans(row):\n",
    "    if row.isna().sum() == 2 and row.count() == 1:\n",
    "        row = row.fillna(row.dropna().iloc[0])\n",
    "\n",
    "    elif row.isna().sum() == 1 and row.count() == 2:\n",
    "        row = row.fillna(row.dropna().iloc[0])\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "data_emotion[['target1', 'target2', 'target3']] = data_emotion[['target1', 'target2', 'target3']].apply(fill_nans, axis=1)\n",
    "\n",
    "data_emotion['target1'] = data_emotion['target1'].astype(int)\n",
    "data_emotion['target2'] = data_emotion['target2'].astype(int)\n",
    "data_emotion['target3'] = data_emotion['target3'].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a8aaa8b880c6ccb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_emotion.head(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "332b2987537a0e76",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_emotion['combined_emotions'] = data_emotion[['target1', 'target2', 'target3']].apply(lambda x: x.dropna().unique().astype(int).tolist(), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "270a8a1cc8fd661a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "verify_data_integrity(data_emotion)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "753d4c967550c0ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "mlb_emotion = MultiLabelBinarizer()\n",
    "emotion_binarized = mlb_emotion.fit_transform(data_emotion['combined_emotions'])\n",
    "emotion_df = pd.DataFrame(emotion_binarized, columns=[f'emotion_{i+1}' for i in range(emotion_binarized.shape[1])])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4272a0bab8f57fd3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "check_data_integrity(emotion_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38d0ae96bdbe486c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multi_label_binarizer_MEISD = pd.concat([data_emotion['Utterances'], emotion_df], axis=1)\n",
    "multi_label_binarizer_MEISD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "754405fbae26709b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5c567cd5cc6cb2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1. Synonym Replacement\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words[:]\n",
    "    num_replacements = max(1, len(words) // 5)  # Replace about 20% of words\n",
    "    random_words = random.sample(words, num_replacements)\n",
    "\n",
    "    for word in random_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            #synonym = synonyms[0].lemmas()[0].name()  # Take first synonym\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            if synonym != word:  # Avoid replacement if the synonym is identical\n",
    "                new_words = [synonym if w == word else w for w in new_words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "# 2. Random Insertion\n",
    "def random_insertion(text, n=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        new_word = random.choice(words)\n",
    "        insert_pos = random.randint(0, len(words))\n",
    "        words.insert(insert_pos, new_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# 3. Random Deletion\n",
    "def random_deletion(text, p=0.3):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text  # Avoid deleting single-word text\n",
    "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    if not new_words:\n",
    "        return random.choice(words)  # Return one word if all words are deleted\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# 4. Back Translation\n",
    "def back_translation(text, src_lang='en', mid_lang='fr', max_retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=src_lang, target=mid_lang).translate(text)\n",
    "            back_translated = GoogleTranslator(source=mid_lang, target=src_lang).translate(translated)\n",
    "            return back_translated\n",
    "        except Exception as e:\n",
    "            print(f\"Back translation error on attempt {attempt + 1}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(1)\n",
    "    raise ValueError(\"Back translation failed\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e09026fb2c262d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_text(text, num_augments=2):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        augmentation_choice = random.choice(['synonym', 'insertion', 'deletion', 'back_translation'])\n",
    "        if augmentation_choice == 'synonym':\n",
    "            augmented_texts.append(synonym_replacement(text))\n",
    "        elif augmentation_choice == 'insertion':\n",
    "            augmented_texts.append(random_insertion(text))\n",
    "        elif augmentation_choice == 'deletion':\n",
    "            augmented_texts.append(random_deletion(text))\n",
    "        elif augmentation_choice == 'back_translation':\n",
    "            augmented_texts.append(back_translation(text))\n",
    "    return augmented_texts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828baedf8d3f7533",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_sums = multi_label_binarizer_MEISD.iloc[:, 1:].sum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_sums.plot(kind='bar', color='skyblue')\n",
    "plt.title('Rozkład klas (emocji)')\n",
    "plt.xlabel('Emocje')\n",
    "plt.ylabel('Liczba próbek')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c1f4e7f58136cf2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split into train and test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e99bc1455429e485"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train, df_temp = train_test_split(multi_label_binarizer_MEISD, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_temp, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f53fda54f02119db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7725fb6a329484a6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_train.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e72544c7a36c6f62",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = multi_label_binarizer_MEISD.columns.tolist()[1:]\n",
    "multi_label_binarizer_MEISD[LABEL_COLUMNS].sum().sort_values().plot(kind=\"barh\");"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69754a4785d3b2c2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_counts = df_train[['emotion_1', 'emotion_2', 'emotion_3', 'emotion_4',\n",
    "                         'emotion_5', 'emotion_6', 'emotion_7', 'emotion_8']].sum(axis=0)\n",
    "\n",
    "max_count = class_counts.max()\n",
    "class_counts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5b582ff5f60ec39",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_balance(class_counts):\n",
    "    \"\"\"\n",
    "    Analizuje balans w danych na podstawie liczby próbek w każdej klasie.\n",
    "\n",
    "    Args:\n",
    "        class_counts (list or dict): Liczność próbek w klasach. Może być listą liczb\n",
    "                                     lub słownikiem {nazwa_klasy: liczba_próbek}.\n",
    "    Returns:\n",
    "        dict: Słownik z obliczonymi miarami balansu:\n",
    "              - Rozpiętość (range)\n",
    "              - Odchylenie standardowe (std)\n",
    "              - Średnia liczba próbek (mean)\n",
    "              - Największa klasa (max_class)\n",
    "              - Najmniejsza klasa (min_class)\n",
    "              - Interpretacja (interpretation)\n",
    "    \"\"\"\n",
    "    if isinstance(class_counts, dict):\n",
    "        class_counts = list(class_counts.values())\n",
    "\n",
    "    max_count = max(class_counts)\n",
    "    min_count = min(class_counts)\n",
    "    mean_count = np.mean(class_counts)\n",
    "    std_count = np.std(class_counts)\n",
    "    range_count = max_count - min_count\n",
    "\n",
    "    # Interpretacja balansu\n",
    "    if std_count < 0.1 * mean_count:\n",
    "        interpretation = \"Dane są bardzo dobrze zbalansowane.\"\n",
    "    elif std_count < 0.3 * mean_count:\n",
    "        interpretation = \"Dane są umiarkowanie zbalansowane.\"\n",
    "    else:\n",
    "        interpretation = \"Dane są niezbalansowane.\"\n",
    "\n",
    "    return {\n",
    "        \"range\": range_count,\n",
    "        \"std\": std_count,\n",
    "        \"mean\": mean_count,\n",
    "        \"max_class\": max_count,\n",
    "        \"min_class\": min_count,\n",
    "        \"interpretation\": interpretation\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f61871ea5bdbb49",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "check_balance(class_counts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aead14f020392fa3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def augment_multilabel_data(df, label_columns, augment_text, num_augments=2):\n",
    "    \"\"\"\n",
    "    Augments multilabel data to balance class distributions, ignoring samples with the most frequent class.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'Utterances' column and label columns.\n",
    "    - label_columns (list): List of column names corresponding to binary label columns.\n",
    "    - augment_text (callable): Function to augment text. Should take a string and return a list of augmented strings.\n",
    "    - num_augments (int): Number of augmented samples to generate per original sample.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Augmented DataFrame with balanced class distributions.\n",
    "    \"\"\"\n",
    "    # Calculate class counts\n",
    "    class_counts = {label: df[label].sum() for label in label_columns}\n",
    "    max_count = max(class_counts.values())\n",
    "    most_frequent_class = max(class_counts, key=class_counts.get)\n",
    "\n",
    "    print(f\"Class counts: {class_counts}\")\n",
    "    print(f\"Maximum class count: {max_count}\")\n",
    "    print(f\"Most frequent class: {most_frequent_class} (ignoring samples with this label)\")\n",
    "\n",
    "    # Initialize storage for augmented data\n",
    "    augmented_data = {'Utterances': [], **{label: [] for label in label_columns}}\n",
    "\n",
    "    for label in label_columns:\n",
    "        if label == most_frequent_class:\n",
    "            print(f\"Skipping augmentation for the most frequent class '{label}'.\")\n",
    "            continue  # Skip augmenting the most frequent class\n",
    "\n",
    "        # Select rows where the current label is 1\n",
    "        class_subset = df[df[label] == 1]\n",
    "        augmented_data['Utterances'].extend(class_subset['Utterances'])\n",
    "        for lbl in label_columns:\n",
    "            augmented_data[lbl].extend(class_subset[lbl])\n",
    "\n",
    "        # Calculate how many samples to augment\n",
    "        num_to_augment = max_count - len(class_subset)\n",
    "\n",
    "        if num_to_augment > 0:\n",
    "            print(f\"Augmenting class '{label}' with {num_to_augment} samples.\")\n",
    "            for _, row in tqdm(class_subset.iterrows(), total=len(class_subset), desc=f\"Augmenting {label}\"):\n",
    "                if num_to_augment <= 0:\n",
    "                    break\n",
    "\n",
    "                # Perform text augmentation\n",
    "                new_texts = augment_text(row['Utterances'], num_augments=num_augments)\n",
    "                for new_text in new_texts:\n",
    "                    if num_to_augment <= 0:\n",
    "                        break\n",
    "                    augmented_data['Utterances'].append(new_text)\n",
    "\n",
    "                    # Add the multilabels for the augmented data\n",
    "                    for lbl in label_columns:\n",
    "                        augmented_data[lbl].append(row[lbl])\n",
    "\n",
    "                    num_to_augment -= 1\n",
    "\n",
    "    # Create a new DataFrame with augmented data\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ba48dafa7fdbdd8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ## Augmentation\n",
    "# augmented_df = augment_multilabel_data(\n",
    "#     df=df_train,\n",
    "#     label_columns=[f\"emotion_{i}\" for i in range(1, 9)],\n",
    "#     augment_text=augment_text,\n",
    "#     num_augments=2\n",
    "# )\n",
    "# \n",
    "# print(augmented_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a58c80b7d9f9328",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class_counts = augmented_df[['emotion_1', 'emotion_2', 'emotion_3', 'emotion_4',\n",
    "#                          'emotion_5', 'emotion_6', 'emotion_7', 'emotion_8']].sum(axis=0)\n",
    "# max_count = class_counts.max()\n",
    "# class_counts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66b9cbac7d9e0fa2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# check_balance(class_counts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f83d17416924fa3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# augmented_df.to_csv('balanced_augmented_emotion_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "255912947b3e7d06",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read Augmetnation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89d19bfd3c0c0d46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# augmented_df = pd.read_csv('balanced_augmented_emotion_data.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "495284a35ade1b26",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# df_train_augmented = pd.concat([df_train, augmented_df], ignore_index=True)\n",
    "# \n",
    "# print(df_train_augmented[['emotion_1', 'emotion_2', 'emotion_3', 'emotion_4',\n",
    "#                           'emotion_5', 'emotion_6', 'emotion_7', 'emotion_8']].sum(axis=0))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7a6c9cf2324646d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run from here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1ec7f25b6e865b6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Original train size: {multi_label_binarizer_MEISD.shape}\")\n",
    "print(f\"Train: {df_train.shape}, Test: {df_test.shape}, Valid: {df_valid.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "359789b7b940660b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# Test the tokenizer\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "# generate encodings\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                  max_length = MAX_LEN,\n",
    "                                  truncation = True,\n",
    "                                  padding = \"max_length\",\n",
    "                                  return_attention_mask = True,\n",
    "                                  return_tensors = \"pt\")\n",
    "# we get a dictionary with three keys (see: https://huggingface.co/transformers/glossary.html) \n",
    "encodings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2a7b6b395283c1e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = df_train.columns.tolist()[1:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdfb2d5004eb0c75",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "LABEL_COLUMNS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97a86f1c1c5e0eef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EmotionalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                data: pd.DataFrame,\n",
    "                tokenizer: BertTokenizer,\n",
    "                max_token_len: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        # Extract text and labels\n",
    "        text = data_row['Utterances']\n",
    "        labels = data_row[LABEL_COLUMNS].astype(float).values  # Convert labels to float\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Return dictionary\n",
    "        return dict(\n",
    "            utterances=text,\n",
    "            input_ids=encoding[\"input_ids\"].squeeze(),\n",
    "            attention_mask=encoding[\"attention_mask\"].squeeze(),\n",
    "            labels=torch.tensor(labels, dtype=torch.float)  # Explicitly set tensor dtype\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f9fa99c62199e58",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = EmotionalDataset(df_train, tokenizer, max_token_len=MAX_LEN)\n",
    "sample_item = train_dataset[0]\n",
    "sample_item.keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "157b09813f955354",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_item[\"utterances\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5da7b9d9b36a1a65",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_item[\"labels\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38eab56920aa855a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c96e3a1eee73c69",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f8a62f76d847e78",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from time import time\n",
    "# \n",
    "# start_time = time()\n",
    "# for i, batch in enumerate(DataLoader(train_dataset, batch_size=1, num_workers=0)):\n",
    "#     if i == 10:  # Zatrzymaj po 10 iteracjach\n",
    "#         break\n",
    "# end_time = time()\n",
    "# print(f\"Time for 10 batches: {end_time - start_time:.2f} seconds\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d55e2ccafe66d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_batch = next(iter(DataLoader(train_dataset, batch_size=8, num_workers=0)))\n",
    "sample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c32c9901643533b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output = bert_model(sample_batch[\"input_ids\"], sample_batch[\"attention_mask\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b196faff70292714",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output.last_hidden_state.shape, output.pooler_output.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b75878ccb4fdaed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model.config.hidden_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be628529910e9c61",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EmotionalDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = EmotionalDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "        self.test_dataset = EmotionalDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e47f715b3a0d2f42",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_module = EmotionalDataModule(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=MAX_LEN\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "435f955d3a2c954f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c25124aeee49d0d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EmotionalTagger(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "\n",
    "        for i, name in enumerate(LABEL_COLUMNS):\n",
    "            class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "            self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c02655435885cfe1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizer scheduler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f01852b641cb6058"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dummy_model = nn.Linear(2, 1)\n",
    "\n",
    "optimizer = AdamW(params=dummy_model.parameters(), lr=0.001)\n",
    "\n",
    "warmup_steps = 20\n",
    "total_training_steps = 100\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "learning_rate_history = []\n",
    "\n",
    "for step in range(total_training_steps):\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    learning_rate_history.append(optimizer.param_groups[0]['lr'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bf141ad5e11b3e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(learning_rate_history, label=\"learning rate\")\n",
    "plt.axvline(x=warmup_steps, color=\"red\", linestyle=(0, (5, 10)), label=\"warmup end\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.tight_layout();"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db8c882f80712ee1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "steps_per_epoch=len(train_dataset) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * EPOCHS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f7d5a1b1614ccb2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e7a9638a578a7f9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = EmotionalTagger(\n",
    "    n_classes=len(LABEL_COLUMNS),\n",
    "    n_warmup_steps=warmup_steps,\n",
    "    n_training_steps=total_training_steps\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7d5bb6d5b144561",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "824f01a1d35a220"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "prediction = torch.FloatTensor(\n",
    "    [10.95873564, 1.07321467, 1.58524066, 0.03839076, 15.72987556, 1.09513213]\n",
    ")\n",
    "labels = torch.FloatTensor(\n",
    "    [1., 0., 0., 0., 1., 0.]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "428ddbac248ec525",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.sigmoid(prediction)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "101ec7015281cdad",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "criterion(torch.sigmoid(prediction), labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3d7881834b2c364",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "_, predictions = model(sample_batch[\"input_ids\"], sample_batch[\"attention_mask\"])\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a6ff8d7b88876d9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "criterion(predictions, sample_batch[\"labels\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "764161445b68ee95",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROC Curve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e2c6bc577fc050"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr = [0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n",
    "       0.11428571, 0.11428571, 0.2       , 0.4       , 1.        ]\n",
    "\n",
    "tpr = [0.        , 0.01265823, 0.67202532, 0.76202532, 0.91468354,\n",
    "       0.97468354, 0.98734177, 0.98734177, 1.        , 1.        ]\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, label=\"ROC\")\n",
    "ax.plot([0.05, 0.95], [0.05, 0.95], transform=ax.transAxes, label=\"Random classifier\", color=\"red\")\n",
    "ax.legend(loc=4)\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.set_title(\"Example ROC curve\")\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3f3e2991a643864",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89ee85fbc2a8f44f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba54c73956ce1e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"toxic-comments\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c3ae1d051ce75ff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae34a6d17fef60b9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    max_epochs=EPOCHS,\n",
    "    #accelerator=\"cpu\",\n",
    "    #devices=1,\n",
    "    #gpus=1,\n",
    "    progress_bar_refresh_rate=30\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844c7289a2f6d037",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for batch in data_module.train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d5b1ebdab86a552"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1a1488bc0d69431"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.test()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c23169a3f6ed50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "823da1ee795e648"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trained_model = EmotionalTagger.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path,\n",
    "    n_classes=len(LABEL_COLUMNS)\n",
    ")\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "955c340396e1e16"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_comment = \"Hi, I'm Meredith and I'm an alch... good at supplier relations\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "    test_comment,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "test_prediction = test_prediction.flatten().numpy()\n",
    "\n",
    "for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "    print(f\"{label}: {prediction}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d7208688c1d3c4d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "test_comment = \"You are such a loser! You'll regret everything you've done to me!\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "    test_comment,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "test_prediction = test_prediction.flatten().numpy()\n",
    "\n",
    "for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "    if prediction < THRESHOLD:\n",
    "        continue\n",
    "    print(f\"{label}: {prediction}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c769d054cc439bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99bbcccc2863e4e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = EmotionalDataset(\n",
    "    df_valid,\n",
    "    tokenizer,\n",
    "    max_token_len=MAX_LEN\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "    _, prediction = trained_model(\n",
    "        item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "        item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "    )\n",
    "    predictions.append(prediction.flatten())\n",
    "    labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8faa8b218cfb2e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accuracy(predictions, labels, threshold=THRESHOLD)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc2fbd23801fae8e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"AUROC per tag\")\n",
    "for i, name in enumerate(LABEL_COLUMNS):\n",
    "    tag_auroc = auroc(predictions[:, i], labels[:, i], pos_label=1)\n",
    "    print(f\"{name}: {tag_auroc}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "808a0beeb80538c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=LABEL_COLUMNS,\n",
    "    zero_division=0\n",
    "))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "987b40075914a700"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "abc4cc481110d26e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
