{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73bc3114f800d2e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('multi_label_binarizer_MEISD.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea967bdf16d7aac8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c3cd1ff6b557b79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For the multilabel classification we use:\n",
    "columns = ['Utterances', 'sentiment_0', 'sentiment_1', 'sentiment_2']\n",
    "multi_columns = df_data[columns].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f03d650c0c3ab9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multi_columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5ac6592e7d0d12c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data['label'] = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].idxmax(axis=1)\n",
    "df_data['label'] = df_data['label'].apply(lambda x: int(x.split('_')[1]))\n",
    "df_data = df_data[['Utterances', 'label']]\n",
    "df_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b5263ecbce69516",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b3904b6bd11e66c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# Test the tokenizer\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "# generate encodings\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                  max_length = 50,\n",
    "                                  truncation = True,\n",
    "                                  padding = \"max_length\",\n",
    "                                  return_attention_mask = True,\n",
    "                                  return_tensors = \"pt\")\n",
    "# we get a dictionary with three keys (see: https://huggingface.co/transformers/glossary.html) \n",
    "encodings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4583ff41d066406d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.sep_token, tokenizer.sep_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4e96b75767cb52b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.cls_token, tokenizer.cls_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6fce1b218dd6f7c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c80afdaab10a97e6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "910c4ed511357e43",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df_data['Utterances']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bcbe8d5956e2a8c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel('Token count')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82787cfc362ec257",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 30 #50 #128  # wiekszosc tokenow zdaje sie byc ponizej 40, klasycznie wklada sie tu 256, my przystaniemy na 30\n",
    "TRAIN_BATCH_SIZE = 32 #8 #16 #32 \n",
    "#Czasami, przy bardzo niskim tempie uczenia i zbyt dużych batchach, model może wolniej konwergować. Spróbuj zmniejszyć wielkość batcha, np. z 16 do 8.\n",
    "VALID_BATCH_SIZE = 32 #8 #16 #32\n",
    "TEST_BATCH_SIZE = 32 #8 #16 #32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05 #1e-05\n",
    "# Ustawienie bardzo niskiego współczynnika uczenia (np. 1e-05) może spowodować, że model uczy się bardzo wolno, co prowadzi do sytuacji, w której po wielu epokach nie ma znaczącej poprawy w wynikach walidacji.\n",
    "\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4169540c849c5cc3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df_data, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "830a2b086cc6cd51",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns = multi_columns.columns\n",
    "categor_freq = multi_columns[columns[1:]].sum() / multi_columns.shape[0]\n",
    "categor_freq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1869914ba83249b7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "print(class_distribution)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48368f8e1c2dbc24",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wykres rozkładu klas\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Sentiment Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fed0bbea9aac8025",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 3)\n",
    "plt.bar(categor_freq.index, categor_freq.values)\n",
    "_ = plt.xticks(rotation=45)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc155bc55d6fc25d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Train: {df_train.shape}, Test: {df_test.shape}, Valid: {df_valid.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c04b06158609497",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        # Upewnij się, że etykiety są typu całkowitego (int)\n",
    "        self.targets = self.df['label'].astype(int).values \n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])  # 'index' jest prawidłowe\n",
    "        #utterances = \" \".join(utterances.split())  # Usuwa niepotrzebne białe znaki\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.long)  # Zapewnij typ long\n",
    "        # print(f\"Target dtype: {target.dtype}\")  # Debugging\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),  # Zapewnij typ long\n",
    "            'utterances': utterances\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3fcc26e8264f157",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target_list = list(df_data.columns)\n",
    "target_list = target_list[1:]\n",
    "target_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73bfd1a79a83c61a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6ba75832e16a38",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=TRAIN_BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=0\n",
    "                                                )\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                              batch_size=VALID_BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=0\n",
    "                                              )\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                               batch_size=TEST_BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0\n",
    "                                               )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12d6b5e711313422",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e81afbd88da7754",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(data.keys())\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac8f47a2b68e7431",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encodings['input_ids'],\n",
    "    attention_mask=encodings['attention_mask']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f51dd2fe8bb830a6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model.config.hidden_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb9bbe80508adf34",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BERTSentimentClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSentimentClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3) #0.5\n",
    "        self.linear = torch.nn.Linear(bert_model.config.hidden_size, 3)\n",
    "        #self.softmax = nn.Softmax(dim=1) #remove for sentiment analysis\n",
    "        #CrossEntropyLoss automatycznie aplikuje funkcję softmax, więc nie ma potrzeby używać Softmax w modelu.\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        #pooler_output = self.pooler_output\n",
    "        dropout_output = self.dropout(output.pooler_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        #output = self.dropout(linear_output)\n",
    "        # output = self.softmax(linear_output)\n",
    "        return linear_output\n",
    "\n",
    "model = BERTSentimentClass()\n",
    "\n",
    "# # Freezing BERT layers:\n",
    "#for name, param in model.bert_model.named_parameters():\n",
    "#    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "#        param.requires_grad = True\n",
    "#    else:\n",
    "#        param.requires_grad = False\n",
    "\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b173035f9a453ad",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7384a11eae5618c1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "total_samples = sum(class_distribution)\n",
    "class_weights = [total_samples / count for count in class_distribution]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea3b10f21b6bef29",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)\n",
    "#change for sentiment analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e254869914b6694",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "# Harmonogram zmiany learning rate\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b88113f1705b015b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "# Learning Rate Tuning\n",
    "#total_steps = len(train_data_loader) * EPOCHS\n",
    "#scheduler = get_linear_schedule_with_warmup(\n",
    "#    optimizer,\n",
    "#    num_warmup_steps=0,\n",
    "#    num_training_steps=total_steps\n",
    "#)\n",
    "#loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e99ccc9880f86131",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ce5b61268865d691"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regularization (Dodanie Weight Decay)\n",
    "Weight decay to inaczej L2 regularizacja, która zapobiega nadmiernemu dopasowaniu (overfittingowi). W Adam możesz dodać weight decay w następujący sposób:\n",
    "\n",
    "Dodanie Weight Decay do Optimizera: Ustaw wartość weight decay w optymalizatorze. Standardowe wartości mieszczą się w zakresie 1e-4 do 1e-5:\n",
    "\n",
    "python code\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "Analiza Wyników: Weight decay zmniejsza wielkość wag w czasie, co stabilizuje uczenie. Po dodaniu weight decay zwróć uwagę na to, czy train_loss i val_loss są bliżej siebie – powinny się zbliżyć, co jest oznaką redukcji overfittingu.\n",
    "\n",
    "Aby zwiększyć regularizację, możesz zastosować tzw. weight decay, czyli L2 regularizację, podczas tworzenia optymalizatora. weight decay dodaje karę (regularizację) na wagi modelu, co może pomóc w ograniczeniu przeuczenia (overfitting). Możesz kontrolować siłę regularizacji za pomocą wartości weight_decay przy tworzeniu optymalizatora, takiego jak Adam.\n",
    "\n",
    "Jak ustawić weight decay w optymalizatorze?\n",
    "Podczas inicjalizacji optymalizatora, dodaj parametr weight_decay i przypisz mu wartość, która określi siłę regularizacji. Przykładowo, weight_decay=0.01 to dobry punkt wyjścia, ale możesz testować różne wartości, takie jak 0.001, 0.01, czy 0.1, aby znaleźć optymalną dla Twojego modelu.\n",
    "\n",
    "# Inicjalizacja optymalizatora Adam z regularizacją L2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "Rola weight_decay\n",
    "Wartość weight_decay=0.01 oznacza umiarkowaną regularizację.\n",
    "Wyższe wartości, np. weight_decay=0.1, zwiększają regularizację, co może zmniejszyć przeuczenie, ale też obniżyć dokładność na zbiorze treningowym.\n",
    "Niższe wartości, np. weight_decay=0.001 lub niższe, mają mniejszy wpływ na regularizację, ale mogą pomóc, jeśli model już ma dobrą dokładność na zbiorze walidacyjnym.\n",
    "\n",
    "Wybór Najlepszej Wartości\n",
    "Wybierz wartość weight_decay, która daje najlepszą równowagę między niskim val_loss a wysoką val_acc. Pamiętaj, że wyższe wartości weight_decay zwiększają regularizację, co może zmniejszyć overfitting, ale też potencjalnie obniżyć zdolność modelu do dopasowania się do danych.\n",
    "\n",
    "Zależność między learning rate a weight_decay\n",
    "Wyższe wartości weight_decay (np. 1e-2) wymagają zazwyczaj niższego learning rate, ponieważ większa regularizacja silniej wpływa na wagę parametrów, co stabilizuje model i zapobiega overfittingowi.\n",
    "Z kolei niższe wartości weight_decay (np. 1e-5 lub 1e-6) pozwalają na większą swobodę dopasowania modelu do danych i mogą lepiej działać przy nieco wyższym learning rate (np. 2e-5 lub 3e-5).\n",
    "\n",
    "\n",
    "7. Dalsze Kroki\n",
    "Layer-Wise Learning Rate: Możesz również eksperymentować z różnymi learning rates dla różnych warstw, co może jeszcze bardziej poprawić wyniki.\n",
    "Early Stopping: Dodaj mechanizm wczesnego zatrzymania treningu, aby uniknąć nadmiernego trenowania modelu.\n",
    "Eksperymenty z Hiperparametrami: Możesz rozważyć korzystanie z narzędzi do automatycznego tuningu hiperparametrów, takich jak Optuna czy Ray Tune, aby systematycznie znaleźć optymalne wartości.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79bc7b7db3c67783"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Rate Tuning\n",
    "\n",
    "Adaptacyjna redukcja learning rate za pomocą torch.optim.lr_scheduler.ReduceLROnPlateau pozwala obniżyć learning rate, kiedy model przestaje poprawiać wyniki. Możemy to zrobić w ten sposób:\n",
    "\n",
    "Inicjalizacja Optimizera i Schedulera\": przy optymalizatorze AdamW (lub innego). Po zdefiniowaniu optymalizatora dodaj ReduceLROnPlateau:\n",
    "\n",
    "python code\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Początkowy learning rate\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Początkowy learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "    mode='min': Zmniejsza learning rate, gdy minimalizowana jest wartość (np. val_loss).\n",
    "    patience=2: Liczba epok bez poprawy przed obniżeniem learning rate.\n",
    "    factor=0.5: Po jakim czynniku obniżyć learning rate (np. z 1e-3 na 5e-4).\n",
    "    verbose=True: Informuje w logach o zmianie learning rate.\n",
    "\n",
    "Wykorzystanie Schedulera podczas trenowania: Po zakończeniu każdej epoki przekaż do schedulera wynik val_loss, by sprawdzić, czy learning rate wymaga obniżenia:\n",
    "\n",
    "python code\n",
    "        scheduler.step(val_loss)\n",
    "Użycie scheduler.step(val_loss) automatycznie sprawi, że learning rate zostanie zmniejszony w miarę potrzeby.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44e396491a02203"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(training_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    model.train()\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), leave=True, colour='steelblue')\n",
    "\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets).item()\n",
    "        num_samples += targets.size(0)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Log to TensorBoard and update progress bar\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(training_loader) + batch_idx)\n",
    "        writer.add_scalar('Accuracy/train', correct_predictions / num_samples, epoch * len(training_loader) + batch_idx)\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    return model, correct_predictions / num_samples, np.mean(losses)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dca06c411b50ef06",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            # Calculate loss before applying argmax\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == targets).item()\n",
    "            num_samples += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    writer.add_scalar('Loss/validation', np.mean(losses), epoch)\n",
    "    writer.add_scalar('Accuracy/validation', correct_predictions / num_samples, epoch)\n",
    "\n",
    "    return correct_predictions / num_samples, np.mean(losses)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e92ef02b9630acb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7bb674ce36fe972d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, epoch):\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(f'Confusion Matrix at Epoch {epoch}')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = torch.tensor(np.frombuffer(buf.getvalue(), dtype=np.uint8)).float()\n",
    "    writer.add_image('Confusion Matrix', image, epoch)\n",
    "\n",
    "    plt.close(figure)  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5033a49d36c4c44e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c1f9311c7351447",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(val_data_loader, model)\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "\n",
    "    # Logowanie strat i dokładności do TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
    "\n",
    "    print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), \"best_model_state.bin\")\n",
    "        best_accuracy = val_acc\n",
    "\n",
    "    scheduler.step(val_loss)     # Learning Rate Tuning\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_data_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            preds = torch.argmax(outputs, axis=1).cpu().detach().numpy()\n",
    "            labels = targets.cpu().detach().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Oblicz confusion matrix\n",
    "    # cm = confusion_matrix(all_labels, all_preds)\n",
    "    # plot_confusion_matrix(cm, class_names=['class0', 'class1', 'class2'], epoch=epoch)\n",
    "\n",
    "writer.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abd94fa0b93e2013",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Overfitting:\n",
    "Niski train_loss (0.2796) wskazuje na to, że model dobrze dopasowuje się do danych treningowych, ale wysoki val_loss (2.9933) oraz brak poprawy w val_acc (0.0000) sugerują, że model może się przeuczać, czyli dopasowuje się zbyt mocno do danych treningowych i traci zdolność do generalizacji na danych testowych.\n",
    "Rozwiązanie:\n",
    "Dodanie technik regularyzacyjnych, jak dropout, L2 regularization.\n",
    "Wykorzystanie większego zbioru danych.\n",
    "Zastosowanie wcześniejszego zatrzymania (early stopping), aby przerwać trening, gdy model zaczyna się przeuczać.\n",
    "# 2. Zbyt niski learning rate:\n",
    "Ustawienie bardzo niskiego współczynnika uczenia (np. 1e-05) może spowodować, że model uczy się bardzo wolno, co prowadzi do sytuacji, w której po wielu epokach nie ma znaczącej poprawy w wynikach walidacji.\n",
    "Rozwiązanie: Spróbuj zwiększyć learning rate np. do 1e-04 i zobacz, czy poprawia to wyniki. Zbyt niski współczynnik uczenia może blokować osiąganie optymalnych wyników.\n",
    "# 3. Zbyt skomplikowany model:\n",
    "Jeśli model jest zbyt złożony w stosunku do dostępnych danych, może to prowadzić do overfittingu. Model nauczy się bardzo dobrze danych treningowych, ale nie będzie w stanie dobrze generalizować.\n",
    "Rozwiązanie: Możesz spróbować uprościć model (np. mniejsza liczba warstw, mniejsza liczba neuronów) lub zebrać większy zbiór danych, jeśli to możliwe.\n",
    "# 4. Problemy z danymi:\n",
    "Dane walidacyjne mogą zawierać problemy, takie jak błędnie oznaczone próbki, brak różnorodności, lub mogą nie być reprezentatywne dla danych treningowych.\n",
    "Rozwiązanie: Sprawdź, czy dane walidacyjne są dobrze zrównoważone i poprawnie oznaczone. Ewentualnie przetestuj na innym zbiorze walidacyjnym.\n",
    "# 5. Złe inicjalizacje wag lub problemy z optymalizacją:\n",
    "Wysoki val_loss i brak poprawy w val_acc mogą wskazywać na problemy z optymalizacją. Np. złe inicjalizacje wag lub nieodpowiedni optymalizator mogą powodować, że model nie znajduje optymalnych rozwiązań.\n",
    "Rozwiązanie: Spróbuj zmienić optymalizator (np. Adam na RMSprop), lub zastosować inne techniki inicjalizacji wag.\n",
    "# 6. Zbyt zróżnicowane klasy:\n",
    "Jeśli Twoje klasy są bardzo niezrównoważone, to model może mieć problem z nauczeniem się klasyfikacji rzadkich klas.\n",
    "Rozwiązanie: Upewnij się, że klasy są zrównoważone lub użyj metod radzenia sobie z niezrównoważonymi danymi (np. class weights w funkcji straty).\n",
    "\n",
    "## 7. Optymalizacja i liczba epok: \n",
    "Jeśli model nie uczy się z oczekiwaną prędkością, możesz rozważyć tuning hiperparametrów, jak np. zmniejszenie wartości learning rate, co może pomóc modelowi lepiej uczyć się przy niższych wartościach początkowych. Możesz też wydłużyć trening, jeśli osiągalne wartości są powolne, ale stopniowo poprawiające się.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa92c9ea6f194565"
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://developers.google.com/machine-learning/crash-course/overfitting/regularization#early_stopping_an_alternative_to_complexity-based_regularization\n",
    "\n",
    "Picking the regularization rate\n",
    "The ideal regularization rate produces a model that generalizes well to new, previously unseen data. Unfortunately, that ideal value is data-dependent, so you must do some tuning.\n",
    "\n",
    "Early stopping: an alternative to complexity-based regularization\n",
    "Early stopping is a regularization method that doesn't involve a calculation of complexity. Instead, early stopping simply means ending training before the model fully converges. For example, you end training when the loss curve for the validation set starts to increase (slope becomes positive).\n",
    "\n",
    "Although early stopping usually increases training loss, it can decrease test loss.\n",
    "\n",
    "Early stopping is a quick, but rarely optimal, form of regularization. The resulting model is very unlikely to be as good as a model trained thoroughly on the ideal regularization rate.\n",
    "\n",
    "Finding equilibrium between learning rate and regularization rate\n",
    "Learning rate and regularization rate tend to pull weights in opposite directions. A high learning rate often pulls weights away from zero; a high regularization rate pulls weights towards zero.\n",
    "\n",
    "If the regularization rate is high with respect to the learning rate, the weak weights tend to produce a model that makes poor predictions. Conversely, if the learning rate is high with respect to the regularization rate, the strong weights tend to produce an overfit model.\n",
    "\n",
    "Your goal is to find the equilibrium between learning rate and regularization rate. This can be challenging. Worst of all, once you find that elusive balance, you may have to ultimately change the learning rate. And, when you change the learning rate, you'll again have to find the ideal regularization rate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3019f67aedc2d335"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f1c49aca7626a1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_xlabel(\"Predicted labels\")\n",
    "    ax.set_ylabel(\"True labels\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_yticklabels(class_names)\n",
    "    return fig\n",
    "\n",
    "# Tworzenie confusion matrix po ewaluacji\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Logowanie confusion matrix jako obraz\n",
    "fig = plot_confusion_matrix(cm, class_names=['class0', 'class1', 'class2'])\n",
    "writer.add_figure('Confusion matrix', fig, epoch)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd274764070cb547",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5303ae57d6d9c2f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5f07c30edc7ba933"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
