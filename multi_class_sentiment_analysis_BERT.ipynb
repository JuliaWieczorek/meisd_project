{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73bc3114f800d2e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('multi_label_binarizer_MEISD.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea967bdf16d7aac8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c3cd1ff6b557b79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For the multilabel classification we use:\n",
    "columns = ['Utterances', 'sentiment_0', 'sentiment_1', 'sentiment_2']\n",
    "multi_columns = df_data[columns].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f03d650c0c3ab9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multi_columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5ac6592e7d0d12c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data['label'] = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].idxmax(axis=1)\n",
    "df_data['label'] = df_data['label'].apply(lambda x: int(x.split('_')[1]))\n",
    "df_data = df_data[['Utterances', 'label']]\n",
    "df_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b5263ecbce69516",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Test the tokenizer\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "# generate encodings\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens = True,\n",
    "                                  max_length = 50,\n",
    "                                  truncation = True,\n",
    "                                  padding = \"max_length\",\n",
    "                                  return_attention_mask = True,\n",
    "                                  return_tensors = \"pt\")\n",
    "# we get a dictionary with three keys (see: https://huggingface.co/transformers/glossary.html) \n",
    "encodings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4583ff41d066406d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df_data['Utterances']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bcbe8d5956e2a8c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel('Token count')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82787cfc362ec257",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 50 #128  # wiekszosc tokenow zdaje sie byc ponizej 40, klasycznie wklada sie tu 256, my przystaniemy na 50\n",
    "TRAIN_BATCH_SIZE = 8 #16 #32 \n",
    "#Czasami, przy bardzo niskim tempie uczenia i zbyt dużych batchach, model może wolniej konwergować. Spróbuj zmniejszyć wielkość batcha, np. z 16 do 8.\n",
    "VALID_BATCH_SIZE = 8 #16 #32\n",
    "TEST_BATCH_SIZE = 8 #16 #32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-05\n",
    "# Ustawienie bardzo niskiego współczynnika uczenia (np. 1e-05) może spowodować, że model uczy się bardzo wolno, co prowadzi do sytuacji, w której po wielu epokach nie ma znaczącej poprawy w wynikach walidacji.\n",
    "\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4169540c849c5cc3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "89b4c7aa557bc5b8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df_data, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "830a2b086cc6cd51",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns = multi_columns.columns\n",
    "categor_freq = multi_columns[columns[1:]].sum() / multi_columns.shape[0]\n",
    "categor_freq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1869914ba83249b7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "print(class_distribution)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48368f8e1c2dbc24",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wykres rozkładu klas\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Sentiment Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fed0bbea9aac8025",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 3)\n",
    "plt.bar(categor_freq.index, categor_freq.values)\n",
    "_ = plt.xticks(rotation=45)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc155bc55d6fc25d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Train: {df_train.shape}, Test: {df_test.shape}, Valid: {df_valid.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c04b06158609497",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        # Upewnij się, że etykiety są typu całkowitego (int)\n",
    "        self.targets = self.df['label'].astype(int).values  # Zapewnij typ int\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])  # 'index' jest prawidłowe\n",
    "        utterances = \" \".join(utterances.split())  # Usuwa niepotrzebne białe znaki\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.long)  # Zapewnij typ long\n",
    "        # print(f\"Target dtype: {target.dtype}\")  # Debugging\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),  # Zapewnij typ long\n",
    "            'utterances': utterances\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3fcc26e8264f157",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target_list = list(df_data.columns)\n",
    "target_list = target_list[1:]\n",
    "target_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73bfd1a79a83c61a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6ba75832e16a38",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = next(iter(train_dataset))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e81afbd88da7754",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(data.keys())\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac8f47a2b68e7431",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=TRAIN_BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=0\n",
    "                                                )\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                              batch_size=VALID_BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=0\n",
    "                                              )\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                               batch_size=TEST_BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0\n",
    "                                               )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12d6b5e711313422",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BERTSentimentClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSentimentClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.linear = torch.nn.Linear(768, 3)\n",
    "        #self.softmax = nn.Softmax(dim=1) #remove for sentiment analysis\n",
    "        #CrossEntropyLoss automatycznie aplikuje funkcję softmax, więc nie ma potrzeby używać Softmax w modelu.\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        #pooler_output = self.pooler_output\n",
    "        dropout_output = self.dropout(output.pooler_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        # output = self.softmax(linear_output)\n",
    "        return linear_output\n",
    "\n",
    "model = BERTSentimentClass()\n",
    "\n",
    "# # Freezing BERT layers:\n",
    "# for param in model.bert_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b173035f9a453ad",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "total_samples = sum(class_distribution)\n",
    "class_weights = [total_samples / count for count in class_distribution]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea3b10f21b6bef29",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    #print(f\"Outputs dtype: {outputs.dtype}\")  # Debugging\n",
    "    #print(f\"Targets dtype: {targets.dtype}\")  # Debugging\n",
    "\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)\n",
    "#change for sentiment analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e254869914b6694",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "# Harmonogram zmiany learning rate\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b88113f1705b015b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d426230188fb904",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training of the model for one epoch\n",
    "def train_model(training_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to training mode (activate droput, batch norm)\n",
    "    model.train()\n",
    "    # initialize the progress bar\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader),\n",
    "                   leave=True, colour='steelblue')\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "\n",
    "        # forward\n",
    "        outputs = model(ids, mask, token_type_ids)  # (batch,predict)=(32,8)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # Debugging\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"Input IDs shape: {ids.shape}\")\n",
    "        print(f\"Targets shape: {targets.shape}\")\n",
    "        print(f\"Outputs shape: {outputs.shape}\")\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n",
    "        # change for sentiment analysis becuase we have switch to Cross Entropy Loss\n",
    "        outputs = torch.argmax(outputs, axis=1).cpu().detach()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        correct_predictions += np.sum(outputs == targets)\n",
    "        num_samples += targets.size  # total number of elements in the 2D array\n",
    "\n",
    "        # Debugging\n",
    "        #print(f\"Raw outputs: {outputs}\")\n",
    "        #print(f\"Targets: {targets}\")\n",
    "        \n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # grad descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Debugging\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #        print(f\"Gradients for {name}: {param.grad}\")\n",
    "\n",
    "        # Log loss and accuracy to TensorBoard\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(training_loader) + batch_idx)\n",
    "        writer.add_scalar('Accuracy/train', correct_predictions / num_samples, epoch * len(training_loader) + batch_idx)\n",
    "\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    # returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions) / num_samples, np.mean(losses)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dca06c411b50ef06",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to eval mode (turn off dropout, fix batch norm)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "            # change for sentiment analysis becuase we have switch to Cross Entropy Loss\n",
    "            outputs = torch.argmax(outputs, axis=1).cpu().detach()      \n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            correct_predictions += np.sum(outputs==targets)\n",
    "            num_samples += targets.size   # total number of elements in the 2D array\n",
    "            all_preds.extend(outputs.numpy())\n",
    "            all_labels.extend(targets)\n",
    "\n",
    "\n",
    "# Log validation loss and accuracy to TensorBoard\n",
    "    writer.add_scalar('Loss/validation', np.mean(losses), epoch)\n",
    "    writer.add_scalar('Accuracy/validation', float(correct_predictions) / num_samples, epoch)\n",
    "\n",
    "    # Confusion matrix\n",
    "    # cm = confusion_matrix(all_labels, all_preds)\n",
    "    # plot_confusion_matrix(cm, class_names=['class0', 'class1', 'class2'], epoch=epoch)\n",
    "\n",
    "\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e92ef02b9630acb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, epoch):\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(f'Confusion Matrix at Epoch {epoch}')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = torch.tensor(np.frombuffer(buf.getvalue(), dtype=np.uint8)).float()\n",
    "    writer.add_image('Confusion Matrix', image, epoch)\n",
    "\n",
    "    plt.close(figure)  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5033a49d36c4c44e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c1f9311c7351447",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "    val_acc, val_loss = eval_model(val_data_loader, model, optimizer)\n",
    "\n",
    "    # Logowanie strat i dokładności do TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
    "\n",
    "    print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), \"best_model_state.bin\")\n",
    "        best_accuracy = val_acc\n",
    "\n",
    "    all_preds = []  \n",
    "    all_labels = [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_data_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            preds = torch.argmax(outputs, axis=1).cpu().detach().numpy() \n",
    "            labels = targets.cpu().detach().numpy() \n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Oblicz confusion matrix\n",
    "    # cm = confusion_matrix(all_labels, all_preds)\n",
    "    # plot_confusion_matrix(cm, class_names=['class0', 'class1', 'class2'], epoch=epoch)\n",
    "\n",
    "writer.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abd94fa0b93e2013",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Overfitting:\n",
    "Opis: Niski train_loss (0.2796) wskazuje na to, że model dobrze dopasowuje się do danych treningowych, ale wysoki val_loss (2.9933) oraz brak poprawy w val_acc (0.0000) sugerują, że model może się przeuczać, czyli dopasowuje się zbyt mocno do danych treningowych i traci zdolność do generalizacji na danych testowych.\n",
    "Rozwiązanie:\n",
    "Dodanie technik regularyzacyjnych, jak dropout, L2 regularization.\n",
    "Wykorzystanie większego zbioru danych.\n",
    "Zastosowanie wcześniejszego zatrzymania (early stopping), aby przerwać trening, gdy model zaczyna się przeuczać.\n",
    "# 2. Zbyt niski learning rate:\n",
    "Opis: Ustawienie bardzo niskiego współczynnika uczenia (np. 1e-05) może spowodować, że model uczy się bardzo wolno, co prowadzi do sytuacji, w której po wielu epokach nie ma znaczącej poprawy w wynikach walidacji.\n",
    "Rozwiązanie: Spróbuj zwiększyć learning rate np. do 1e-04 i zobacz, czy poprawia to wyniki. Zbyt niski współczynnik uczenia może blokować osiąganie optymalnych wyników.\n",
    "# 3. Zbyt skomplikowany model:\n",
    "Opis: Jeśli model jest zbyt złożony w stosunku do dostępnych danych, może to prowadzić do overfittingu. Model nauczy się bardzo dobrze danych treningowych, ale nie będzie w stanie dobrze generalizować.\n",
    "Rozwiązanie: Możesz spróbować uprościć model (np. mniejsza liczba warstw, mniejsza liczba neuronów) lub zebrać większy zbiór danych, jeśli to możliwe.\n",
    "# 4. Problemy z danymi:\n",
    "Opis: Dane walidacyjne mogą zawierać problemy, takie jak błędnie oznaczone próbki, brak różnorodności, lub mogą nie być reprezentatywne dla danych treningowych.\n",
    "Rozwiązanie: Sprawdź, czy dane walidacyjne są dobrze zrównoważone i poprawnie oznaczone. Ewentualnie przetestuj na innym zbiorze walidacyjnym.\n",
    "# 5. Złe inicjalizacje wag lub problemy z optymalizacją:\n",
    "Opis: Wysoki val_loss i brak poprawy w val_acc mogą wskazywać na problemy z optymalizacją. Np. złe inicjalizacje wag lub nieodpowiedni optymalizator mogą powodować, że model nie znajduje optymalnych rozwiązań.\n",
    "Rozwiązanie: Spróbuj zmienić optymalizator (np. Adam na RMSprop), lub zastosować inne techniki inicjalizacji wag.\n",
    "# 6. Zbyt zróżnicowane klasy:\n",
    "Opis: Jeśli Twoje klasy są bardzo niezrównoważone, to model może mieć problem z nauczeniem się klasyfikacji rzadkich klas.\n",
    "Rozwiązanie: Upewnij się, że klasy są zrównoważone lub użyj metod radzenia sobie z niezrównoważonymi danymi (np. class weights w funkcji straty).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa92c9ea6f194565"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f1c49aca7626a1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_xlabel(\"Predicted labels\")\n",
    "    ax.set_ylabel(\"True labels\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_yticklabels(class_names)\n",
    "    return fig\n",
    "\n",
    "# Tworzenie confusion matrix po ewaluacji\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Logowanie confusion matrix jako obraz\n",
    "fig = plot_confusion_matrix(cm, class_names=['class0', 'class1', 'class2'])\n",
    "writer.add_figure('Confusion matrix', fig, epoch)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd274764070cb547",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5303ae57d6d9c2f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5f07c30edc7ba933"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
