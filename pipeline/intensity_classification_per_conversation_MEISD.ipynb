{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61c6d47e5a26b8d7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4\n",
    "MAX_LEN = 100\n",
    "BATCH = 8\n",
    "PRE_TRAINED_MODEL_NAME = \"distilbert-base-uncased\" #'bert-base-cased'\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "THRESHOLD = 0.2\n",
    "DROPOUT_RATE = 0.5\n",
    "WEIGHT_DECAY = 0.2\n",
    "MODE = 'min'\n",
    "PATIENCE = 2\n",
    "FACTOR = 0.5\n",
    "VERBOSE = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88a75ce76d5afc9c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/juwieczo/DataspellProjects/meisd_project/pipeline/balanced_augmented_data_primary_intensity.csv')\n",
    "# Zamień wartości na liczby całkowite\n",
    "# df['intensity'] = pd.to_numeric(df['intensity'], errors='coerce').fillna(0)\n",
    "# df['intensity2'] = pd.to_numeric(df['intensity2'], errors='coerce').fillna(0)\n",
    "# df['intensity3'] = pd.to_numeric(df['intensity3'], errors='coerce').fillna(0)\n",
    "\n",
    "# Zamień wartości zawierające tylko białe znaki lub '`', 'neu', 'po' na NaN\n",
    "# df['intensity'] = df['intensity'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "# df['intensity'] = df['intensity'].replace(['`', 'neu', 'po'], np.nan)\n",
    "# df['intensity2'] = df['intensity2'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "# df['intensity2'] = df['intensity2'].replace(['`', 'neu', 'po'], np.nan)\n",
    "# df['intensity3'] = df['intensity3'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "# df['intensity3'] = df['intensity3'].replace(['`', 'neu', 'po'], np.nan)\n",
    "\n",
    "# Użyj forward fill, aby uzupełnić brakujące wartości poprzedzającą wartością\n",
    "# df['intensity'] = df['intensity'].ffill()\n",
    "# df['intensity2'] = df['intensity2'].ffill()\n",
    "# df['intensity3'] = df['intensity3'].ffill()\n",
    "\n",
    "# Usuń znaki niebędące cyframi (np. '`') za pomocą wyrażeń regularnych\n",
    "# df['intensity'] = df['intensity'].replace(r'\\D', '', regex=True).astype(int)  # Usuwa wszystko, co nie jest cyfrą\n",
    "# df['intensity2'] = df['intensity2'].replace(r'\\D', '', regex=True).astype(int)\n",
    "# df['intensity3'] = df['intensity3'].replace(r'\\D', '', regex=True).astype(int)\n",
    "\n",
    "missing_count = df['label'].isna().sum()\n",
    "print(f\"Liczba braków w kolumnie 'intensity': {missing_count}\")\n",
    "unique_values = df['label'].unique()\n",
    "print(f\"Unikalne wartości w kolumnie 'intensity': {unique_values}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c32f6bc864fddd97",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#columns = ['Utterances', 'dialog_ids', 'uttr_ids', 'intensity', 'intensity2', 'intensity3']\n",
    "columns = ['Utterances', 'label']\n",
    "df = df[columns].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f489ce945c69132",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# first_25_data = []\n",
    "# last_25_data = []\n",
    "# \n",
    "# def process_group(group):\n",
    "#     num_rows = len(group)\n",
    "#     quarter_size = max(1, num_rows // 4)\n",
    "# \n",
    "#     # First 25%\n",
    "#     first_25 = group.iloc[:quarter_size]\n",
    "#     primary_intensity = max(\n",
    "#         group['intensity'].iloc[0],\n",
    "#         group['intensity2'].iloc[0],\n",
    "#         group['intensity3'].iloc[0]\n",
    "#     )\n",
    "#     first_25 = first_25.assign(primary_intensity=primary_intensity)\n",
    "# \n",
    "#     # Last 25%\n",
    "#     last_25 = group.iloc[-quarter_size:]\n",
    "#     final_intensity = max(\n",
    "#         group['intensity'].iloc[-1],\n",
    "#         group['intensity2'].iloc[-1],\n",
    "#         group['intensity3'].iloc[-1]\n",
    "#     )\n",
    "#     last_25 = last_25.assign(final_intensity=final_intensity)\n",
    "# \n",
    "#     first_25_data.append(first_25)\n",
    "#     last_25_data.append(last_25)\n",
    "# \n",
    "# df.groupby('dialog_ids').apply(process_group)\n",
    "# \n",
    "# first_25_df = pd.concat(first_25_data).reset_index(drop=True)\n",
    "# last_25_df = pd.concat(last_25_data).reset_index(drop=True)\n",
    "# \n",
    "# grouped_first_25 = first_25_df.groupby('dialog_ids').agg({\n",
    "#     'Utterances': ' '.join,\n",
    "#     'primary_intensity': 'first'\n",
    "# }).reset_index()\n",
    "# \n",
    "# grouped_last_25 = last_25_df.groupby('dialog_ids').agg({\n",
    "#     'Utterances': ' '.join,\n",
    "#     'final_intensity': 'first'\n",
    "# }).reset_index()\n",
    "# \n",
    "# df = grouped_first_25.drop(df.columns[0], axis=1)\n",
    "# \n",
    "# # grouped_first_25.to_csv('first_25_percent.csv', index=False)\n",
    "# # grouped_last_25.to_csv('last_25_percent.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e56f08b5653e50db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#grouped_first_25.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f201920484f65a4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# \n",
    "# # Test the tokenizer\n",
    "# test_text = \"We are testing BERT tokenizer.\"\n",
    "# # generate encodings\n",
    "# encodings = tokenizer.encode_plus(test_text,\n",
    "#                                   add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                                   max_length = 50,\n",
    "#                                   truncation = True,\n",
    "#                                   padding = \"max_length\",\n",
    "#                                   return_attention_mask = True,\n",
    "#                                   return_tensors = \"pt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8049e14d2a5eb2f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer\n",
    "\n",
    "# Jeśli używasz DistilBERT, musisz załadować tokenizer dla DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "encodings = tokenizer.encode_plus(\n",
    "    test_text,\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=50,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", encodings[\"input_ids\"])\n",
    "print(\"Attention Mask:\", encodings[\"attention_mask\"])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61380a94d9a6e7a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbdd95a0a34cab18",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df['Utterances']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6e84cca3a6993fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 100])\n",
    "plt.xlabel('Token count')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "749900659c725beb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#df = grouped_first_25"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fffaa3ad682825bd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        self.targets = self.df['label'].astype(int).values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),\n",
    "            'utterances': utterances\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7f70271994ce23d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e26dd199592b9a07",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Original train size: {df.shape}\")\n",
    "print(f\"Validation size: {df_valid.shape}, Test size: {df_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd8a98fbef943116",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_frequencies = df_train['label'].value_counts()\n",
    "label_frequencies_percent = df_train['label'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7987bc066a1b131",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target_list = list(df.columns)\n",
    "target_list = target_list[1:]\n",
    "target_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5d2bf8b8703971e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class BERT_IntensityClass(torch.nn.Module):\n",
    "#     def __init__(self, bert_model):\n",
    "#         super(BERT_IntensityClass, self).__init__()\n",
    "#         self.bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "#         self.dropout = torch.nn.Dropout(p=DROPOUT_RATE) #0.5\n",
    "#         self.linear = torch.nn.Linear(bert_model.config.hidden_size, NUM_CLASSES)\n",
    "#         #self.softmax = nn.Softmax(dim=1) #remove for sentiment analysis\n",
    "#         #CrossEntropyLoss automatycznie aplikuje funkcję softmax, więc nie ma potrzeby używać Softmax w modelu.\n",
    "# \n",
    "# \n",
    "#     def forward(self, input_ids, attn_mask, token_type_ids=None):\n",
    "#         output = self.bert_model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "#         #pooler_output = self.pooler_output\n",
    "#         dropout_output = self.dropout(output.pooler_output)\n",
    "#         linear_output = self.linear(dropout_output)\n",
    "#         #output = self.dropout(linear_output)\n",
    "#         # output = self.softmax(linear_output)\n",
    "#         return linear_output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e66c5471d9631d7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DistilBERT_IntensityClass(torch.nn.Module):\n",
    "    def __init__(self, distilbert_model, dropout_rate=DROPOUT_RATE, num_classes=NUM_CLASSES):\n",
    "        super(DistilBERT_IntensityClass, self).__init__()\n",
    "        self.distilbert_model = distilbert_model\n",
    "        self.dropout = torch.nn.Dropout(p=DROPOUT_RATE)\n",
    "        self.linear = torch.nn.Linear(self.distilbert_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        # DistilBERT model processing\n",
    "        output = self.distilbert_model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "        # Use the last hidden state (the embedding for [CLS] token is at index 0)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
    "        # Apply dropout\n",
    "        dropout_output = self.dropout(cls_output)\n",
    "        # Get final class logits\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d48855c6a6f9e0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# model = BERT_IntensityClass(bert_model)\n",
    "# model.to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90aee7dd6a5312b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel\n",
    "distilbert_model = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = DistilBERT_IntensityClass(distilbert_model)\n",
    "model.to(device)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e37f0d6ce056d33",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab9f1d0ec1d16f8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH, shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f6bb1de72c32ea0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# data = next(iter(train_data_loader))\n",
    "# outputs = model(data[\"input_ids\"], attn_mask=data[\"attention_mask\"])\n",
    "# print(outputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ca1f227bf8d526",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad73480f389e4909",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test_text = \"We are testing BERT tokenizer.\"\n",
    "# encodings = tokenizer.encode_plus(test_text,\n",
    "#                                   add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "#                                   max_length=50,\n",
    "#                                   truncation=True,\n",
    "#                                   padding=\"max_length\",\n",
    "#                                   return_attention_mask=True,\n",
    "#                                   return_tensors=\"pt\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "472bb4d14d690cb3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer\n",
    "\n",
    "# Jeśli używasz DistilBERT, musisz załadować tokenizer dla DistilBERT\n",
    "\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "encodings = tokenizer.encode_plus(\n",
    "    test_text,\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=50,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", encodings[\"input_ids\"])\n",
    "print(\"Attention Mask:\", encodings[\"attention_mask\"])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44d24164d47e1a32",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# last_hidden_state, pooled_output = bert_model(\n",
    "#     input_ids=encodings['input_ids'],\n",
    "#     attention_mask=encodings['attention_mask']\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5255f143cf8725fd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Pass the inputs through the DistilBERT model\n",
    "output = distilbert_model(\n",
    "    input_ids=encodings['input_ids'],\n",
    "    attention_mask=encodings['attention_mask']\n",
    ")\n",
    "\n",
    "# Extract the last hidden state\n",
    "last_hidden_state = output.last_hidden_state\n",
    "\n",
    "# Extract the representation of the [CLS] token (first token in the sequence)\n",
    "cls_output = last_hidden_state[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "# Now you can use cls_output for downstream tasks (e.g., classification)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "284094d3a727a604",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_distribution = df_train['label'].value_counts(normalize=True)\n",
    "total_samples = sum(class_distribution)\n",
    "class_weights = [total_samples / count for count in class_distribution]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83abf052a0741cf6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bf88e8032b196ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad813080e544a445",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "#EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=MODE, patience=PATIENCE, factor=FACTOR, verbose=VERBOSE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30a39e6dab7d1f20",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(training_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.train()\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), leave=True, colour='steelblue')\n",
    "\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "        outputs = model(ids, mask)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Calculate predictions and accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets).item()\n",
    "        num_samples += targets.size(0)\n",
    "\n",
    "        # Collect predictions and labels for F1-score\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    # Calculate F1-score for training data\n",
    "    train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return model, correct_predictions / num_samples, np.mean(losses), train_f1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdf002b23294d72f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, epoch):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(ids, mask)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Calculate predictions and accuracy\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == targets).item()\n",
    "            num_samples += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Logowanie do TensorBoard\n",
    "    writer.add_scalar('Loss/validation', avg_loss, epoch)\n",
    "    writer.add_scalar('F1-Score/validation', val_f1, epoch)\n",
    "\n",
    "    return correct_predictions / num_samples, avg_loss, val_f1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a99195b89cece76f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, epoch):\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(f'Confusion Matrix at Epoch {epoch}')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = torch.tensor(np.frombuffer(buf.getvalue(), dtype=np.uint8)).float()\n",
    "    writer.add_image('Confusion Matrix', image, epoch)\n",
    "\n",
    "    plt.close(figure)  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "affa34ced3abe45f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Główna pętla treningowa\n",
    "# %%time\n",
    "history = defaultdict(list)\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "\n",
    "    model, train_acc, train_loss, train_f1 = train_model(train_data_loader, model, optimizer)\n",
    "    print(f'Train loss {train_loss:.4f} | Train accuracy {train_acc:.4f} | Train F1 {train_f1:.4f}')\n",
    "\n",
    "    val_acc, val_loss, val_f1 = eval_model(val_data_loader, model, epoch)\n",
    "    print(f'Val loss {val_loss:.4f} | Val accuracy {val_acc:.4f} | Val F1 {val_f1:.4f}')\n",
    "\n",
    "    # Logowanie metryk do TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('F1-Score/train', train_f1, epoch)\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    # Sprawdzenie najlepszej F1 i zapisanie modelu\n",
    "    if val_f1 > best_f1:\n",
    "        torch.save(model.state_dict(), \"best_model_state.bin\")\n",
    "        best_f1 = val_f1\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    scheduler.step(val_loss)  # Tuning LR\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa4135f847993eba",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "33e1aa162763d9af",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a72238f3b1baa3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ac0a9ce76506c057"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "32d751feba8be1f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
