{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:29.403365Z",
     "start_time": "2025-01-30T18:16:20.993566Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "BATCH = 8\n",
    "PRE_TRAINED_MODEL_NAME = \"distilbert-base-uncased\" #'bert-base-cased'\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 0.0001\n",
    "THRESHOLD = 0.2\n",
    "DROPOUT_RATE = 0.4\n",
    "WEIGHT_DECAY = 0.001\n",
    "MODE = 'min'\n",
    "PATIENCE = 2\n",
    "FACTOR = 0.5\n",
    "VERBOSE = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:29.418945Z",
     "start_time": "2025-01-30T18:16:29.406510Z"
    }
   },
   "id": "f006aa2984af8778",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/juwieczo/DataspellProjects/meisd_project/pipeline/balanced_augmented_data_primary_intensity.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:29.451539Z",
     "start_time": "2025-01-30T18:16:29.423581Z"
    }
   },
   "id": "c674bfe87925edfa",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#columns = ['Utterances', 'dialog_ids', 'uttr_ids', 'intensity', 'intensity2', 'intensity3']\n",
    "columns = ['Utterances', 'label']\n",
    "df = df[columns].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:29.463940Z",
     "start_time": "2025-01-30T18:16:29.454866Z"
    }
   },
   "id": "8580f2f42c04afad",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101,  2057,  2024,  5604, 14324, 19204, 17629,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "encodings = tokenizer.encode_plus(\n",
    "    test_text,\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=50,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", encodings[\"input_ids\"])\n",
    "print(\"Attention Mask:\", encodings[\"attention_mask\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:30.670271Z",
     "start_time": "2025-01-30T18:16:29.466039Z"
    }
   },
   "id": "45cfdb6c6374e057",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df['Utterances']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.679952Z",
     "start_time": "2025-01-30T18:16:30.672536Z"
    }
   },
   "id": "d6eb97d52a569d49",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        self.targets = self.df['label'].astype(float).values  # Zmieniamy na float\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.float)  # Używamy float\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': target  # Używamy ciągłego celu\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.689239Z",
     "start_time": "2025-01-30T18:16:31.680908Z"
    }
   },
   "id": "8e92c118559e759c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.755031Z",
     "start_time": "2025-01-30T18:16:31.691253Z"
    }
   },
   "id": "c639478fade0aae9",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: (1584, 2)\n",
      "Validation size: (238, 2), Test size: (238, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original train size: {df.shape}\")\n",
    "print(f\"Validation size: {df_valid.shape}, Test size: {df_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.765868Z",
     "start_time": "2025-01-30T18:16:31.758849Z"
    }
   },
   "id": "ff030ef4fe8710fd",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    26.263538\n",
      "1    25.902527\n",
      "3    25.090253\n",
      "2    22.743682\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    291\n",
      "1    287\n",
      "3    278\n",
      "2    252\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = df_train['label'].value_counts()\n",
    "label_frequencies_percent = df_train['label'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.780507Z",
     "start_time": "2025-01-30T18:16:31.768053Z"
    }
   },
   "id": "3594d54cfeb80a4f",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['label']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list = list(df.columns)\n",
    "target_list = target_list[1:]\n",
    "target_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.792277Z",
     "start_time": "2025-01-30T18:16:31.782527Z"
    }
   },
   "id": "5114abfc3ea5f092",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, dropout_rate):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, 1)  # Jedna jednostka wyjściowa\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = output.last_hidden_state[:, 0, :]  # Użycie [CLS] tokena\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return self.out(pooled_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.802204Z",
     "start_time": "2025-01-30T18:16:31.795605Z"
    }
   },
   "id": "7b6de40a4b1fd317",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH, shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.816757Z",
     "start_time": "2025-01-30T18:16:31.806178Z"
    }
   },
   "id": "2c0d1dbac1e2538e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# \n",
    "# def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "#     ds = CustomDataset(\n",
    "#         df=df,\n",
    "#         tokenizer=tokenizer,\n",
    "#         max_len=max_len\n",
    "#     )\n",
    "#     return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "# \n",
    "# train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH)\n",
    "# valid_data_loader = create_data_loader(df_valid, tokenizer, MAX_LEN, BATCH)\n",
    "# test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.826824Z",
     "start_time": "2025-01-30T18:16:31.819899Z"
    }
   },
   "id": "e70914a069b21b54",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_targets = []\n",
    "    total_preds = []\n",
    "    losses = []\n",
    "\n",
    "    for batch in tqdm(data_loader, desc='Training', leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        targets = batch[\"targets\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids).squeeze(-1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        total_targets.extend(targets.cpu().numpy())\n",
    "        total_preds.extend(outputs.cpu().detach().numpy())\n",
    "\n",
    "    mse, mae, r2, pearson_corr = compute_metrics(np.array(total_targets), np.array(total_preds))\n",
    "    return np.mean(losses), mse, mae, r2, pearson_corr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.840986Z",
     "start_time": "2025-01-30T18:16:31.829911Z"
    }
   },
   "id": "6240d7b71759e66",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_targets = []\n",
    "    total_preds = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Validation', leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids).squeeze(-1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            total_targets.extend(targets.cpu().numpy())\n",
    "            total_preds.extend(outputs.cpu().detach().numpy())\n",
    "\n",
    "    mse, mae, r2, pearson_corr = compute_metrics(np.array(total_targets), np.array(total_preds))\n",
    "    return np.mean(losses), mse, mae, r2, pearson_corr\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.854415Z",
     "start_time": "2025-01-30T18:16:31.844168Z"
    }
   },
   "id": "68be400c410542e8",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, device, epochs=10, patience=3):\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        train_loss, train_mse, train_mae, train_r2, train_pearson = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        val_loss, val_mse, val_mae, val_r2, val_pearson = eval_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R2: {train_r2:.4f}, Pearson: {train_pearson:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, R2: {val_r2:.4f}, Pearson: {val_pearson:.4f}\")\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.866982Z",
     "start_time": "2025-01-30T18:16:31.857521Z"
    }
   },
   "id": "37a98d57893457e4",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, mode='min', delta=0):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.epochs_no_improve = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or \\\n",
    "                (self.mode == 'min' and score < self.best_score - self.delta) or \\\n",
    "                (self.mode == 'max' and score > self.best_score + self.delta):\n",
    "            self.best_score = score\n",
    "            self.epochs_no_improve = 0\n",
    "        else:\n",
    "            self.epochs_no_improve += 1\n",
    "            if self.epochs_no_improve >= self.patience:\n",
    "                self.early_stop = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.878887Z",
     "start_time": "2025-01-30T18:16:31.868975Z"
    }
   },
   "id": "3ac3266c28d64f19",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "#     model.train()\n",
    "#     total_targets = []\n",
    "#     total_preds = []\n",
    "#     losses = []\n",
    "# \n",
    "#     for batch in tqdm(data_loader, desc='Training', leave=False):\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "#         targets = batch[\"targets\"].to(device)\n",
    "# \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids, attention_mask, token_type_ids).squeeze(-1)\n",
    "#         loss = loss_fn(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# \n",
    "#         losses.append(loss.item())\n",
    "#         total_targets.extend(targets.cpu().numpy())\n",
    "#         total_preds.extend(outputs.cpu().detach().numpy())\n",
    "# \n",
    "#     mse, mae, r2, pearson_corr = compute_metrics(np.array(total_targets), np.array(total_preds))\n",
    "#     return np.mean(losses), mse, mae, r2, pearson_corr\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.888139Z",
     "start_time": "2025-01-30T18:16:31.880880Z"
    }
   },
   "id": "a78b95dc760caa29",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pearson_corr, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, mae, r2, pearson_corr\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:31.899161Z",
     "start_time": "2025-01-30T18:16:31.891242Z"
    }
   },
   "id": "3152108c8bc1fac0",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\juwieczo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = RegressionModel(pretrained_model_name=PRE_TRAINED_MODEL_NAME, dropout_rate=DROPOUT_RATE)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=MODE, patience=PATIENCE, factor=FACTOR, verbose=VERBOSE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:16:34.684535Z",
     "start_time": "2025-01-30T18:16:31.902307Z"
    }
   },
   "id": "5a8846f4288f100c",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1028, MSE: 2.1055, MAE: 1.1571, R2: -0.6501, Pearson: 0.0220\n",
      "Val Loss: 1.6171, MSE: 1.6089, MAE: 1.0616, R2: -0.2893, Pearson: 0.1315\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5413, MSE: 1.5445, MAE: 1.0563, R2: -0.2104, Pearson: -0.0011\n",
      "Val Loss: 1.3084, MSE: 1.3020, MAE: 1.0125, R2: -0.0434, Pearson: 0.1717\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4115, MSE: 1.4142, MAE: 1.0235, R2: -0.1084, Pearson: 0.0103\n",
      "Val Loss: 1.3890, MSE: 1.3854, MAE: 0.9720, R2: -0.1102, Pearson: 0.2484\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3793, MSE: 1.3811, MAE: 1.0178, R2: -0.0824, Pearson: 0.0299\n",
      "Val Loss: 1.3596, MSE: 1.3527, MAE: 1.0187, R2: -0.0840, Pearson: 0.2794\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4056, MSE: 1.4028, MAE: 1.0268, R2: -0.0994, Pearson: -0.0043\n",
      "Val Loss: 1.3159, MSE: 1.3118, MAE: 0.9799, R2: -0.0512, Pearson: 0.2991\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_model(model, train_data_loader, val_data_loader, loss_fn, optimizer, device, epochs=10, patience=3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T19:30:28.700852Z",
     "start_time": "2025-01-30T18:16:34.686531Z"
    }
   },
   "id": "357bb9dacf1d46e3",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# \n",
    "# PATIENCE = 3 \n",
    "# best_val_loss = float(\"inf\")\n",
    "# epochs_no_improve = 0\n",
    "# \n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=FACTOR, patience=1, verbose=True)\n",
    "# \n",
    "# for epoch in range(EPOCHS):\n",
    "#     print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "# \n",
    "#     train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device)\n",
    "#     val_loss = eval_model(model, val_data_loader, loss_fn, device)\n",
    "# \n",
    "#     print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "# \n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         epochs_no_improve = 0\n",
    "#         torch.save(model.state_dict(), \"best_model.pt\")\n",
    "#         print(\"Model saved!\")\n",
    "#     else:\n",
    "#         epochs_no_improve += 1\n",
    "#         print(f'No improvement for {epochs_no_improve} epoch(s).')\n",
    "# \n",
    "#     scheduler.step(val_loss)\n",
    "# \n",
    "#     # Early Stopping\n",
    "#     if epochs_no_improve >= PATIENCE:\n",
    "#         print(\"Early stopping triggered!\")\n",
    "#         break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T19:30:28.727822Z",
     "start_time": "2025-01-30T19:30:28.711014Z"
    }
   },
   "id": "9e0c8ab71384517e",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "# \n",
    "# test_loss = eval_model(model, test_data_loader, loss_fn, device)\n",
    "# print(f\"Test loss: {test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T19:30:28.775841Z",
     "start_time": "2025-01-30T19:30:28.733793Z"
    }
   },
   "id": "793ace46af81f8e7",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Przyklad wizualizacji\n",
    "# plt.scatter(total_targets, total_preds, alpha=0.5)\n",
    "# plt.xlabel(\"Prawdziwe wartości\")\n",
    "# plt.ylabel(\"Predykcje\")\n",
    "# plt.title(\"Porównanie predykcji z rzeczywistością\")\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T19:30:28.786959Z",
     "start_time": "2025-01-30T19:30:28.779828Z"
    }
   },
   "id": "fa633fc1783b18b8",
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
