{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:03.506842Z",
     "start_time": "2024-12-19T14:50:00.898655Z"
    }
   },
   "id": "61c6d47e5a26b8d7",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "BATCH = 16\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "THRESHOLD = 0.2\n",
    "DROPOUT_RATE = 0.3\n",
    "WEIGHT_DECAY = 0.1\n",
    "MODE = 'min'\n",
    "PATIENCE = 2\n",
    "FACTOR = 0.5\n",
    "VERBOSE = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:03.511019Z",
     "start_time": "2024-12-19T14:50:03.507848Z"
    }
   },
   "id": "88a75ce76d5afc9c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba braków w kolumnie 'intensity': 0\n",
      "Unikalne wartości w kolumnie 'intensity': [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D:/julixus/MEISD/meisd_project/data/MEISD_text.csv')\n",
    "# Zamień wartości na liczby całkowite\n",
    "df['intensity'] = pd.to_numeric(df['intensity'], errors='coerce').fillna(0)\n",
    "df['intensity2'] = pd.to_numeric(df['intensity2'], errors='coerce').fillna(0)\n",
    "df['intensity3'] = pd.to_numeric(df['intensity3'], errors='coerce').fillna(0)\n",
    "\n",
    "# Zamień wartości zawierające tylko białe znaki lub '`', 'neu', 'po' na NaN\n",
    "df['intensity'] = df['intensity'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df['intensity'] = df['intensity'].replace(['`', 'neu', 'po'], np.nan)\n",
    "df['intensity2'] = df['intensity2'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df['intensity2'] = df['intensity2'].replace(['`', 'neu', 'po'], np.nan)\n",
    "df['intensity3'] = df['intensity3'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df['intensity3'] = df['intensity3'].replace(['`', 'neu', 'po'], np.nan)\n",
    "\n",
    "# Użyj forward fill, aby uzupełnić brakujące wartości poprzedzającą wartością\n",
    "df['intensity'] = df['intensity'].ffill()\n",
    "df['intensity2'] = df['intensity2'].ffill()\n",
    "df['intensity3'] = df['intensity3'].ffill()\n",
    "\n",
    "# Usuń znaki niebędące cyframi (np. '`') za pomocą wyrażeń regularnych\n",
    "df['intensity'] = df['intensity'].replace(r'\\D', '', regex=True).astype(int)  # Usuwa wszystko, co nie jest cyfrą\n",
    "df['intensity2'] = df['intensity2'].replace(r'\\D', '', regex=True).astype(int)\n",
    "df['intensity3'] = df['intensity3'].replace(r'\\D', '', regex=True).astype(int)\n",
    "\n",
    "missing_count = df['intensity'].isna().sum()\n",
    "print(f\"Liczba braków w kolumnie 'intensity': {missing_count}\")\n",
    "unique_values = df['intensity'].unique()\n",
    "print(f\"Unikalne wartości w kolumnie 'intensity': {unique_values}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:03.566899Z",
     "start_time": "2024-12-19T14:50:03.512025Z"
    }
   },
   "id": "c32f6bc864fddd97",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns = ['Utterances', 'dialog_ids', 'uttr_ids', 'intensity', 'intensity2', 'intensity3']\n",
    "df = df[columns].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:03.572532Z",
     "start_time": "2024-12-19T14:50:03.567905Z"
    }
   },
   "id": "f489ce945c69132",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julix\\AppData\\Local\\Temp\\ipykernel_1964\\2868896721.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('dialog_ids').apply(process_group)\n"
     ]
    }
   ],
   "source": [
    "first_25_data = []\n",
    "last_25_data = []\n",
    "\n",
    "def process_group(group):\n",
    "    num_rows = len(group)\n",
    "    quarter_size = max(1, num_rows // 4)\n",
    "\n",
    "    # First 25%\n",
    "    first_25 = group.iloc[:quarter_size]\n",
    "    primary_intensity = max(\n",
    "        group['intensity'].iloc[0],\n",
    "        group['intensity2'].iloc[0],\n",
    "        group['intensity3'].iloc[0]\n",
    "    )\n",
    "    first_25 = first_25.assign(primary_intensity=primary_intensity)\n",
    "\n",
    "    # Last 25%\n",
    "    last_25 = group.iloc[-quarter_size:]\n",
    "    final_intensity = max(\n",
    "        group['intensity'].iloc[-1],\n",
    "        group['intensity2'].iloc[-1],\n",
    "        group['intensity3'].iloc[-1]\n",
    "    )\n",
    "    last_25 = last_25.assign(final_intensity=final_intensity)\n",
    "\n",
    "    first_25_data.append(first_25)\n",
    "    last_25_data.append(last_25)\n",
    "\n",
    "df.groupby('dialog_ids').apply(process_group)\n",
    "\n",
    "first_25_df = pd.concat(first_25_data).reset_index(drop=True)\n",
    "last_25_df = pd.concat(last_25_data).reset_index(drop=True)\n",
    "\n",
    "grouped_first_25 = first_25_df.groupby('dialog_ids').agg({\n",
    "    'Utterances': ' '.join,\n",
    "    'primary_intensity': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "grouped_last_25 = last_25_df.groupby('dialog_ids').agg({\n",
    "    'Utterances': ' '.join,\n",
    "    'final_intensity': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "df = grouped_first_25.drop(df.columns[0], axis=1)\n",
    "\n",
    "# grouped_first_25.to_csv('first_25_percent.csv', index=False)\n",
    "# grouped_last_25.to_csv('last_25_percent.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.372119Z",
     "start_time": "2024-12-19T14:50:03.573539Z"
    }
   },
   "id": "e56f08b5653e50db",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   dialog_ids                                         Utterances  \\\n0           1  look around you say hello to your competition ...   \n1           2  i'm george o'malley uh, we met at the mixer. y...   \n2           3  seattle is surrounded by water on three sides ...   \n3           4  yes no other reason? just a favor for an old p...   \n4           5  if he doesn't respond to these tests in the ne...   \n\n   primary_intensity  \n0                  0  \n1                  2  \n2                  1  \n3                  1  \n4                  2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dialog_ids</th>\n      <th>Utterances</th>\n      <th>primary_intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>look around you say hello to your competition ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>i'm george o'malley uh, we met at the mixer. y...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>seattle is surrounded by water on three sides ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>yes no other reason? just a favor for an old p...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>if he doesn't respond to these tests in the ne...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_first_25.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.382288Z",
     "start_time": "2024-12-19T14:50:04.373128Z"
    }
   },
   "id": "f201920484f65a4",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# # Test the tokenizer\n",
    "# test_text = \"We are testing BERT tokenizer.\"\n",
    "# # generate encodings\n",
    "# encodings = tokenizer.encode_plus(test_text,\n",
    "#                                   add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                                   max_length = 50,\n",
    "#                                   truncation = True,\n",
    "#                                   padding = \"max_length\",\n",
    "#                                   return_attention_mask = True,\n",
    "#                                   return_tensors = \"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.385985Z",
     "start_time": "2024-12-19T14:50:04.383295Z"
    }
   },
   "id": "a8049e14d2a5eb2f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   dialog_ids                                         Utterances  \\\n0           1  you're cristina, right? - patton, monroe which...   \n1           2  i have five rules. memorize them rule number o...   \n2           3  \"i'm your sister, i'm your daughter.\" you're s...   \n3           4  just be quick about it you're the one that's s...   \n4           5  people do wake up. that's why we do a series o...   \n\n   final_intensity  \n0                2  \n1                0  \n2                0  \n3                3  \n4                1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dialog_ids</th>\n      <th>Utterances</th>\n      <th>final_intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>you're cristina, right? - patton, monroe which...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>i have five rules. memorize them rule number o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>\"i'm your sister, i'm your daughter.\" you're s...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>just be quick about it you're the one that's s...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>people do wake up. that's why we do a series o...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_last_25.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.393213Z",
     "start_time": "2024-12-19T14:50:04.386991Z"
    }
   },
   "id": "cbdd95a0a34cab18",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# token_lens = []\n",
    "# \n",
    "# for txt in grouped_first_25['Utterances']:\n",
    "#     tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "#     token_lens.append(len(tokens))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.396567Z",
     "start_time": "2024-12-19T14:50:04.394219Z"
    }
   },
   "id": "f6e84cca3a6993fc",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# sns.distplot(token_lens)\n",
    "# plt.xlim([0, 100])\n",
    "# plt.xlabel('Token count')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.399793Z",
     "start_time": "2024-12-19T14:50:04.397574Z"
    }
   },
   "id": "749900659c725beb",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = grouped_first_25"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.403590Z",
     "start_time": "2024-12-19T14:50:04.400880Z"
    }
   },
   "id": "fffaa3ad682825bd",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        self.targets = self.df['primary_intensity'].astype(int).values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),\n",
    "            'utterances': utterances\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.409269Z",
     "start_time": "2024-12-19T14:50:04.403590Z"
    }
   },
   "id": "a7f70271994ce23d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.424403Z",
     "start_time": "2024-12-19T14:50:04.410276Z"
    }
   },
   "id": "e26dd199592b9a07",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: (1124, 3)\n",
      "Validation size: (169, 3), Test size: (169, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original train size: {df.shape}\")\n",
    "print(f\"Validation size: {df_valid.shape}, Test size: {df_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.430082Z",
     "start_time": "2024-12-19T14:50:04.426408Z"
    }
   },
   "id": "bd8a98fbef943116",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary_intensity\n",
      "2    36.259542\n",
      "1    25.954198\n",
      "3    24.045802\n",
      "0    13.740458\n",
      "Name: proportion, dtype: float64\n",
      "primary_intensity\n",
      "2    285\n",
      "1    204\n",
      "3    189\n",
      "0    108\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = df_train['primary_intensity'].value_counts()\n",
    "label_frequencies_percent = df_train['primary_intensity'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.436245Z",
     "start_time": "2024-12-19T14:50:04.431089Z"
    }
   },
   "id": "d7987bc066a1b131",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Utterances', 'primary_intensity']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list = list(df.columns)\n",
    "target_list = target_list[1:]\n",
    "target_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.441019Z",
     "start_time": "2024-12-19T14:50:04.437251Z"
    }
   },
   "id": "b5d2bf8b8703971e",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BERT_IntensityClass(torch.nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BERT_IntensityClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(p=DROPOUT_RATE) #0.5\n",
    "        self.linear = torch.nn.Linear(bert_model.config.hidden_size, 4)\n",
    "        #self.softmax = nn.Softmax(dim=1) #remove for sentiment analysis\n",
    "        #CrossEntropyLoss automatycznie aplikuje funkcję softmax, więc nie ma potrzeby używać Softmax w modelu.\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids=None):\n",
    "        output = self.bert_model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "        #pooler_output = self.pooler_output\n",
    "        dropout_output = self.dropout(output.pooler_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        #output = self.dropout(linear_output)\n",
    "        # output = self.softmax(linear_output)\n",
    "        return linear_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:04.447004Z",
     "start_time": "2024-12-19T14:50:04.442026Z"
    }
   },
   "id": "7e66c5471d9631d7",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = BERT_IntensityClass(bert_model)\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:05.602626Z",
     "start_time": "2024-12-19T14:50:04.448010Z"
    }
   },
   "id": "a90aee7dd6a5312b",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:05.606750Z",
     "start_time": "2024-12-19T14:50:05.603633Z"
    }
   },
   "id": "1ab9f1d0ec1d16f8",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH, shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:05.611409Z",
     "start_time": "2024-12-19T14:50:05.607756Z"
    }
   },
   "id": "8f6bb1de72c32ea0",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8218,  0.1523,  0.0214,  0.9397],\n",
      "        [-0.4001, -0.3687, -1.0602,  0.5248],\n",
      "        [ 0.0544, -0.0565, -0.1296,  0.5058],\n",
      "        [-0.2056, -0.5422,  0.0390,  0.3091],\n",
      "        [-0.6269, -0.2221, -0.6858,  0.1848],\n",
      "        [-0.4890, -0.3220, -0.9971,  0.4721],\n",
      "        [-0.9046, -0.3751, -0.3018, -0.1501],\n",
      "        [-0.8581, -0.0189, -0.3390,  0.4816],\n",
      "        [-0.0285,  0.3266, -0.2241,  0.5947],\n",
      "        [-0.3713, -0.0086, -0.2536,  0.1862],\n",
      "        [-0.3464, -0.2028, -0.0665,  0.2834],\n",
      "        [-0.6835, -0.0222, -0.7268,  0.6274],\n",
      "        [-0.8713, -0.0814, -0.4733,  0.6332],\n",
      "        [-0.8349, -0.4078, -0.5522,  0.3083],\n",
      "        [ 0.0060,  0.0745, -0.4262,  0.0141],\n",
      "        [-0.2284, -0.2526, -0.3686,  0.2936]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "outputs = model(data[\"input_ids\"], attn_mask=data[\"attention_mask\"])\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:07.044073Z",
     "start_time": "2024-12-19T14:50:05.612415Z"
    }
   },
   "id": "d5ca1f227bf8d526",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                                  max_length=50,\n",
    "                                  truncation=True,\n",
    "                                  padding=\"max_length\",\n",
    "                                  return_attention_mask=True,\n",
    "                                  return_tensors=\"pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:07.048451Z",
     "start_time": "2024-12-19T14:50:07.045079Z"
    }
   },
   "id": "472bb4d14d690cb3",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encodings['input_ids'],\n",
    "    attention_mask=encodings['attention_mask']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:07.467298Z",
     "start_time": "2024-12-19T14:50:07.049457Z"
    }
   },
   "id": "5255f143cf8725fd",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.7579, 3.8529, 4.1587, 7.2778])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution = df_train['primary_intensity'].value_counts(normalize=True)\n",
    "total_samples = sum(class_distribution)\n",
    "class_weights = [total_samples / count for count in class_distribution]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:07.473702Z",
     "start_time": "2024-12-19T14:50:07.467298Z"
    }
   },
   "id": "83abf052a0741cf6",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:07.477443Z",
     "start_time": "2024-12-19T14:50:07.474708Z"
    }
   },
   "id": "1bf88e8032b196ae",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:07.616134Z",
     "start_time": "2024-12-19T14:50:07.479450Z"
    }
   },
   "id": "ad813080e544a445",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "#EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=MODE, patience=PATIENCE, factor=FACTOR, verbose=VERBOSE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:08.093735Z",
     "start_time": "2024-12-19T14:50:07.617140Z"
    }
   },
   "id": "30a39e6dab7d1f20",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(training_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.train()\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), leave=True, colour='steelblue')\n",
    "\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Calculate predictions and accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets).item()\n",
    "        num_samples += targets.size(0)\n",
    "\n",
    "        # Collect predictions and labels for F1-score\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    # Calculate F1-score for training data\n",
    "    train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return model, correct_predictions / num_samples, np.mean(losses), train_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:08.099883Z",
     "start_time": "2024-12-19T14:50:08.094741Z"
    }
   },
   "id": "fdf002b23294d72f",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, epoch):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Calculate predictions and accuracy\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == targets).item()\n",
    "            num_samples += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Logowanie do TensorBoard\n",
    "    writer.add_scalar('Loss/validation', avg_loss, epoch)\n",
    "    writer.add_scalar('F1-Score/validation', val_f1, epoch)\n",
    "\n",
    "    return correct_predictions / num_samples, avg_loss, val_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:08.104876Z",
     "start_time": "2024-12-19T14:50:08.100892Z"
    }
   },
   "id": "a99195b89cece76f",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, epoch):\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(f'Confusion Matrix at Epoch {epoch}')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = torch.tensor(np.frombuffer(buf.getvalue(), dtype=np.uint8)).float()\n",
    "    writer.add_image('Confusion Matrix', image, epoch)\n",
    "\n",
    "    plt.close(figure)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T14:50:08.111081Z",
     "start_time": "2024-12-19T14:50:08.105881Z"
    }
   },
   "id": "affa34ced3abe45f",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a066d4020c34119ad85a1a0831ac6f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.7006 | Train accuracy 0.3193 | Train F1 0.3061\n",
      "Val loss 3.2117 | Val accuracy 0.2781 | Val F1 0.1210\n",
      "Saved new best model.\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a2fa09fb55944ccbf95b67c61edec09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.0344 | Train accuracy 0.2557 | Train F1 0.2511\n",
      "Val loss 2.2121 | Val accuracy 0.2781 | Val F1 0.1210\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bcaf7e2bb5d4296ad02c12155885a43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.4029 | Train accuracy 0.2837 | Train F1 0.2742\n",
      "Val loss 2.8984 | Val accuracy 0.3077 | Val F1 0.1448\n",
      "Saved new best model.\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ca632feccd34d259159c9471cd540d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.2360 | Train accuracy 0.2659 | Train F1 0.2612\n",
      "Val loss 2.7495 | Val accuracy 0.3077 | Val F1 0.1448\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24a07e1e05bf49fa92664dbce686e9a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.0763 | Train accuracy 0.3015 | Train F1 0.2901\n",
      "Val loss 3.0991 | Val accuracy 0.2959 | Val F1 0.1351\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3fc356c791f4c6ba6335cb706b6e965"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.9147 | Train accuracy 0.2684 | Train F1 0.2645\n",
      "Val loss 1.4829 | Val accuracy 0.2781 | Val F1 0.1210\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5a9d5b704504d6c8336cf25321ebfdb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.8293 | Train accuracy 0.2697 | Train F1 0.2551\n",
      "Val loss 1.7430 | Val accuracy 0.3077 | Val F1 0.1448\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f0dd9413b5d40e8a71440404501fe91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.7727 | Train accuracy 0.2748 | Train F1 0.2623\n",
      "Val loss 1.2871 | Val accuracy 0.2781 | Val F1 0.1210\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a33e53bc601f476995390bfe87104b8a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.7959 | Train accuracy 0.2748 | Train F1 0.2566\n",
      "Val loss 1.6378 | Val accuracy 0.2781 | Val F1 0.1210\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f3e4a121df74662bf83f9ba2888f586"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.7949 | Train accuracy 0.2646 | Train F1 0.2519\n",
      "Val loss 2.2363 | Val accuracy 0.3077 | Val F1 0.1448\n"
     ]
    }
   ],
   "source": [
    "# Główna pętla treningowa\n",
    "# %%time\n",
    "history = defaultdict(list)\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "\n",
    "    model, train_acc, train_loss, train_f1 = train_model(train_data_loader, model, optimizer)\n",
    "    print(f'Train loss {train_loss:.4f} | Train accuracy {train_acc:.4f} | Train F1 {train_f1:.4f}')\n",
    "\n",
    "    val_acc, val_loss, val_f1 = eval_model(val_data_loader, model, epoch)\n",
    "    print(f'Val loss {val_loss:.4f} | Val accuracy {val_acc:.4f} | Val F1 {val_f1:.4f}')\n",
    "\n",
    "    # Logowanie metryk do TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('F1-Score/train', train_f1, epoch)\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    # Sprawdzenie najlepszej F1 i zapisanie modelu\n",
    "    if val_f1 > best_f1:\n",
    "        torch.save(model.state_dict(), \"best_model_state.bin\")\n",
    "        best_f1 = val_f1\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    scheduler.step(val_loss)  # Tuning LR\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T15:13:49.468034Z",
     "start_time": "2024-12-19T14:50:08.112087Z"
    }
   },
   "id": "aa4135f847993eba",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T15:13:49.471229Z",
     "start_time": "2024-12-19T15:13:49.469039Z"
    }
   },
   "id": "33e1aa162763d9af",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a72238f3b1baa3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
