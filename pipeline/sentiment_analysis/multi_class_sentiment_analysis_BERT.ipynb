{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from googletrans import Translator\n",
    "from deep_translator import GoogleTranslator\n",
    "import random\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73bc3114f800d2e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#df_data = pd.read_csv('multi_label_binarizer_MEISD.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea967bdf16d7aac8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('balanced_augmented_data.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47137d58e541c92a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c3cd1ff6b557b79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For the multiclass classification we use:\n",
    "columns = ['Utterances', 'sentiment_0', 'sentiment_1', 'sentiment_2']\n",
    "multi_columns = df_data[columns].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f03d650c0c3ab9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multi_columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5ac6592e7d0d12c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data['label'] = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].idxmax(axis=1)\n",
    "df_data['label'] = df_data['label'].apply(lambda x: int(x.split('_')[1]))\n",
    "df_data = df_data[['Utterances', 'label']]\n",
    "df_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b5263ecbce69516",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b3904b6bd11e66c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# Test the tokenizer\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "# generate encodings\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                  max_length = 50,\n",
    "                                  truncation = True,\n",
    "                                  padding = \"max_length\",\n",
    "                                  return_attention_mask = True,\n",
    "                                  return_tensors = \"pt\")\n",
    "# we get a dictionary with three keys (see: https://huggingface.co/transformers/glossary.html) \n",
    "encodings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4583ff41d066406d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.sep_token, tokenizer.sep_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4e96b75767cb52b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.cls_token, tokenizer.cls_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6fce1b218dd6f7c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c80afdaab10a97e6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "910c4ed511357e43",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df_data['Utterances']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bcbe8d5956e2a8c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel('Token count')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82787cfc362ec257",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 30 #50 #128  # wiekszosc tokenow zdaje sie byc ponizej 40, klasycznie wklada sie tu 256, my przystaniemy na 30\n",
    "TRAIN_BATCH_SIZE = 16 #8 #16 #32 \n",
    "#Czasami, przy bardzo niskim tempie uczenia i zbyt dużych batchach, model może wolniej konwergować. Spróbuj zmniejszyć wielkość batcha, np. z 16 do 8.\n",
    "VALID_BATCH_SIZE = 16 #8 #16 #32\n",
    "TEST_BATCH_SIZE = 16 #8 #16 #32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.00001 #1e-05\n",
    "# Ustawienie bardzo niskiego współczynnika uczenia (np. 1e-05) może spowodować, że model uczy się bardzo wolno, co prowadzi do sytuacji, w której po wielu epokach nie ma znaczącej poprawy w wynikach walidacji.\n",
    "\n",
    "THRESHOLD = 0.2 # threshold for the sigmoid\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4169540c849c5cc3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "# 1. Synonym Replacement\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words[:]\n",
    "    num_replacements = max(1, len(words) // 5)  # Replace about 20% of words\n",
    "    random_words = random.sample(words, num_replacements)\n",
    "\n",
    "    for word in random_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            #synonym = synonyms[0].lemmas()[0].name()  # Take first synonym\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            if synonym != word:  # Avoid replacement if the synonym is identical\n",
    "                new_words = [synonym if w == word else w for w in new_words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "# 2. Random Insertion\n",
    "def random_insertion(text, n=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        new_word = random.choice(words)\n",
    "        insert_pos = random.randint(0, len(words))\n",
    "        words.insert(insert_pos, new_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# 3. Random Deletion\n",
    "def random_deletion(text, p=0.3):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text  # Avoid deleting single-word text\n",
    "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    if not new_words:\n",
    "        return random.choice(words)  # Return one word if all words are deleted\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# 4. Back Translation\n",
    "def back_translation(text, src_lang='en', mid_lang='fr', max_retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=src_lang, target=mid_lang).translate(text)\n",
    "            back_translated = GoogleTranslator(source=mid_lang, target=src_lang).translate(translated)\n",
    "            return back_translated\n",
    "        except Exception as e:\n",
    "            print(f\"Back translation error on attempt {attempt + 1}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(1)\n",
    "    raise ValueError(\"Back translation failed\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df8410a8704c0089",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_text(text, num_augments=2):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        augmentation_choice = random.choice(['synonym', 'insertion', 'deletion', 'back_translation'])\n",
    "        if augmentation_choice == 'synonym':\n",
    "            augmented_texts.append(synonym_replacement(text))\n",
    "        elif augmentation_choice == 'insertion':\n",
    "            augmented_texts.append(random_insertion(text))\n",
    "        elif augmentation_choice == 'deletion':\n",
    "            augmented_texts.append(random_deletion(text))\n",
    "        elif augmentation_choice == 'back_translation':\n",
    "            augmented_texts.append(back_translation(text))\n",
    "    return augmented_texts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e0bdc2c2e2c2b44",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df_data, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "830a2b086cc6cd51",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_counts = df_train['label'].value_counts()\n",
    "max_count = class_counts.max()\n",
    "\n",
    "augmented_data = {'Utterances': [], 'label': []}\n",
    "\n",
    "for label in class_counts.index:\n",
    "    class_subset = df_train[df_train['label'] == label]\n",
    "    augmented_data['Utterances'].extend(class_subset['Utterances'])\n",
    "    augmented_data['label'].extend(class_subset['label'])\n",
    "\n",
    "    num_to_augment = max_count - len(class_subset)\n",
    "    for _, row in tqdm(class_subset.iterrows(), total=num_to_augment, desc=f\"Augmenting class {label}\"):\n",
    "        if num_to_augment <= 0:\n",
    "            break\n",
    "        new_texts = augment_text(row['Utterances'], num_augments=2)\n",
    "        for new_text in new_texts:\n",
    "            if num_to_augment <= 0:\n",
    "                break\n",
    "            augmented_data['Utterances'].append(new_text)\n",
    "            augmented_data['label'].append(label)\n",
    "            num_to_augment -= 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c754ed339547d7bc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "873a02f811b5b489",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "augmented_df.to_csv('balanced_augmented_data.csv', index=False)\n",
    "print(\"Augmented data saved to 'balanced_augmented_data.csv'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "704a320f25d50fab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# augmented_data = {'Utterances': [], 'label': []}\n",
    "# \n",
    "# for index, row in tqdm(df_train.iterrows(), total=len(df_train), desc=\"Augmenting data\"):\n",
    "#     original_text = row['Utterances']\n",
    "#     sentiment_labels = row[['label']].tolist()\n",
    "#     #emotion_labels = row[['emotion_1', 'emotion_2', 'emotion_3', 'emotion_4', 'emotion_5', 'emotion_6', 'emotion_7', 'emotion_8', 'emotion_9']].tolist()\n",
    "#     #intensity_labels = row[['intensity_1', 'intensity_2', 'intensity_3']].tolist()\n",
    "# \n",
    "#     augmented_data['Utterances'].append(original_text)\n",
    "#     augmented_data['label'].append(sentiment_labels)\n",
    "#     #augmented_data['Emotion_Labels'].append(emotion_labels)\n",
    "#     #augmented_data['Intensity_Labels'].append(intensity_labels)\n",
    "# \n",
    "#     new_texts = augment_text(original_text, num_augments=2)\n",
    "#     for new_text in new_texts:\n",
    "#         augmented_data['Utterances'].append(new_text)\n",
    "#         augmented_data['label'].append(sentiment_labels)  # Kopiuj etykiety do nowych przykładów\n",
    "#         #augmented_data['Emotion_Labels'].append(emotion_labels)\n",
    "#         #augmented_data['Intensity_Labels'].append(intensity_labels)\n",
    "# \n",
    "# augmented_df = pd.DataFrame(augmented_data)\n",
    "# #augmented_df.to_csv('augmented_data.csv', index=False)\n",
    "# #print(\"Augmented data saved to 'augmented_data.csv'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae95de2a84419ff8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# augmented_df.to_csv('augmented_data.csv', index=False)\n",
    "# print(\"Augmented data saved to 'augmented_data.csv'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "477c96b92f2d9584",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Original train size: {df_train.shape}\")\n",
    "#print(f\"Augmented train size: {augmented_df.shape}\")\n",
    "\n",
    "df_train = augmented_df\n",
    "\n",
    "print(f\"Validation size: {df_valid.shape}, Test size: {df_test.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d731d55496bb4d21",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Original train size: {df_train.shape}\")\n",
    "#print(f\"Augmented train size: {augmented_df.shape}\")\n",
    "\n",
    "# df_train = augmented_df\n",
    "\n",
    "print(f\"Validation size: {df_valid.shape}, Test size: {df_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18268b76af386902",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_frequencies = df_train['label'].value_counts()\n",
    "label_frequencies_percent = df_train['label'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a65c8fb4f51be18",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#columns = multi_columns.columns\n",
    "#categor_freq = multi_columns[columns[1:]].sum() / multi_columns.shape[0]\n",
    "#categor_freq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1869914ba83249b7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "#print(class_distribution)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48368f8e1c2dbc24",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_distribution = df_train['label'].value_counts(normalize=True)\n",
    "print(class_distribution)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc61b895ba4d755c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wykres rozkładu klas\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Sentiment Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fed0bbea9aac8025",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# plt.rcParams[\"figure.figsize\"] = (15, 3)\n",
    "# plt.bar(categor_freq.index, categor_freq.values)\n",
    "# _ = plt.xticks(rotation=45)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc155bc55d6fc25d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Train: {df_train.shape}, Test: {df_test.shape}, Valid: {df_valid.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c04b06158609497",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        # Upewnij się, że etykiety są typu całkowitego (int)\n",
    "        self.targets = self.df['label'].astype(int).values \n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])  # 'index' jest prawidłowe\n",
    "        #utterances = \" \".join(utterances.split())  # Usuwa niepotrzebne białe znaki\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.long)  # Zapewnij typ long\n",
    "        # print(f\"Target dtype: {target.dtype}\")  # Debugging\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),  # Zapewnij typ long\n",
    "            'utterances': utterances\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3fcc26e8264f157",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target_list = list(df_data.columns)\n",
    "target_list = target_list[1:]\n",
    "target_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73bfd1a79a83c61a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6ba75832e16a38",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=TRAIN_BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=0\n",
    "                                                )\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                              batch_size=VALID_BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=0\n",
    "                                              )\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                               batch_size=TEST_BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0\n",
    "                                               )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12d6b5e711313422",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e81afbd88da7754",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(data.keys())\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac8f47a2b68e7431",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encodings['input_ids'],\n",
    "    attention_mask=encodings['attention_mask']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f51dd2fe8bb830a6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model.config.hidden_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb9bbe80508adf34",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BERTSentimentClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSentimentClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3) #0.5\n",
    "        self.linear = torch.nn.Linear(bert_model.config.hidden_size, 3)\n",
    "        #self.softmax = nn.Softmax(dim=1) #remove for sentiment analysis\n",
    "        #CrossEntropyLoss automatycznie aplikuje funkcję softmax, więc nie ma potrzeby używać Softmax w modelu.\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "        #pooler_output = self.pooler_output\n",
    "        dropout_output = self.dropout(output.pooler_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        #output = self.dropout(linear_output)\n",
    "        # output = self.softmax(linear_output)\n",
    "        return linear_output\n",
    "\n",
    "model = BERTSentimentClass()\n",
    "\n",
    "# # Freezing BERT layers:\n",
    "#for name, param in model.bert_model.named_parameters():\n",
    "#    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "#        param.requires_grad = True\n",
    "#    else:\n",
    "#        param.requires_grad = False\n",
    "\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b173035f9a453ad",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7384a11eae5618c1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "832cdad2864e4307"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "#total_samples = sum(class_distribution)\n",
    "#class_weights = [total_samples / count for count in class_distribution]\n",
    "#class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "#class_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea3b10f21b6bef29",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_distribution = df_train['label'].value_counts(normalize=True)\n",
    "total_samples = sum(class_distribution)\n",
    "class_weights = [total_samples / count for count in class_distribution]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76c59d85dbe5fc53",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)\n",
    "#change for sentiment analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e254869914b6694",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "# Harmonogram zmiany learning rate\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b88113f1705b015b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "#EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "# Learning Rate Tuning\n",
    "#total_steps = len(train_data_loader) * EPOCHS\n",
    "#scheduler = get_linear_schedule_with_warmup(\n",
    "#    optimizer,\n",
    "#    num_warmup_steps=0,\n",
    "#    num_training_steps=total_steps\n",
    "#)\n",
    "#loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e99ccc9880f86131",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ce5b61268865d691"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regularization (Dodanie Weight Decay)\n",
    "Weight decay to inaczej L2 regularizacja, która zapobiega nadmiernemu dopasowaniu (overfittingowi). W Adam możesz dodać weight decay w następujący sposób:\n",
    "\n",
    "Dodanie Weight Decay do Optimizera: Ustaw wartość weight decay w optymalizatorze. Standardowe wartości mieszczą się w zakresie 1e-4 do 1e-5:\n",
    "\n",
    "python code\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "Analiza Wyników: Weight decay zmniejsza wielkość wag w czasie, co stabilizuje uczenie. Po dodaniu weight decay zwróć uwagę na to, czy train_loss i val_loss są bliżej siebie – powinny się zbliżyć, co jest oznaką redukcji overfittingu.\n",
    "\n",
    "Aby zwiększyć regularizację, możesz zastosować tzw. weight decay, czyli L2 regularizację, podczas tworzenia optymalizatora. weight decay dodaje karę (regularizację) na wagi modelu, co może pomóc w ograniczeniu przeuczenia (overfitting). Możesz kontrolować siłę regularizacji za pomocą wartości weight_decay przy tworzeniu optymalizatora, takiego jak Adam.\n",
    "\n",
    "Jak ustawić weight decay w optymalizatorze?\n",
    "Podczas inicjalizacji optymalizatora, dodaj parametr weight_decay i przypisz mu wartość, która określi siłę regularizacji. Przykładowo, weight_decay=0.01 to dobry punkt wyjścia, ale możesz testować różne wartości, takie jak 0.001, 0.01, czy 0.1, aby znaleźć optymalną dla Twojego modelu.\n",
    "\n",
    "# Inicjalizacja optymalizatora Adam z regularizacją L2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "Rola weight_decay\n",
    "Wartość weight_decay=0.01 oznacza umiarkowaną regularizację.\n",
    "Wyższe wartości, np. weight_decay=0.1, zwiększają regularizację, co może zmniejszyć przeuczenie, ale też obniżyć dokładność na zbiorze treningowym.\n",
    "Niższe wartości, np. weight_decay=0.001 lub niższe, mają mniejszy wpływ na regularizację, ale mogą pomóc, jeśli model już ma dobrą dokładność na zbiorze walidacyjnym.\n",
    "\n",
    "Wybór Najlepszej Wartości\n",
    "Wybierz wartość weight_decay, która daje najlepszą równowagę między niskim val_loss a wysoką val_acc. Pamiętaj, że wyższe wartości weight_decay zwiększają regularizację, co może zmniejszyć overfitting, ale też potencjalnie obniżyć zdolność modelu do dopasowania się do danych.\n",
    "\n",
    "Zależność między learning rate a weight_decay\n",
    "Wyższe wartości weight_decay (np. 1e-2) wymagają zazwyczaj niższego learning rate, ponieważ większa regularizacja silniej wpływa na wagę parametrów, co stabilizuje model i zapobiega overfittingowi.\n",
    "Z kolei niższe wartości weight_decay (np. 1e-5 lub 1e-6) pozwalają na większą swobodę dopasowania modelu do danych i mogą lepiej działać przy nieco wyższym learning rate (np. 2e-5 lub 3e-5).\n",
    "\n",
    "\n",
    "7. Dalsze Kroki\n",
    "Layer-Wise Learning Rate: Możesz również eksperymentować z różnymi learning rates dla różnych warstw, co może jeszcze bardziej poprawić wyniki.\n",
    "Early Stopping: Dodaj mechanizm wczesnego zatrzymania treningu, aby uniknąć nadmiernego trenowania modelu.\n",
    "Eksperymenty z Hiperparametrami: Możesz rozważyć korzystanie z narzędzi do automatycznego tuningu hiperparametrów, takich jak Optuna czy Ray Tune, aby systematycznie znaleźć optymalne wartości.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79bc7b7db3c67783"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Rate Tuning\n",
    "\n",
    "Adaptacyjna redukcja learning rate za pomocą torch.optim.lr_scheduler.ReduceLROnPlateau pozwala obniżyć learning rate, kiedy model przestaje poprawiać wyniki. Możemy to zrobić w ten sposób:\n",
    "\n",
    "Inicjalizacja Optimizera i Schedulera\": przy optymalizatorze AdamW (lub innego). Po zdefiniowaniu optymalizatora dodaj ReduceLROnPlateau:\n",
    "\n",
    "python code\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Początkowy learning rate\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Początkowy learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "    mode='min': Zmniejsza learning rate, gdy minimalizowana jest wartość (np. val_loss).\n",
    "    patience=2: Liczba epok bez poprawy przed obniżeniem learning rate.\n",
    "    factor=0.5: Po jakim czynniku obniżyć learning rate (np. z 1e-3 na 5e-4).\n",
    "    verbose=True: Informuje w logach o zmianie learning rate.\n",
    "\n",
    "Wykorzystanie Schedulera podczas trenowania: Po zakończeniu każdej epoki przekaż do schedulera wynik val_loss, by sprawdzić, czy learning rate wymaga obniżenia:\n",
    "\n",
    "python code\n",
    "        scheduler.step(val_loss)\n",
    "Użycie scheduler.step(val_loss) automatycznie sprawi, że learning rate zostanie zmniejszony w miarę potrzeby.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44e396491a02203"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(training_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.train()\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), leave=True, colour='steelblue')\n",
    "\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Calculate predictions and accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets).item()\n",
    "        num_samples += targets.size(0)\n",
    "\n",
    "        # Collect predictions and labels for F1-score\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    # Calculate F1-score for training data\n",
    "    train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return model, correct_predictions / num_samples, np.mean(losses), train_f1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dca06c411b50ef06",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, epoch):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Calculate predictions and accuracy\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == targets).item()\n",
    "            num_samples += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Logowanie do TensorBoard\n",
    "    writer.add_scalar('Loss/validation', avg_loss, epoch)\n",
    "    writer.add_scalar('F1-Score/validation', val_f1, epoch)\n",
    "\n",
    "    return correct_predictions / num_samples, avg_loss, val_f1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e92ef02b9630acb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, epoch):\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(f'Confusion Matrix at Epoch {epoch}')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = torch.tensor(np.frombuffer(buf.getvalue(), dtype=np.uint8)).float()\n",
    "    writer.add_image('Confusion Matrix', image, epoch)\n",
    "\n",
    "    plt.close(figure)  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5033a49d36c4c44e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c1f9311c7351447",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "patience = 3  # Ile epok czekasz na poprawę\n",
    "patience_counter = 0\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3581dd085ee1067b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Główna pętla treningowa\n",
    "# %%time\n",
    "history = defaultdict(list)\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "\n",
    "    model, train_acc, train_loss, train_f1 = train_model(train_data_loader, model, optimizer)\n",
    "    print(f'Train loss {train_loss:.4f} | Train accuracy {train_acc:.4f} | Train F1 {train_f1:.4f}')\n",
    "\n",
    "    val_acc, val_loss, val_f1 = eval_model(val_data_loader, model, epoch)\n",
    "    print(f'Val loss {val_loss:.4f} | Val accuracy {val_acc:.4f} | Val F1 {val_f1:.4f}')\n",
    "\n",
    "    # Logowanie metryk do TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('F1-Score/train', train_f1, epoch)\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    # Sprawdzenie najlepszej F1 i zapisanie modelu\n",
    "    if val_f1 > best_f1:\n",
    "        torch.save(model.state_dict(), \"best_model_state.bin\")\n",
    "        best_f1 = val_f1\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    scheduler.step(val_loss)  # Tuning LR\n",
    "\n",
    "writer.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f188f1f8f1e365d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Overfitting:\n",
    "Niski train_loss (0.2796) wskazuje na to, że model dobrze dopasowuje się do danych treningowych, ale wysoki val_loss (2.9933) oraz brak poprawy w val_acc (0.0000) sugerują, że model może się przeuczać, czyli dopasowuje się zbyt mocno do danych treningowych i traci zdolność do generalizacji na danych testowych.\n",
    "Rozwiązanie:\n",
    "Dodanie technik regularyzacyjnych, jak dropout, L2 regularization.\n",
    "Wykorzystanie większego zbioru danych.\n",
    "Zastosowanie wcześniejszego zatrzymania (early stopping), aby przerwać trening, gdy model zaczyna się przeuczać.\n",
    "# 2. Zbyt niski learning rate:\n",
    "Ustawienie bardzo niskiego współczynnika uczenia (np. 1e-05) może spowodować, że model uczy się bardzo wolno, co prowadzi do sytuacji, w której po wielu epokach nie ma znaczącej poprawy w wynikach walidacji.\n",
    "Rozwiązanie: Spróbuj zwiększyć learning rate np. do 1e-04 i zobacz, czy poprawia to wyniki. Zbyt niski współczynnik uczenia może blokować osiąganie optymalnych wyników.\n",
    "# 3. Zbyt skomplikowany model:\n",
    "Jeśli model jest zbyt złożony w stosunku do dostępnych danych, może to prowadzić do overfittingu. Model nauczy się bardzo dobrze danych treningowych, ale nie będzie w stanie dobrze generalizować.\n",
    "Rozwiązanie: Możesz spróbować uprościć model (np. mniejsza liczba warstw, mniejsza liczba neuronów) lub zebrać większy zbiór danych, jeśli to możliwe.\n",
    "# 4. Problemy z danymi:\n",
    "Dane walidacyjne mogą zawierać problemy, takie jak błędnie oznaczone próbki, brak różnorodności, lub mogą nie być reprezentatywne dla danych treningowych.\n",
    "Rozwiązanie: Sprawdź, czy dane walidacyjne są dobrze zrównoważone i poprawnie oznaczone. Ewentualnie przetestuj na innym zbiorze walidacyjnym.\n",
    "# 5. Złe inicjalizacje wag lub problemy z optymalizacją:\n",
    "Wysoki val_loss i brak poprawy w val_acc mogą wskazywać na problemy z optymalizacją. Np. złe inicjalizacje wag lub nieodpowiedni optymalizator mogą powodować, że model nie znajduje optymalnych rozwiązań.\n",
    "Rozwiązanie: Spróbuj zmienić optymalizator (np. Adam na RMSprop), lub zastosować inne techniki inicjalizacji wag.\n",
    "# 6. Zbyt zróżnicowane klasy:\n",
    "Jeśli Twoje klasy są bardzo niezrównoważone, to model może mieć problem z nauczeniem się klasyfikacji rzadkich klas.\n",
    "Rozwiązanie: Upewnij się, że klasy są zrównoważone lub użyj metod radzenia sobie z niezrównoważonymi danymi (np. class weights w funkcji straty).\n",
    "\n",
    "## 7. Optymalizacja i liczba epok: \n",
    "Jeśli model nie uczy się z oczekiwaną prędkością, możesz rozważyć tuning hiperparametrów, jak np. zmniejszenie wartości learning rate, co może pomóc modelowi lepiej uczyć się przy niższych wartościach początkowych. Możesz też wydłużyć trening, jeśli osiągalne wartości są powolne, ale stopniowo poprawiające się.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa92c9ea6f194565"
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://developers.google.com/machine-learning/crash-course/overfitting/regularization#early_stopping_an_alternative_to_complexity-based_regularization\n",
    "\n",
    "Picking the regularization rate\n",
    "The ideal regularization rate produces a model that generalizes well to new, previously unseen data. Unfortunately, that ideal value is data-dependent, so you must do some tuning.\n",
    "\n",
    "Early stopping: an alternative to complexity-based regularization\n",
    "Early stopping is a regularization method that doesn't involve a calculation of complexity. Instead, early stopping simply means ending training before the model fully converges. For example, you end training when the loss curve for the validation set starts to increase (slope becomes positive).\n",
    "\n",
    "Although early stopping usually increases training loss, it can decrease test loss.\n",
    "\n",
    "Early stopping is a quick, but rarely optimal, form of regularization. The resulting model is very unlikely to be as good as a model trained thoroughly on the ideal regularization rate.\n",
    "\n",
    "Finding equilibrium between learning rate and regularization rate\n",
    "Learning rate and regularization rate tend to pull weights in opposite directions. A high learning rate often pulls weights away from zero; a high regularization rate pulls weights towards zero.\n",
    "\n",
    "If the regularization rate is high with respect to the learning rate, the weak weights tend to produce a model that makes poor predictions. Conversely, if the learning rate is high with respect to the regularization rate, the strong weights tend to produce an overfit model.\n",
    "\n",
    "Your goal is to find the equilibrium between learning rate and regularization rate. This can be challenging. Worst of all, once you find that elusive balance, you may have to ultimately change the learning rate. And, when you change the learning rate, you'll again have to find the ideal regularization rate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3019f67aedc2d335"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f1c49aca7626a1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_xlabel(\"Predicted labels\")\n",
    "    ax.set_ylabel(\"True labels\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_yticklabels(class_names)\n",
    "    return figF\n",
    "\n",
    "# Tworzenie confusion matrix po ewaluacji\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Logowanie confusion matrix jako obraz\n",
    "fig = plot_confusion_matrix(cm, class_names=['class0', 'class1', 'class2'])\n",
    "writer.add_figure('Confusion matrix', fig, epoch)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd274764070cb547",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5303ae57d6d9c2f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5f07c30edc7ba933"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
