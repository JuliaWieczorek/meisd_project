{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:52:34.094177Z",
     "start_time": "2024-11-12T15:52:33.547574Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Julix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Julix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = pd.read_csv('multi_label_binarizer_MEISD.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:52:47.692924Z",
     "start_time": "2024-11-12T15:52:47.659493Z"
    }
   },
   "id": "f0215a512fa1e780",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                        Utterances  sentiment_0  sentiment_1  \\\n0                                  look around you            0            0   \n1                    say hello to your competition            0            0   \n2  eight of you will switch to an easier specialty            0            0   \n3        five of you will crack under the pressure            0            0   \n4                two of you will be asked to leave            0            0   \n\n   sentiment_2  emotion_1  emotion_2  emotion_3  emotion_4  emotion_5  \\\n0            1          1          0          0          0          0   \n1            1          1          0          0          0          0   \n2            1          1          0          0          0          0   \n3            1          1          0          0          0          0   \n4            1          1          0          0          0          0   \n\n   emotion_6  emotion_7  emotion_8  emotion_9  intensity_1  intensity_2  \\\n0          0          0          0          0            0            0   \n1          0          0          0          0            0            0   \n2          0          0          0          0            0            0   \n3          0          0          0          0            0            0   \n4          0          0          0          0            0            0   \n\n   intensity_3  \n0            0  \n1            0  \n2            0  \n3            0  \n4            0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Utterances</th>\n      <th>sentiment_0</th>\n      <th>sentiment_1</th>\n      <th>sentiment_2</th>\n      <th>emotion_1</th>\n      <th>emotion_2</th>\n      <th>emotion_3</th>\n      <th>emotion_4</th>\n      <th>emotion_5</th>\n      <th>emotion_6</th>\n      <th>emotion_7</th>\n      <th>emotion_8</th>\n      <th>emotion_9</th>\n      <th>intensity_1</th>\n      <th>intensity_2</th>\n      <th>intensity_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>look around you</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>say hello to your competition</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eight of you will switch to an easier specialty</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>five of you will crack under the pressure</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>two of you will be asked to leave</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:53:08.388423Z",
     "start_time": "2024-11-12T15:53:08.378695Z"
    }
   },
   "id": "a5430b4aae4bd977",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Utterances', 'sentiment_0', 'sentiment_1', 'sentiment_2', 'emotion_1',\n       'emotion_2', 'emotion_3', 'emotion_4', 'emotion_5', 'emotion_6',\n       'emotion_7', 'emotion_8', 'emotion_9', 'intensity_1', 'intensity_2',\n       'intensity_3'],\n      dtype='object')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:55:11.345659Z",
     "start_time": "2024-11-12T15:55:11.341287Z"
    }
   },
   "id": "1dd96ea744dd69c8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                        Utterances\n0                                  look around you\n1                    say hello to your competition\n2  eight of you will switch to an easier specialty\n3        five of you will crack under the pressure\n4                two of you will be asked to leave",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Utterances</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>look around you</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>say hello to your competition</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eight of you will switch to an easier specialty</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>five of you will crack under the pressure</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>two of you will be asked to leave</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_to_drop = ['sentiment_0', 'sentiment_1', 'sentiment_2', 'emotion_1',\n",
    "                'emotion_2', 'emotion_3', 'emotion_4', 'emotion_5', 'emotion_6',\n",
    "                'emotion_7', 'emotion_8', 'emotion_9', 'intensity_1', 'intensity_2',\n",
    "                'intensity_3']\n",
    "data.drop(list_to_drop, axis=1, inplace=True)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:55:58.945709Z",
     "start_time": "2024-11-12T15:55:58.939062Z"
    }
   },
   "id": "b17f5be8cdb1573d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples to be used is : 20017\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of examples to be used is : {len(data)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:56:06.725302Z",
     "start_time": "2024-11-12T15:56:06.722304Z"
    }
   },
   "id": "5ad388123900d676",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Julix\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:59:49.999020Z",
     "start_time": "2024-11-12T15:59:49.526475Z"
    }
   },
   "id": "8f2ccebf8450ab7b",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "\n",
    "    return list(synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:56:33.877605Z",
     "start_time": "2024-11-12T15:56:33.873428Z"
    }
   },
   "id": "6b3ce973aff5d8bc",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = []\n",
    "for w in stopwords.words('english'):\n",
    "    stop_words.append(w)\n",
    "print(stop_words)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:56:42.810919Z",
     "start_time": "2024-11-12T15:56:42.801522Z"
    }
   },
   "id": "cc1458063b7c266e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def synonym_replacement(words, n):\n",
    "    words = words.split()\n",
    "\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "\n",
    "        if num_replaced >= n:  #only replace up to n words\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:56:53.070489Z",
     "start_time": "2024-11-12T15:56:53.065850Z"
    }
   },
   "id": "9f3df794582d7f0f",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example of Synonym Replacement: hey military man how are you doing\n"
     ]
    }
   ],
   "source": [
    "print(f\" Example of Synonym Replacement: {synonym_replacement('hey man how are you doing',3)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T15:59:56.919404Z",
     "start_time": "2024-11-12T15:59:56.869690Z"
    }
   },
   "id": "f3d08874f612579f",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is your arena\n"
     ]
    }
   ],
   "source": [
    "trial_sent = data['Utterances'][6]\n",
    "print(trial_sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:01:19.383813Z",
     "start_time": "2024-11-12T16:01:19.380326Z"
    }
   },
   "id": "929708f87d39c656",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example of Synonym Replacement: this is your bowl\n",
      " Example of Synonym Replacement: this is your sports stadium\n",
      " Example of Synonym Replacement: this is your sphere\n"
     ]
    }
   ],
   "source": [
    "# Create 3 Augmented Sentences per data \n",
    "\n",
    "for n in range(3):\n",
    "    print(f\" Example of Synonym Replacement: {synonym_replacement(trial_sent, n)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:01:20.015848Z",
     "start_time": "2024-11-12T16:01:20.012778Z"
    }
   },
   "id": "f5fa1685b314d385",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def random_insertion(words, n):\n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:01:21.423264Z",
     "start_time": "2024-11-12T16:01:21.420128Z"
    }
   },
   "id": "2988fea710ad2911",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "\n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words) - 1)\n",
    "    new_words.insert(random_idx, random_synonym)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:01:21.769701Z",
     "start_time": "2024-11-12T16:01:21.765807Z"
    }
   },
   "id": "55688a6a600e3b18",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this make up is your arena\n",
      "bowl this is your bowl arena\n",
      "bowl this make up bowl is your arena\n"
     ]
    }
   ],
   "source": [
    "print(random_insertion(trial_sent, 1))\n",
    "print(random_insertion(trial_sent, 2))\n",
    "print(random_insertion(trial_sent, 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:01:22.055198Z",
     "start_time": "2024-11-12T16:01:22.051171Z"
    }
   },
   "id": "46ab339d18db76e",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def aug(sent, n, p):\n",
    "    print(f\" Original Sentence : {sent}\")\n",
    "    print(f\" SR Augmented Sentence : {synonym_replacement(sent, n)}\")\n",
    "    print(f\" RI Augmented Sentence : {random_insertion(sent, n)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:01:22.345893Z",
     "start_time": "2024-11-12T16:01:22.342270Z"
    }
   },
   "id": "be439074474d20b1",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original Sentence : this is your arena\n",
      " SR Augmented Sentence : this is your stadium\n",
      " RI Augmented Sentence : this is bowl your make up bowl arena arena\n"
     ]
    }
   ],
   "source": [
    "aug(trial_sent,4,0.3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:01:22.671692Z",
     "start_time": "2024-11-12T16:01:22.668108Z"
    }
   },
   "id": "a502c8266aae9c47",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "223d206d52407f76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "My part"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea865ddc58f51787"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Julix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Julix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from googletrans import Translator\n",
    "from deep_translator import GoogleTranslator\n",
    "import random\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "translator = Translator()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T17:10:48.528724Z",
     "start_time": "2024-11-12T17:10:47.956063Z"
    }
   },
   "id": "a62d9214c210c2e7",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1. Synonym Replacement\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words[:]\n",
    "    num_replacements = max(1, len(words) // 5)  # Replace about 20% of words\n",
    "    random_words = random.sample(words, num_replacements)\n",
    "\n",
    "    for word in random_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()  # Take first synonym\n",
    "            if synonym != word:  # Avoid replacement if the synonym is identical\n",
    "                new_words = [synonym if w == word else w for w in new_words]\n",
    "    return ' '.join(new_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:33:08.997933Z",
     "start_time": "2024-11-12T16:33:08.993328Z"
    }
   },
   "id": "af1296afb6e76278",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 2. Random Insertion\n",
    "def random_insertion(text, n=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        new_word = random.choice(words)\n",
    "        insert_pos = random.randint(0, len(words))\n",
    "        words.insert(insert_pos, new_word)\n",
    "    return ' '.join(words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:33:17.645761Z",
     "start_time": "2024-11-12T16:33:17.642093Z"
    }
   },
   "id": "5462ad5983766cf8",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 3. Random Deletion\n",
    "def random_deletion(text, p=0.3):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text  # Avoid deleting single-word text\n",
    "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    if not new_words:\n",
    "        return random.choice(words)  # Return one word if all words are deleted\n",
    "    return ' '.join(new_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:33:34.227010Z",
     "start_time": "2024-11-12T16:33:34.222951Z"
    }
   },
   "id": "22188e984382bd56",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def back_translation(text, src_lang='en', mid_lang='fr'):\n",
    "#     try:\n",
    "#         translated = translator.translate(text, src=src_lang, dest=mid_lang).text\n",
    "#         back_translated = translator.translate(translated, src=mid_lang, dest=src_lang).text\n",
    "#         return back_translated\n",
    "#     except Exception as e:\n",
    "#         print(f\"Back translation error: {e}\")\n",
    "#         return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:33:42.068818Z",
     "start_time": "2024-11-12T16:33:42.065938Z"
    }
   },
   "id": "6e149635ce891f3d",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 4. Back Translation\n",
    "def back_translation(text, src_lang='en', mid_lang='fr', max_retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=src_lang, target=mid_lang).translate(text)\n",
    "            back_translated = GoogleTranslator(source=mid_lang, target=src_lang).translate(translated)\n",
    "            return back_translated\n",
    "        except Exception as e:\n",
    "            print(f\"Back translation error on attempt {attempt + 1}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(1)  \n",
    "    raise ValueError(\"Back translation failed\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:48:39.641822Z",
     "start_time": "2024-11-12T16:48:39.636795Z"
    }
   },
   "id": "9fa5affe503027d5",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = \"I'm totally forgettable\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:33:48.482086Z",
     "start_time": "2024-11-12T16:33:48.479474Z"
    }
   },
   "id": "ce10a3398b13a9a1",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back translation error: 'NoneType' object has no attribute 'group'\n"
     ]
    }
   ],
   "source": [
    "# Apply augmentations\n",
    "augmented_texts = [\n",
    "    synonym_replacement(text),\n",
    "    random_insertion(text),\n",
    "    random_deletion(text),\n",
    "    back_translation(text)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:33:59.236478Z",
     "start_time": "2024-11-12T16:33:58.753468Z"
    }
   },
   "id": "7a7ee0b339411aff",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I'm totally forgettable\n",
      "Synonym Replacement: I'm wholly forgettable\n",
      "Random Insertion: forgettable I'm totally forgettable\n",
      "Random Deletion: I'm totally\n",
      "Back Translation: I'm totally forgettable\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\", text)\n",
    "print(\"Synonym Replacement:\", augmented_texts[0])\n",
    "print(\"Random Insertion:\", augmented_texts[1])\n",
    "print(\"Random Deletion:\", augmented_texts[2])\n",
    "print(\"Back Translation:\", augmented_texts[3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:34:30.787073Z",
     "start_time": "2024-11-12T16:34:30.783304Z"
    }
   },
   "id": "e7a1f3b6613b0a2",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('multi_label_binarizer_MEISD.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:39:27.215488Z",
     "start_time": "2024-11-12T16:39:27.192026Z"
    }
   },
   "id": "120a80b71ac49f6d",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_text(text, num_augments=2):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        augmentation_choice = random.choice(['synonym', 'insertion', 'deletion', 'back_translation'])\n",
    "        if augmentation_choice == 'synonym':\n",
    "            augmented_texts.append(synonym_replacement(text))\n",
    "        elif augmentation_choice == 'insertion':\n",
    "            augmented_texts.append(random_insertion(text))\n",
    "        elif augmentation_choice == 'deletion':\n",
    "            augmented_texts.append(random_deletion(text))\n",
    "        elif augmentation_choice == 'back_translation':\n",
    "            augmented_texts.append(back_translation(text))\n",
    "    return augmented_texts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:39:32.192404Z",
     "start_time": "2024-11-12T16:39:32.188672Z"
    }
   },
   "id": "c3edeb39c68daaa8",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "augmented_data = {'Utterances': [], 'Sentiment_Labels': [], 'Emotion_Labels': [], 'Intensity_Labels': []}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:41:53.727779Z",
     "start_time": "2024-11-12T16:41:53.724719Z"
    }
   },
   "id": "65e24f81852964f6",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting data:   1%|          | 216/20017 [01:49<2:47:37,  1.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m augmented_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEmotion_Labels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(emotion_labels)\n\u001B[0;32m     10\u001B[0m augmented_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIntensity_Labels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(intensity_labels)\n\u001B[1;32m---> 12\u001B[0m new_texts \u001B[38;5;241m=\u001B[39m augment_text(original_text, num_augments\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m new_text \u001B[38;5;129;01min\u001B[39;00m new_texts:\n\u001B[0;32m     14\u001B[0m     augmented_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUtterances\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(new_text)\n",
      "Cell \u001B[1;32mIn[39], line 12\u001B[0m, in \u001B[0;36maugment_text\u001B[1;34m(text, num_augments)\u001B[0m\n\u001B[0;32m     10\u001B[0m         augmented_texts\u001B[38;5;241m.\u001B[39mappend(random_deletion(text))\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m augmentation_choice \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mback_translation\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 12\u001B[0m         augmented_texts\u001B[38;5;241m.\u001B[39mappend(back_translation(text))\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m augmented_texts\n",
      "Cell \u001B[1;32mIn[45], line 7\u001B[0m, in \u001B[0;36mback_translation\u001B[1;34m(text, src_lang, mid_lang, max_retries)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m      6\u001B[0m     translated \u001B[38;5;241m=\u001B[39m GoogleTranslator(source\u001B[38;5;241m=\u001B[39msrc_lang, target\u001B[38;5;241m=\u001B[39mmid_lang)\u001B[38;5;241m.\u001B[39mtranslate(text)\n\u001B[1;32m----> 7\u001B[0m     back_translated \u001B[38;5;241m=\u001B[39m GoogleTranslator(source\u001B[38;5;241m=\u001B[39mmid_lang, target\u001B[38;5;241m=\u001B[39msrc_lang)\u001B[38;5;241m.\u001B[39mtranslate(translated)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m back_translated\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\deep_translator\\google.py:67\u001B[0m, in \u001B[0;36mGoogleTranslator.translate\u001B[1;34m(self, text, **kwargs)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpayload_key:\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url_params[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpayload_key] \u001B[38;5;241m=\u001B[39m text\n\u001B[1;32m---> 67\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_base_url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url_params, proxies\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproxies\n\u001B[0;32m     69\u001B[0m )\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m429\u001B[39m:\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRequests()\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\requests\\api.py:73\u001B[0m, in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m request(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, params\u001B[38;5;241m=\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\requests\\sessions.py:747\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    744\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[1;32m--> 747\u001B[0m     r\u001B[38;5;241m.\u001B[39mcontent\n\u001B[0;32m    749\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\requests\\models.py:899\u001B[0m, in \u001B[0;36mResponse.content\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    897\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 899\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_content(CONTENT_CHUNK_SIZE)) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    902\u001B[0m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[0;32m    903\u001B[0m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\requests\\models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[1;34m()\u001B[0m\n\u001B[0;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    815\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 816\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    817\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    818\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\urllib3\\response.py:931\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m    915\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001B[39;00m\n\u001B[0;32m    917\u001B[0m \u001B[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;124;03m    'content-encoding' header.\u001B[39;00m\n\u001B[0;32m    929\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    930\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunked \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupports_chunked_reads():\n\u001B[1;32m--> 931\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_chunked(amt, decode_content\u001B[38;5;241m=\u001B[39mdecode_content)\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    933\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\urllib3\\response.py:1071\u001B[0m, in \u001B[0;36mHTTPResponse.read_chunked\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m   1068\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1071\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_chunk_length()\n\u001B[0;32m   1072\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_left \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1073\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\urllib3\\response.py:999\u001B[0m, in \u001B[0;36mHTTPResponse._update_chunk_length\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    997\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_left \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    998\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 999\u001B[0m line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline()  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[0;32m   1000\u001B[0m line \u001B[38;5;241m=\u001B[39m line\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\conda\\Lib\\socket.py:707\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 707\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[0;32m    708\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    709\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\Lib\\ssl.py:1253\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1249\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1250\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1251\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1252\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread(nbytes, buffer)\n\u001B[0;32m   1254\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32mD:\\conda\\Lib\\ssl.py:1105\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1104\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1105\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[0;32m   1106\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1107\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Augmenting data\"):\n",
    "    original_text = row['Utterances']\n",
    "    sentiment_labels = row[['sentiment_0', 'sentiment_1', 'sentiment_2']].tolist()\n",
    "    emotion_labels = row[['emotion_1', 'emotion_2', 'emotion_3', 'emotion_4', 'emotion_5', 'emotion_6', 'emotion_7', 'emotion_8', 'emotion_9']].tolist()\n",
    "    intensity_labels = row[['intensity_1', 'intensity_2', 'intensity_3']].tolist()\n",
    "\n",
    "    augmented_data['Utterances'].append(original_text)\n",
    "    augmented_data['Sentiment_Labels'].append(sentiment_labels)\n",
    "    augmented_data['Emotion_Labels'].append(emotion_labels)\n",
    "    augmented_data['Intensity_Labels'].append(intensity_labels)\n",
    "\n",
    "    new_texts = augment_text(original_text, num_augments=2)\n",
    "    for new_text in new_texts:\n",
    "        augmented_data['Utterances'].append(new_text)\n",
    "        augmented_data['Sentiment_Labels'].append(sentiment_labels)  # Kopiuj etykiety do nowych przykładów\n",
    "        augmented_data['Emotion_Labels'].append(emotion_labels)\n",
    "        augmented_data['Intensity_Labels'].append(intensity_labels)\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "augmented_df.to_csv('augmented_data.csv', index=False)\n",
    "print(\"Augmented data saved to 'augmented_data.csv'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T17:15:17.068055Z",
     "start_time": "2024-11-12T17:13:26.745818Z"
    }
   },
   "id": "4bfe78758f0c1cc4",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d902e74a5bd7cdc5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b2751447f4df5ffa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "LLM vs. traditional\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1d621a8eabf0c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('C:/Users/juwieczo/DataspellProjects/meisd_project/data/filtered_negative_MEISD_intensity_max_first_25_conv.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a00505dd6b7ed34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_frequencies = df_data['max_intensity'].value_counts()\n",
    "label_frequencies_percent = df_data['max_intensity'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ffaf468929aac5f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data['label'] = (df_data['max_intensity'] == 2).astype(int)\n",
    "#df_data['label'] = (df_data['label'] == 2).astype(int)\n",
    "\n",
    "columns = ['Utterances', 'label']\n",
    "df = df_data[columns].copy()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f3cdf3dbf8fad93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "#dataset = load_data(\"D:/julixus/MEISD/meisd_project/data/ESConv.json\")\n",
    "dataset = load_data(\"C:/Users/juwieczo/DataspellProjects/meisd_project/data/ESConv.json\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a583ce7a9cc06225"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 24\u001B[0m\n\u001B[0;32m     17\u001B[0m         result\u001B[38;5;241m.\u001B[39mappend({\n\u001B[0;32m     18\u001B[0m             key: entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurvey_score\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseeker\u001B[39m\u001B[38;5;124m'\u001B[39m][key],\n\u001B[0;32m     19\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdialog\u001B[39m\u001B[38;5;124m'\u001B[39m: selected_dialog\n\u001B[0;32m     20\u001B[0m         })\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m---> 24\u001B[0m first_25_percent \u001B[38;5;241m=\u001B[39m extract_seeker_data(\u001B[43mdataset\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minitial_emotion_intensity\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m#last_25_percent = extract_seeker_data(dataset, 'final_emotion_intensity')\u001B[39;00m\n\u001B[0;32m     27\u001B[0m first_25_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(first_25_percent)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_seeker_data(data, key):\n",
    "    result = []\n",
    "\n",
    "    for entry in data:\n",
    "        dialog = entry['dialog']\n",
    "        seeker_dialog = [item['content'].strip() for item in dialog if item['speaker'] == 'seeker']\n",
    "\n",
    "        quarter_length = max(1, len(seeker_dialog) // 4)\n",
    "\n",
    "        if key == 'initial_emotion_intensity':\n",
    "            selected_dialog = seeker_dialog[:quarter_length]\n",
    "        elif key == 'final_emotion_intensity':\n",
    "            selected_dialog = seeker_dialog[-quarter_length:]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        result.append({\n",
    "            key: entry['survey_score']['seeker'][key],\n",
    "            'dialog': selected_dialog\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n",
    "first_25_percent = extract_seeker_data(dataset, 'initial_emotion_intensity')\n",
    "#last_25_percent = extract_seeker_data(dataset, 'final_emotion_intensity')\n",
    "\n",
    "first_25_df = pd.DataFrame(first_25_percent)\n",
    "#last_25_df = pd.DataFrame(last_25_percent)\n",
    "\n",
    "label_counts = first_25_df['initial_emotion_intensity'].value_counts()\n",
    "least_common_label = label_counts.idxmin()\n",
    "first_25_df = first_25_df[first_25_df['initial_emotion_intensity'] != least_common_label]\n",
    "first_25_df['initial_emotion_intensity'] = pd.to_numeric(first_25_df['initial_emotion_intensity'], errors='coerce')\n",
    "first_25_df['initial_emotion_intensity'] = first_25_df['initial_emotion_intensity'] - 2\n",
    "\n",
    "first_25_df.rename(columns={\n",
    "    'dialog': 'Utterances',\n",
    "    'initial_emotion_intensity': 'label'\n",
    "})\n",
    "df_data['label'] = (df_data['max_intensity'] == 2).astype(int)\n",
    "columns = ['Utterances', 'label']\n",
    "df = df_data[columns].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-17T19:54:31.963744Z",
     "start_time": "2025-06-17T19:54:31.539229Z"
    }
   },
   "id": "c2e8a3e8a6601f08",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1. Synonym Replacement\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words[:]\n",
    "    num_replacements = max(1, len(words) // 5)  # Replace about 20% of words\n",
    "    random_words = random.sample(words, num_replacements)\n",
    "\n",
    "    for word in random_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            #synonym = synonyms[0].lemmas()[0].name()  # Take first synonym\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            if synonym != word:  # Avoid replacement if the synonym is identical\n",
    "                new_words = [synonym if w == word else w for w in new_words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "# 2. Random Insertion\n",
    "def random_insertion(text, n=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        new_word = random.choice(words)\n",
    "        insert_pos = random.randint(0, len(words))\n",
    "        words.insert(insert_pos, new_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# 3. Random Deletion\n",
    "def random_deletion(text, p=0.3):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text  # Avoid deleting single-word text\n",
    "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    if not new_words:\n",
    "        return random.choice(words)  # Return one word if all words are deleted\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "# 4. Back Translation\n",
    "def back_translation(text, src_lang='en', mid_lang='fr', max_retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=src_lang, target=mid_lang).translate(text)\n",
    "            back_translated = GoogleTranslator(source=mid_lang, target=src_lang).translate(translated)\n",
    "            return back_translated\n",
    "        except Exception as e:\n",
    "            print(f\"Back translation error on attempt {attempt + 1}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(1)\n",
    "    raise ValueError(\"Back translation failed\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-17T19:54:31.965742Z"
    }
   },
   "id": "3f98673cbbde4f4c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"C:/Users/juwieczo/DataspellProjects/meisd_project/chatbot/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "            n_ctx=2048, n_threads=6, verbose=True)\n",
    "\n",
    "\n",
    "def generate_esconv_style_response(original_text, examples):\n",
    "    prompt = f\"\"\"\n",
    "You are an emotional support assistant. Below are example supportive messages from real conversations:\n",
    "{examples}\n",
    "\n",
    "Now, rewrite the following message in a similar supportive tone:\n",
    "\"{original_text}\"\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "    output = llm(prompt, max_tokens=150, temperature=0.8, stop=[\"User:\", \"Assistant:\"])\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20f2783c9566cbff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "def extract_seeker_utterances(json_path, max_dialogs=20, max_examples=5):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    seeker_lines = []\n",
    "    for convo in data[:max_dialogs]:  # ogranicz dla szybkości\n",
    "        for turn in convo.get(\"dialog\", []):\n",
    "            if turn.get(\"speaker\") == \"seeker\":\n",
    "                content = turn.get(\"content\", \"\").strip()\n",
    "                if 20 < len(content) < 150:  # odfiltrowanie krótkich i bardzo długich\n",
    "                    seeker_lines.append(content)\n",
    "\n",
    "    random.shuffle(seeker_lines)\n",
    "    selected = seeker_lines[:max_examples]\n",
    "    formatted = \"\\n\".join([f\"- {line}\" for line in selected])\n",
    "    return formatted\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39384f868412e0a8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "style_examples = extract_seeker_utterances(\"ESConv.json\")\n",
    "print(style_examples)\n",
    "text = \"Nobody listens to me, I feel completely ignored.\"\n",
    "print(generate_esconv_style_response(text, style_examples))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bbbac1f7e167d21"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_text(text, num_augments=2, mode='mixed'):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        if mode == 'llm':\n",
    "            augmented_texts.append(generate_esconv_style_response(text, style_examples))\n",
    "        elif mode == 'classic':\n",
    "            choice = random.choice(['synonym', 'insertion', 'deletion', 'back_translation'])\n",
    "            if choice == 'synonym':\n",
    "                augmented_texts.append(synonym_replacement(text))\n",
    "            elif choice == 'insertion':\n",
    "                augmented_texts.append(random_insertion(text))\n",
    "            elif choice == 'deletion':\n",
    "                augmented_texts.append(random_deletion(text))\n",
    "            elif choice == 'back_translation':\n",
    "                augmented_texts.append(back_translation(text))\n",
    "        elif mode == 'mixed':\n",
    "            if random.random() < 0.5:\n",
    "                augmented_texts.append(generate_esconv_style_response(text, style_examples))\n",
    "            else:\n",
    "                augmented_texts.append(random_deletion(text))  # lub random.choice z klasycznych\n",
    "    return augmented_texts\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42616726513033b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_binary_data(df, label_column, augment_text, num_augments=2):\n",
    "    \"\"\"\n",
    "    Augments binary classification data to balance class distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'Utterances' column and a binary label column.\n",
    "    - label_column (str): Column name of the binary target label (0 or 1).\n",
    "    - augment_text (callable): Function to augment text. Should take a string and return a list of augmented strings.\n",
    "    - num_augments (int): Number of augmented samples to generate per original sample.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Augmented DataFrame with balanced class distribution.\n",
    "    \"\"\"\n",
    "    # Oblicz liczność klas\n",
    "    class_counts = df[label_column].value_counts()\n",
    "    min_class, max_class = class_counts.idxmin(), class_counts.idxmax()\n",
    "    num_min, num_max = class_counts[min_class], class_counts[max_class]\n",
    "\n",
    "    print(f\"Liczność klas przed augmentacją: {class_counts.to_dict()}\")\n",
    "\n",
    "    # Pobierz próbki z mniejszej klasy\n",
    "    class_subset = df[df[label_column] == min_class].copy()\n",
    "\n",
    "    # Oblicz ile dodatkowych próbek potrzebujemy\n",
    "    num_to_add = num_max - num_min\n",
    "\n",
    "    # Inicjalizacja nowego zbioru danych\n",
    "    augmented_data = {'Utterances': [], label_column: []}\n",
    "\n",
    "    # Augmentuj dane, ale tylko do momentu wyrównania liczby próbek\n",
    "    augment_per_sample = max(1, num_to_add // len(class_subset))  # Ile augmentacji na 1 próbkę\n",
    "    remaining = num_to_add  # Ile jeszcze próbek musimy dodać\n",
    "\n",
    "    for _, row in tqdm(class_subset.iterrows(), total=len(class_subset), desc=f\"Augmenting class {min_class}\"):\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "\n",
    "        # Wykonaj augmentację tekstu\n",
    "        new_texts = augment_text(row['Utterances'], num_augments=min(augment_per_sample, remaining))\n",
    "\n",
    "        for new_text in new_texts:\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            augmented_data['Utterances'].append(new_text)\n",
    "            augmented_data[label_column].append(min_class)\n",
    "            remaining -= 1  # Zmniejsz licznik brakujących próbek\n",
    "\n",
    "    # Tworzenie DataFrame z nowymi próbkami\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "    # Połączenie oryginalnych danych z nowymi danymi\n",
    "    final_df = pd.concat([df, augmented_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Sprawdź finalny rozkład klas\n",
    "    final_counts = final_df[label_column].value_counts()\n",
    "    print(f\"Liczność klas po augmentacji: {final_counts.to_dict()}\")\n",
    "\n",
    "    return final_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f8a59428bc8838"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def augment_binary_data_percent(df, label_column, augment_text, augment_percent=25):\n",
    "    \"\"\"\n",
    "    Augments binary classification data by a specified percentage.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'Utterances' column and a binary label column.\n",
    "    - label_column (str): Column name of the binary target label (0 or 1).\n",
    "    - augment_text (callable): Function to augment text. Should take a string and return a list of augmented strings.\n",
    "    - augment_percent (int): Percentage increase for each class (e.g., 25 means adding 25% more samples per class).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Augmented DataFrame with increased class distributions.\n",
    "    \"\"\"\n",
    "    # Oblicz liczność klas\n",
    "    class_counts = df[label_column].value_counts()\n",
    "    print(f\"Liczność klas przed augmentacją: {class_counts.to_dict()}\")\n",
    "\n",
    "    # Inicjalizacja nowego zbioru danych\n",
    "    augmented_data = {'Utterances': [], label_column: []}\n",
    "\n",
    "    for label in class_counts.index:\n",
    "        class_subset = df[df[label_column] == label].copy()\n",
    "        num_to_add = int(class_counts[label] * (augment_percent / 100))\n",
    "\n",
    "        augment_per_sample = max(1, num_to_add // len(class_subset))\n",
    "        remaining = num_to_add\n",
    "\n",
    "        for _, row in tqdm(class_subset.iterrows(), total=len(class_subset), desc=f\"Augmenting class {label}\"):\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "\n",
    "            new_texts = augment_text(row['Utterances'], num_augments=min(augment_per_sample, remaining))\n",
    "\n",
    "            for new_text in new_texts:\n",
    "                if remaining <= 0:\n",
    "                    break\n",
    "                augmented_data['Utterances'].append(new_text)\n",
    "                augmented_data[label_column].append(label)\n",
    "                remaining -= 1\n",
    "\n",
    "    # Tworzenie DataFrame z nowymi próbkami\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "    # Połączenie oryginalnych danych z nowymi danymi\n",
    "    final_df = pd.concat([df, augmented_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Sprawdź finalny rozkład klas\n",
    "    final_counts = final_df[label_column].value_counts()\n",
    "    print(f\"Liczność klas po augmentacji: {final_counts.to_dict()}\")\n",
    "\n",
    "    return final_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61204f3dea9af865"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_text(text, num_augments=2, use_llm_local=False):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        if use_llm_local and random.random() < 0.5:\n",
    "            augmented_texts.append(generate_esconv_style_response(text, style_examples))\n",
    "        else:\n",
    "            # ... klasyczne augmentacje\n",
    "            augmented_texts.append(random_deletion(text))\n",
    "    return augmented_texts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44624a16234321a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_classic = augment_binary_data_percent(df=original_df, label_column='label', augment_text=lambda t, n: augment_text(t, n, mode='classic'), augment_percent=70)\n",
    "\n",
    "df_llm = augment_binary_data_percent(df=original_df, label_column='label', augment_text=lambda t, n: augment_text(t, n, mode='llm'), augment_percent=70)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cb6444d7ad5fd06"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "## Augmentation\n",
    "augmented_df = augment_binary_data(\n",
    "    df=df,\n",
    "    label_column='label',\n",
    "    augment_text=augment_text,\n",
    "    num_augments=2\n",
    ")\n",
    "\n",
    "print(augmented_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238bc32343e8188a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = augment_binary_data_percent(\n",
    "    df=augmented_df,\n",
    "    label_column='label',\n",
    "    augment_text=augment_text,\n",
    "    augment_percent=70\n",
    ")\n",
    "\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c058aa61d979158"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(df, label_column='label'):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['Utterances'], df[label_column], test_size=0.2, random_state=42)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "    return classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Porównanie\n",
    "classic_metrics = evaluate_model(df_classic)\n",
    "llm_metrics = evaluate_model(df_llm)\n",
    "\n",
    "print(\"Classic augmentation:\")\n",
    "print(classic_metrics)\n",
    "\n",
    "print(\"LLM-based augmentation:\")\n",
    "print(llm_metrics)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a62efce1fbd6b969"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
