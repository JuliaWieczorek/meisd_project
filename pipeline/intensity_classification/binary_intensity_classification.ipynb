{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-24T18:56:04.401427Z",
     "start_time": "2025-06-24T18:55:57.928237Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'httpcore' has no attribute 'SyncHTTPTransport'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wordnet\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdeep_translator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GoogleTranslator\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogletrans\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Translator\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\__init__.py:6\u001B[0m\n\u001B[0;32m      2\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTranslator\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      3\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m3.0.0\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogletrans\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Translator\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogletrans\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LANGCODES, LANGUAGES  \u001B[38;5;66;03m# noqa\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\client.py:25\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogletrans\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Translated, Detected\n\u001B[0;32m     22\u001B[0m EXCLUDES \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mca\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfr\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;43;01mclass\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43;01mTranslator\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;250;43m    \u001B[39;49m\u001B[38;5;124;43;03m\"\"\"Google Translate ajax API implementation class\u001B[39;49;00m\n\u001B[0;32m     27\u001B[0m \n\u001B[0;32m     28\u001B[0m \u001B[38;5;124;43;03m    You have to create an instance of Translator to use this API\u001B[39;49;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;124;43;03m    :type raise_exception: boolean\u001B[39;49;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;124;43;03m    \"\"\"\u001B[39;49;00m\n\u001B[0;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_urls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEFAULT_USER_AGENT\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mraise_exception\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEFAULT_RAISE_EXCEPTION\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyping\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttpcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSyncHTTPTransport\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mTimeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\client.py:55\u001B[0m, in \u001B[0;36mTranslator\u001B[1;34m()\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTranslator\u001B[39;00m:\n\u001B[0;32m     26\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Google Translate ajax API implementation class\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \n\u001B[0;32m     28\u001B[0m \u001B[38;5;124;03m    You have to create an instance of Translator to use this API\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;124;03m    :type raise_exception: boolean\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, service_urls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, user_agent\u001B[38;5;241m=\u001B[39mDEFAULT_USER_AGENT,\n\u001B[0;32m     54\u001B[0m                  raise_exception\u001B[38;5;241m=\u001B[39mDEFAULT_RAISE_EXCEPTION,\n\u001B[1;32m---> 55\u001B[0m                  proxies: typing\u001B[38;5;241m.\u001B[39mDict[\u001B[38;5;28mstr\u001B[39m, \u001B[43mhttpcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSyncHTTPTransport\u001B[49m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, timeout: Timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     57\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient \u001B[38;5;241m=\u001B[39m httpx\u001B[38;5;241m.\u001B[39mClient()\n\u001B[0;32m     58\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m proxies \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# pragma: nocover\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'httpcore' has no attribute 'SyncHTTPTransport'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel#, AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from deep_translator import GoogleTranslator\n",
    "from googletrans import Translator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "translator = Translator()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "BATCH = 32\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased' #\"roberta-base\" #'bert-base-cased'\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.00001 # dla MEISD\n",
    "LEARNING_RATE_FINE = 5e-6    # dla fine-tuningu na ESConv\n",
    "THRESHOLD = 0.2 #prog decyzyjny\n",
    "DROPOUT_RATE = 0.3\n",
    "WEIGHT_DECAY = 0.001\n",
    "LSTM_LAYERS = 2\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "FOCAL_LOSS_ALFA = 4\n",
    "FOCAL_LOSS_GAMMA = 2\n",
    "MODE='min'\n",
    "PATIENCE=2\n",
    "FACTOR=0.5\n",
    "VERBOSE=True\n",
    "output_dir = './fine_tuned_bert_lstm_model'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:35.084145Z",
     "start_time": "2025-06-12T13:45:35.076306Z"
    }
   },
   "id": "7a7d5ffe51d43f10",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('C:/Users/juwieczo/DataspellProjects/meisd_project/data/filtered_negative_MEISD_intensity_max_first_25_conv.csv')\n",
    "#df_data = pd.read_csv('C:/Users/juwieczo/DataspellProjects/meisd_project/pipeline/balanced_augmented_data_primary_intensity.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:35.572711Z",
     "start_time": "2025-06-12T13:45:35.559982Z"
    }
   },
   "id": "7511ba9431cb25f7",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   dialog_ids                                         Utterances  \\\n0           1                                        like i said   \n1           2             now you think i'm gay. no, i'm not gay   \n2           3                         now i have to like it here   \n3           4  yes no other reason? just a favor for an old p...   \n4           5  if he doesn't respond to these tests in the ne...   \n\n   max_intensity  \n0              1  \n1              1  \n2              2  \n3              2  \n4              2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dialog_ids</th>\n      <th>Utterances</th>\n      <th>max_intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>like i said</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>now you think i'm gay. no, i'm not gay</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>now i have to like it here</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>yes no other reason? just a favor for an old p...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>if he doesn't respond to these tests in the ne...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:35.947872Z",
     "start_time": "2025-06-12T13:45:35.928605Z"
    }
   },
   "id": "85b5b15638a903f5",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_intensity\n",
      "2    49.677419\n",
      "1    29.032258\n",
      "3    21.290323\n",
      "Name: proportion, dtype: float64\n",
      "max_intensity\n",
      "2    539\n",
      "1    315\n",
      "3    231\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = df_data['max_intensity'].value_counts()\n",
    "label_frequencies_percent = df_data['max_intensity'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:36.441626Z",
     "start_time": "2025-06-12T13:45:36.431969Z"
    }
   },
   "id": "132893087c52c8a6",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data['label'] = (df_data['max_intensity'] == 2).astype(int)\n",
    "#df_data['label'] = (df_data['label'] == 2).astype(int)\n",
    "\n",
    "columns = ['Utterances', 'label']\n",
    "df = df_data[columns].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:36.716073Z",
     "start_time": "2025-06-12T13:45:36.705560Z"
    }
   },
   "id": "e27346da902b31a4",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1. Synonym Replacement\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words[:]\n",
    "    num_replacements = max(1, len(words) // 5)  # Replace about 20% of words\n",
    "    random_words = random.sample(words, num_replacements)\n",
    "\n",
    "    for word in random_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            #synonym = synonyms[0].lemmas()[0].name()  # Take first synonym\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            if synonym != word:  # Avoid replacement if the synonym is identical\n",
    "                new_words = [synonym if w == word else w for w in new_words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "# 2. Random Insertion\n",
    "def random_insertion(text, n=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        new_word = random.choice(words)\n",
    "        insert_pos = random.randint(0, len(words))\n",
    "        words.insert(insert_pos, new_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# 3. Random Deletion\n",
    "def random_deletion(text, p=0.3):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text  # Avoid deleting single-word text\n",
    "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    if not new_words:\n",
    "        return random.choice(words)  # Return one word if all words are deleted\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# 4. Back Translation\n",
    "def back_translation(text, src_lang='en', mid_lang='fr', max_retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=src_lang, target=mid_lang).translate(text)\n",
    "            back_translated = GoogleTranslator(source=mid_lang, target=src_lang).translate(translated)\n",
    "            return back_translated\n",
    "        except Exception as e:\n",
    "            print(f\"Back translation error on attempt {attempt + 1}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(1)\n",
    "    raise ValueError(\"Back translation failed\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:36.896407Z",
     "start_time": "2025-06-12T13:45:36.885266Z"
    }
   },
   "id": "7fcc085f5564eeb6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"C:/Users/juwieczo/DataspellProjects/meisd_project/chatbot/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, n_threads=6, verbose=True)\n",
    "\n",
    "def generate_esconv_style_response(original_text, examples):\n",
    "    prompt = f\"\"\"\n",
    "You are an emotional support assistant. Below are example supportive messages from real conversations:\n",
    "{examples}\n",
    "\n",
    "Now, rewrite the following message in a similar supportive tone:\n",
    "\"{original_text}\"\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "    output = llm(prompt, max_tokens=150, temperature=0.8, stop=[\"User:\", \"Assistant:\"])\n",
    "    return output[\"choices\"][0][\"text\"].strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6ae327ba3ff72a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def extract_seeker_utterances(json_path, max_dialogs=20, max_examples=5):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    seeker_lines = []\n",
    "    for convo in data[:max_dialogs]:  # ogranicz dla szybkości\n",
    "        for turn in convo.get(\"dialog\", []):\n",
    "            if turn.get(\"speaker\") == \"seeker\":\n",
    "                content = turn.get(\"content\", \"\").strip()\n",
    "                if 20 < len(content) < 150:  # odfiltrowanie krótkich i bardzo długich\n",
    "                    seeker_lines.append(content)\n",
    "\n",
    "    random.shuffle(seeker_lines)\n",
    "    selected = seeker_lines[:max_examples]\n",
    "    formatted = \"\\n\".join([f\"- {line}\" for line in selected])\n",
    "    return formatted\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc3d3d3388af510a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "style_examples = extract_seeker_utterances(\"ESConv.json\")\n",
    "print(style_examples)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87f76c2e96547f5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = \"Nobody listens to me, I feel completely ignored.\"\n",
    "print(generate_esconv_style_response(text, style_examples))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "747ce2c7f70234c0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_text(text, num_augments=2):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        augmentation_choice = random.choice(['synonym', 'insertion', 'deletion', 'back_translation'])\n",
    "        if augmentation_choice == 'synonym':\n",
    "            augmented_texts.append(synonym_replacement(text))\n",
    "        elif augmentation_choice == 'insertion':\n",
    "            augmented_texts.append(random_insertion(text))\n",
    "        elif augmentation_choice == 'deletion':\n",
    "            augmented_texts.append(random_deletion(text))\n",
    "        elif augmentation_choice == 'back_translation':\n",
    "            augmented_texts.append(back_translation(text))\n",
    "    return augmented_texts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:37.067443Z",
     "start_time": "2025-06-12T13:45:37.059186Z"
    }
   },
   "id": "507e1faa1a6e9329",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def augment_binary_data(df, label_column, augment_text, num_augments=2):\n",
    "    \"\"\"\n",
    "    Augments binary classification data to balance class distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'Utterances' column and a binary label column.\n",
    "    - label_column (str): Column name of the binary target label (0 or 1).\n",
    "    - augment_text (callable): Function to augment text. Should take a string and return a list of augmented strings.\n",
    "    - num_augments (int): Number of augmented samples to generate per original sample.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Augmented DataFrame with balanced class distribution.\n",
    "    \"\"\"\n",
    "    # Oblicz liczność klas\n",
    "    class_counts = df[label_column].value_counts()\n",
    "    min_class, max_class = class_counts.idxmin(), class_counts.idxmax()\n",
    "    num_min, num_max = class_counts[min_class], class_counts[max_class]\n",
    "\n",
    "    print(f\"Liczność klas przed augmentacją: {class_counts.to_dict()}\")\n",
    "\n",
    "    # Pobierz próbki z mniejszej klasy\n",
    "    class_subset = df[df[label_column] == min_class].copy()\n",
    "\n",
    "    # Oblicz ile dodatkowych próbek potrzebujemy\n",
    "    num_to_add = num_max - num_min\n",
    "\n",
    "    # Inicjalizacja nowego zbioru danych\n",
    "    augmented_data = {'Utterances': [], label_column: []}\n",
    "\n",
    "    # Augmentuj dane, ale tylko do momentu wyrównania liczby próbek\n",
    "    augment_per_sample = max(1, num_to_add // len(class_subset))  # Ile augmentacji na 1 próbkę\n",
    "    remaining = num_to_add  # Ile jeszcze próbek musimy dodać\n",
    "\n",
    "    for _, row in tqdm(class_subset.iterrows(), total=len(class_subset), desc=f\"Augmenting class {min_class}\"):\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "\n",
    "        # Wykonaj augmentację tekstu\n",
    "        new_texts = augment_text(row['Utterances'], num_augments=min(augment_per_sample, remaining))\n",
    "\n",
    "        for new_text in new_texts:\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            augmented_data['Utterances'].append(new_text)\n",
    "            augmented_data[label_column].append(min_class)\n",
    "            remaining -= 1  # Zmniejsz licznik brakujących próbek\n",
    "\n",
    "    # Tworzenie DataFrame z nowymi próbkami\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "    # Połączenie oryginalnych danych z nowymi danymi\n",
    "    final_df = pd.concat([df, augmented_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Sprawdź finalny rozkład klas\n",
    "    final_counts = final_df[label_column].value_counts()\n",
    "    print(f\"Liczność klas po augmentacji: {final_counts.to_dict()}\")\n",
    "\n",
    "    return final_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:37.455843Z",
     "start_time": "2025-06-12T13:45:37.442344Z"
    }
   },
   "id": "bf74b5425e90d5a1",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def augment_binary_data_percent(df, label_column, augment_text, augment_percent=25):\n",
    "    \"\"\"\n",
    "    Augments binary classification data by a specified percentage.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'Utterances' column and a binary label column.\n",
    "    - label_column (str): Column name of the binary target label (0 or 1).\n",
    "    - augment_text (callable): Function to augment text. Should take a string and return a list of augmented strings.\n",
    "    - augment_percent (int): Percentage increase for each class (e.g., 25 means adding 25% more samples per class).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Augmented DataFrame with increased class distributions.\n",
    "    \"\"\"\n",
    "    # Oblicz liczność klas\n",
    "    class_counts = df[label_column].value_counts()\n",
    "    print(f\"Liczność klas przed augmentacją: {class_counts.to_dict()}\")\n",
    "\n",
    "    # Inicjalizacja nowego zbioru danych\n",
    "    augmented_data = {'Utterances': [], label_column: []}\n",
    "\n",
    "    for label in class_counts.index:\n",
    "        class_subset = df[df[label_column] == label].copy()\n",
    "        num_to_add = int(class_counts[label] * (augment_percent / 100))\n",
    "\n",
    "        augment_per_sample = max(1, num_to_add // len(class_subset))\n",
    "        remaining = num_to_add\n",
    "\n",
    "        for _, row in tqdm(class_subset.iterrows(), total=len(class_subset), desc=f\"Augmenting class {label}\"):\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "\n",
    "            new_texts = augment_text(row['Utterances'], num_augments=min(augment_per_sample, remaining))\n",
    "\n",
    "            for new_text in new_texts:\n",
    "                if remaining <= 0:\n",
    "                    break\n",
    "                augmented_data['Utterances'].append(new_text)\n",
    "                augmented_data[label_column].append(label)\n",
    "                remaining -= 1\n",
    "\n",
    "    # Tworzenie DataFrame z nowymi próbkami\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "    # Połączenie oryginalnych danych z nowymi danymi\n",
    "    final_df = pd.concat([df, augmented_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Sprawdź finalny rozkład klas\n",
    "    final_counts = final_df[label_column].value_counts()\n",
    "    print(f\"Liczność klas po augmentacji: {final_counts.to_dict()}\")\n",
    "\n",
    "    return final_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:37.695757Z",
     "start_time": "2025-06-12T13:45:37.680209Z"
    }
   },
   "id": "120d3b0d5958947f",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_text(text, num_augments=2, use_llm_local=False):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        if use_llm_local and random.random() < 0.5:\n",
    "            augmented_texts.append(generate_esconv_style_response(text, style_examples))\n",
    "        else:\n",
    "            # ... klasyczne augmentacje\n",
    "            augmented_texts.append(random_deletion(text))\n",
    "    return augmented_texts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab9114f603d2e47b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczność klas przed augmentacją: {0: 546, 1: 539}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting class 1:   1%|▏         | 7/539 [00:03<04:58,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczność klas po augmentacji: {1: 546, 0: 546}\n",
      "                                             Utterances  label\n",
      "0     terry womack? i've had three members of the bl...      1\n",
      "1                             Oh. Well then shut me up.      0\n",
      "2     Well that was depressing, I think I just bough...      1\n",
      "3     so we don't even get to be up front where the ...      1\n",
      "4     i need to know the exact time that he arrived ...      0\n",
      "...                                                 ...    ...\n",
      "1087                   it's my foot it hurts like crazy      0\n",
      "1088                                                Hi.      0\n",
      "1089             now you think i'm gay. no, i'm not gay      0\n",
      "1090  god, you have no idea i spend my days just dra...      0\n",
      "1091                                            it's me      0\n",
      "\n",
      "[1092 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Augmentation\n",
    "augmented_df = augment_binary_data(\n",
    "    df=df,\n",
    "    label_column='label',\n",
    "    augment_text=augment_text,\n",
    "    num_augments=2\n",
    ")\n",
    "\n",
    "print(augmented_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:45:41.853942Z",
     "start_time": "2025-06-12T13:45:37.895614Z"
    }
   },
   "id": "230e9a2bfc05843e",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczność klas przed augmentacją: {1: 546, 0: 546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting class 1:  70%|██████▉   | 382/546 [03:56<01:41,  1.62it/s]\n",
      "Augmenting class 0:  70%|██████▉   | 382/546 [03:04<01:19,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczność klas po augmentacji: {0: 928, 1: 928}\n",
      "                                             Utterances  label\n",
      "0     i need someone who knows what they're doing to...      0\n",
      "1     Goodbye Mike, we'll see you at the wedding, fe...      1\n",
      "2          Stannis Baratheon is coming. His whole army.      1\n",
      "3     I don't know, because I told her to go fuck th...      1\n",
      "4     I will say whatever you want me to say that I ...      0\n",
      "...                                                 ...    ...\n",
      "1851  Anyway, I already told you, I take a break fro...      0\n",
      "1852  and my ability to control this situation dimin...      1\n",
      "1853  is it my turn to talk about the compromises i ...      1\n",
      "1854  yeah, but apparently, but not very good at either      1\n",
      "1855  All right. Here is what happened Buckminster F...      0\n",
      "\n",
      "[1856 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = augment_binary_data_percent(\n",
    "    df=augmented_df,\n",
    "    label_column='label',\n",
    "    augment_text=augment_text,\n",
    "    augment_percent=70\n",
    ")\n",
    "\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:42.635868Z",
     "start_time": "2025-06-12T13:45:41.857149Z"
    }
   },
   "id": "fc800c4ee0aa2eb0",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    50.0\n",
      "1    50.0\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    928\n",
      "1    928\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = df['label'].value_counts()\n",
    "label_frequencies_percent = df['label'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:42.651896Z",
     "start_time": "2025-06-12T13:52:42.636869Z"
    }
   },
   "id": "656eec943b8ae25b",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:42.672666Z",
     "start_time": "2025-06-12T13:52:42.654916Z"
    }
   },
   "id": "c053cc2ea6cbe64f",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: (1085, 4)\n",
      "Validation size: (279, 2), Test size: (278, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original train size: {df_data.shape}\")\n",
    "print(f\"Validation size: {df_valid.shape}, Test size: {df_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:42.684030Z",
     "start_time": "2025-06-12T13:52:42.677659Z"
    }
   },
   "id": "8d69bb5cab8227c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                          Utterances  label\n0  i need someone who knows what they're doing to...      0\n1  Goodbye Mike, we'll see you at the wedding, fe...      1\n2       Stannis Baratheon is coming. His whole army.      1\n3  I don't know, because I told her to go fuck th...      1\n4  I will say whatever you want me to say that I ...      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Utterances</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i need someone who knows what they're doing to...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Goodbye Mike, we'll see you at the wedding, fe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Stannis Baratheon is coming. His whole army.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I don't know, because I told her to go fuck th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I will say whatever you want me to say that I ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:42.701968Z",
     "start_time": "2025-06-12T13:52:42.686030Z"
    }
   },
   "id": "6313a5510286cb08",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    50.0\n",
      "1    50.0\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    928\n",
      "1    928\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = df['label'].value_counts()\n",
    "label_frequencies_percent = df['label'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:42.715333Z",
     "start_time": "2025-06-12T13:52:42.703969Z"
    }
   },
   "id": "7478b534ccd10119",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "class_distribution = df['label'].value_counts(normalize=True)\n",
    "print(class_distribution)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:42.723477Z",
     "start_time": "2025-06-12T13:52:42.717351Z"
    }
   },
   "id": "7064823e84ebfa8",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Sentiment Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:43.174628Z",
     "start_time": "2025-06-12T13:52:42.726475Z"
    }
   },
   "id": "73c13e7f14210a32",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['label']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list = list(df.columns)\n",
    "target_list = target_list[1:]\n",
    "target_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:43.190875Z",
     "start_time": "2025-06-12T13:52:43.182871Z"
    }
   },
   "id": "981cb9e10ae1a9a5",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        self.targets = self.df['label'].astype(int).values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            #'token_type_ids': inputs[\"token_type_ids\"].flatten(), -> nie potrzebne przy RoBERTa\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),\n",
    "            'utterances': utterances\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:43.206250Z",
     "start_time": "2025-06-12T13:52:43.196867Z"
    }
   },
   "id": "43e8c28009bdd96a",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class BERTBinarySentimentClassificationClass(nn.Module):\n",
    "#     def __init__(self, bert_model):\n",
    "#         super(BERTBinarySentimentClassificationClass, self).__init__()\n",
    "#         self.bert = bert_model\n",
    "#         self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "#         self.out = nn.Linear(self.bert.config.hidden_size, 1)  # Binary classification (1 output)\n",
    "# \n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "#         # Forward pass przez BERT\n",
    "#         outputs = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "#         pooled_output = outputs.pooler_output\n",
    "#         dropout_output = self.dropout(pooled_output)\n",
    "#         return self.out(dropout_output)\n",
    "#         #return torch.sigmoid(self.out(dropout_output))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:43.217371Z",
     "start_time": "2025-06-12T13:52:43.209251Z"
    }
   },
   "id": "6e94d6131ac088b5",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class RoBERTaBinarySentimentClassificationClass(nn.Module):\n",
    "#     def __init__(self, roberta_model):\n",
    "#         super(RoBERTaBinarySentimentClassificationClass, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "#         self.out = nn.Linear(self.roberta.config.hidden_size, 1)  # Binary classification\n",
    "# \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Forward pass przez RoBERTa\n",
    "#         outputs = self.roberta(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         # Sprawdzenie, czy pooler_output jest dostępne\n",
    "#         if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "#             pooled_output = outputs.pooler_output  # Preferowane, jeśli dostępne\n",
    "#         else:\n",
    "#             pooled_output = outputs.last_hidden_state[:, 0, :]  # Wykorzystanie tokena [CLS]\n",
    "# \n",
    "#         dropout_output = self.dropout(pooled_output)\n",
    "#         return self.out(dropout_output)\n",
    "#         # return torch.sigmoid(self.out(dropout_output))  # Jeśli używasz BCEWithLogitsLoss, sigmoid nie jest potrzebny"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:43.225293Z",
     "start_time": "2025-06-12T13:52:43.219364Z"
    }
   },
   "id": "e0a40b7814203672",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BERTLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, lstm_hidden_dim=128, lstm_layers=1, dropout_rate=DROPOUT_RATE, bidirectional=True):\n",
    "        super(BERTLSTMClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.bert.config.hidden_size,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if lstm_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # Jeśli LSTM jest dwukierunkowe, rozmiar wejścia do fc to lstm_hidden_dim*2, w przeciwnym wypadku lstm_hidden_dim\n",
    "        fc_input_dim = lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # Pobranie reprezentacji z BERTa (cała sekwencja)\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # last_hidden_state: tensor o wymiarach (batch_size, seq_length, hidden_size)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "\n",
    "        # Przepuszczenie przez LSTM\n",
    "        lstm_output, (hidden, _) = self.lstm(sequence_output)\n",
    "        # Jeśli LSTM jest dwukierunkowe, ukryty stan z ostatniej warstwy ma wymiar (num_layers*2, batch, hidden_dim)\n",
    "        if self.lstm.bidirectional:\n",
    "            # Pobieramy ostatnie stany z obu kierunków i je łączymy\n",
    "            hidden_forward = hidden[-2, :, :]  # ostatnia warstwa, kierunek \"forward\"\n",
    "            hidden_backward = hidden[-1, :, :]  # ostatnia warstwa, kierunek \"backward\"\n",
    "            hidden = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "\n",
    "        dropout_output = self.dropout(hidden)\n",
    "        logits = self.fc(dropout_output)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:43.242932Z",
     "start_time": "2025-06-12T13:52:43.232228Z"
    }
   },
   "id": "2082ab52f98981f",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class BERTCNNClassifier(nn.Module):\n",
    "#     def __init__(self, bert_model, num_filters=100, filter_sizes=[2, 3, 4], dropout_rate=0.3):\n",
    "#         super(BERTCNNClassifier, self).__init__()\n",
    "#         self.bert = bert_model\n",
    "#         # Tworzymy listę konwolucji: każdy filtr działa na \"okno\" o danej wielkości\n",
    "#         self.convs = nn.ModuleList([\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=1,\n",
    "#                 out_channels=num_filters,\n",
    "#                 kernel_size=(fs, self.bert.config.hidden_size)\n",
    "#             )\n",
    "#             for fs in filter_sizes\n",
    "#         ])\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
    "# \n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "#         bert_output = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "#         # last_hidden_state: (batch_size, seq_length, hidden_size)\n",
    "#         sequence_output = bert_output.last_hidden_state\n",
    "# \n",
    "#         # Dodajemy wymiar kanału (potrzebny dla CNN)\n",
    "#         x = sequence_output.unsqueeze(1)  # (batch_size, 1, seq_length, hidden_size)\n",
    "# \n",
    "#         # Przepuszczamy przez każdy filtr konwolucyjny\n",
    "#         conved = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "#         # conved[i] ma wymiary: (batch_size, num_filters, seq_length - filter_size + 1)\n",
    "# \n",
    "#         # Wykonujemy max-pooling na każdej mapie cech\n",
    "#         pooled = [F.max_pool1d(feature_map, kernel_size=feature_map.size(2)).squeeze(2) for feature_map in conved]\n",
    "#         # pooled[i] ma wymiary: (batch_size, num_filters)\n",
    "# \n",
    "#         # Łączymy wyniki z różnych filtrów\n",
    "#         cat = torch.cat(pooled, dim=1)  # (batch_size, num_filters * len(filter_sizes))\n",
    "# \n",
    "#         dropout_output = self.dropout(cat)\n",
    "#         logits = self.fc(dropout_output)\n",
    "#         return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:43.252190Z",
     "start_time": "2025-06-12T13:52:43.246054Z"
    }
   },
   "id": "e21b2c05a1cc0abd",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def freeze_bert_layers(model, freeze_until_layer=6):\n",
    "    \"\"\"\n",
    "    Zamraża wszystkie warstwy enkodera BERT, których numer jest mniejszy niż freeze_until_layer.\n",
    "    W modelu BERT-base (12 warstw): przy freeze_until_layer=6, zamrożone zostaną warstwy 0-5.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        # Szukamy parametrów, które należą do warstw enkodera\n",
    "        if \"bert.encoder.layer\" in name:\n",
    "            # Nazwa ma postać: bert.encoder.layer.<layer_num>...\n",
    "            try:\n",
    "                layer_num = int(name.split('.')[3])\n",
    "            except:\n",
    "                continue  # zabezpieczenie, gdyby parsowanie się nie udało\n",
    "            if layer_num < freeze_until_layer:\n",
    "                param.requires_grad = False\n",
    "                # Możesz też dodać print dla debugowania:\n",
    "                print(f\"Freezing parameter: {name}\")\n",
    "    print(f\"Frozen BERT layers: 0-{freeze_until_layer - 1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:43.266796Z",
     "start_time": "2025-06-12T13:52:43.255188Z"
    }
   },
   "id": "1e7fdb5f2c851622",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#model_path = 'best_model_state.bin'\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#model = BERTBinarySentimentClassificationClass(bert_model)\n",
    "model = BERTLSTMClassifier(bert_model, lstm_layers=LSTM_LAYERS, lstm_hidden_dim=LSTM_HIDDEN_DIM)\n",
    "#model = BERTCNNClassifier(bert_model)\n",
    "\n",
    "# roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "# model = RoBERTaBinarySentimentClassificationClass(roberta_model)\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:45.189653Z",
     "start_time": "2025-06-12T13:52:43.269793Z"
    }
   },
   "id": "d5256b71691e1c46",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:45.197407Z",
     "start_time": "2025-06-12T13:52:45.191651Z"
    }
   },
   "id": "aadf862ae0047947",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH, shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:45.203657Z",
     "start_time": "2025-06-12T13:52:45.199357Z"
    }
   },
   "id": "de1b1918cc1209d9",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0478],\n",
      "        [-0.0968],\n",
      "        [-0.0622],\n",
      "        [-0.0685],\n",
      "        [-0.0900],\n",
      "        [-0.0642],\n",
      "        [-0.0509],\n",
      "        [-0.0031],\n",
      "        [-0.0046],\n",
      "        [-0.0384],\n",
      "        [-0.0328],\n",
      "        [-0.0039],\n",
      "        [-0.0450],\n",
      "        [-0.0103],\n",
      "        [-0.0476],\n",
      "        [-0.0575],\n",
      "        [-0.0457],\n",
      "        [-0.0783],\n",
      "        [-0.0325],\n",
      "        [-0.0206],\n",
      "        [-0.0268],\n",
      "        [-0.0693],\n",
      "        [-0.0808],\n",
      "        [-0.0061],\n",
      "        [-0.0521],\n",
      "        [ 0.0226],\n",
      "        [-0.0412],\n",
      "        [ 0.0488],\n",
      "        [-0.0052],\n",
      "        [ 0.0282],\n",
      "        [-0.0690],\n",
      "        [-0.0591]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "outputs = model(data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:59.494322Z",
     "start_time": "2025-06-12T13:52:45.205657Z"
    }
   },
   "id": "8e2ccb1ee90e217c",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                  max_length = 50,\n",
    "                                  truncation = True,\n",
    "                                  padding = \"max_length\",\n",
    "                                  return_attention_mask = True,\n",
    "                                  return_tensors = \"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:52:59.585896Z",
     "start_time": "2025-06-12T13:52:59.516267Z"
    }
   },
   "id": "f1b009a33dc77303",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#bert_model = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encodings['input_ids'],\n",
    "    attention_mask=encodings['attention_mask']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:53:05.544971Z",
     "start_time": "2025-06-12T13:52:59.590463Z"
    }
   },
   "id": "2bec695a35b6a3f",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def loss_fn(outputs, targets):\n",
    "#     # jesli uzywamy BCEWithLogitsLoss, to w modelu nie musimy dodawać sigmoid, bo ta funkcja już zawiera operację sigmoid.\n",
    "#     return torch.nn.BCEWithLogitsLoss()(outputs.squeeze(-1), targets.float())\n",
    "\n",
    "# def loss_fn(outputs, targets):\n",
    "#     return torch.nn.BCELoss()(outputs.squeeze(), targets.float())\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs powinny być wyjściem z modelu (logity)\n",
    "        bce_loss = self.bce(inputs, targets.float())\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        # Obliczenie p_t: dla próbki z target=1 mamy probas, dla target=0 mamy 1 - probas\n",
    "        p_t = targets * probas + (1 - targets) * (1 - probas)\n",
    "        loss = self.alpha * (1 - p_t) ** self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "loss_fn = FocalLoss(alpha=FOCAL_LOSS_ALFA, gamma=FOCAL_LOSS_GAMMA, reduction='mean')\n",
    "#gamma\n",
    "#Odpowiada za redukowanie wagi łatwo sklasyfikowanych przykładów. Wyższa wartość gamma sprawia, że model skupia się bardziej na trudnych przypadkach.\n",
    "#Jeśli gamma jest zbyt wysoka (np. 2 lub więcej), może to powodować, że model zaniedbuje uczenie się na przykładach, które są łatwiejsze do sklasyfikowania, co może negatywnie wpływać na ogólną stabilność treningu.\n",
    "#W twoim przypadku, skoro klasa 1 nie jest przewidywana, warto spróbować obniżyć wartość gamma (np. do 1), aby nie tłumić gradientów zbyt mocno.\n",
    "\n",
    "#alpha\n",
    "#Umożliwia balansowanie klas poprzez przypisanie większej wagi przykładom z klasy, która jest niedoreprezentowana lub trudniejsza do nauki.\n",
    "# W twoim kodzie alpha=1 oznacza, że wszystkie próbki są traktowane jednakowo. Jeśli obserwujesz, że jedna klasa (tu klasa 1) jest pomijana, możesz spróbować nadać jej większą wagę.\n",
    "# Możesz rozważyć modyfikację alpha na wektor, np. alpha = [0.25, 0.75] lub inne wartości, w zależności od nierównowagi między klasami. To pozwoli nadać przykładom z klasy 1 większy wpływ na stratę.\n",
    "\n",
    "\n",
    "#Zmodyfikuj implementację, aby alpha było wektorem wag (np. alpha = [waga_klasy0, waga_klasy1]), co pozwoli na precyzyjne balansowanie strat między klasami.\n",
    "#Przeprowadź serię eksperymentów (np. grid search) z różnymi wartościami gamma i alpha, aby znaleźć najlepszą kombinację, która poprawi metryki takie jak F1-score, szczególnie dla klasy 1.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:53:05.798138Z",
     "start_time": "2025-06-12T13:53:05.638721Z"
    }
   },
   "id": "77a7cac8c06c5ae1",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juwieczo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# learning rate scheduler\n",
    "# ReduceLROnPlateau może obniżać lr do bardzo małych wartości, co czasem prowadzi do problemów. Możesz dodać min_lr, aby ograniczyć ten efekt:\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=MODE, patience=PATIENCE, factor=FACTOR, verbose=VERBOSE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:53:11.097976Z",
     "start_time": "2025-06-12T13:53:05.815096Z"
    }
   },
   "id": "ea368685bd23d290",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(training_loader, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trenuje model na danych treningowych i zwraca model, dokładność, średni loss oraz F1-score.\n",
    "\n",
    "    Args:\n",
    "        training_loader (DataLoader): DataLoader z danymi treningowymi.\n",
    "        model (torch.nn.Module): Model do trenowania.\n",
    "        optimizer (torch.optim.Optimizer): Optymalizator do aktualizacji wag modelu.\n",
    "        loss_fn (callable): Funkcja strat, np. nn.BCEWithLogitsLoss.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): Wytrenowany model.\n",
    "        train_accuracy (float): Dokładność modelu na zbiorze treningowym.\n",
    "        avg_loss (float): Średnia wartość funkcji strat.\n",
    "        train_f1 (float): F1-score (binary) na zbiorze treningowym.\n",
    "    \"\"\"\n",
    "    # Inicjalizacja zmiennych do śledzenia wyników\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), leave=True, colour='steelblue')\n",
    "\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        #token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.float)  # Binary targets jako float\n",
    "\n",
    "        #outputs = model(ids, mask, token_type_ids if 'token_type_ids' in data else None)\n",
    "        outputs = model(ids, mask if 'token_type_ids' in data else None)\n",
    "\n",
    "        outputs = outputs.squeeze(-1)  # Dopasowanie wymiarów do binary classification (1D)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        preds = torch.sigmoid(outputs) >= 0.5  # Sigmoid + progowanie przy 0.5\n",
    "        correct_predictions += torch.sum(preds == targets).item()\n",
    "        num_samples += targets.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    train_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return model, correct_predictions / num_samples, np.mean(losses), train_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:53:11.162761Z",
     "start_time": "2025-06-12T13:53:11.117915Z"
    }
   },
   "id": "b856ca62d79ec79c",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, epoch):\n",
    "    \"\"\"\n",
    "    Ewaluacja modelu na danych walidacyjnych.\n",
    "\n",
    "    Args:\n",
    "        validation_loader (DataLoader): DataLoader z danymi walidacyjnymi.\n",
    "        model (torch.nn.Module): Model do oceny.\n",
    "        loss_fn (callable): Funkcja strat, np. nn.BCEWithLogitsLoss.\n",
    "        epoch (int): Aktualny numer epoki do logowania w TensorBoard.\n",
    "\n",
    "    Returns:\n",
    "        val_accuracy (float): Dokładność modelu na zbiorze walidacyjnym.\n",
    "        avg_loss (float): Średnia wartość funkcji strat na zbiorze walidacyjnym.\n",
    "        val_f1 (float): F1-score (binary) na zbiorze walidacyjnym.\n",
    "    \"\"\"\n",
    "    # Inicjalizacja zmiennych do śledzenia wyników\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Ustaw model w tryb ewaluacyjny\n",
    "    model.eval()\n",
    "\n",
    "    # Wyłącz gradienty dla ewaluacji\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            #token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.float)  # Binary targets jako float\n",
    "\n",
    "            #outputs = model(ids, mask, token_type_ids if 'token_type_ids' in data else None)\n",
    "            outputs = model(ids, mask if 'token_type_ids' in data else None)\n",
    "\n",
    "            outputs = outputs.squeeze(-1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds = torch.sigmoid(outputs) >= 0.5  # Sigmoid + progowanie przy 0.5\n",
    "            correct_predictions += torch.sum(preds == targets).item()\n",
    "            num_samples += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    writer.add_scalar('Loss/validation', avg_loss, epoch)\n",
    "    writer.add_scalar('F1-Score/validation', val_f1, epoch)\n",
    "\n",
    "    return correct_predictions / num_samples, avg_loss, val_f1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-07T12:42:41.184403Z",
     "start_time": "2025-08-07T12:42:41.164887Z"
    }
   },
   "id": "85ae992f90f3f334",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, epoch):\n",
    "    \"\"\"\n",
    "    Ewaluacja modelu na danych walidacyjnych.\n",
    "\n",
    "    Args:\n",
    "        validation_loader (DataLoader): DataLoader z danymi walidacyjnymi.\n",
    "        model (torch.nn.Module): Model do oceny.\n",
    "        loss_fn (callable): Funkcja strat, np. nn.BCEWithLogitsLoss.\n",
    "        epoch (int): Aktualny numer epoki do logowania w TensorBoard.\n",
    "\n",
    "    Returns:\n",
    "        val_accuracy (float): Dokładność modelu na zbiorze walidacyjnym.\n",
    "        avg_loss (float): Średnia wartość funkcji strat na zbiorze walidacyjnym.\n",
    "        val_f1 (float): F1-score (binary) na zbiorze walidacyjnym.\n",
    "    \"\"\"\n",
    "    # Inicjalizacja zmiennych do śledzenia wyników\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Ustaw model w tryb ewaluacyjny\n",
    "    model.eval()\n",
    "\n",
    "    # Wyłącz gradienty dla ewaluacji\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            #token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.float)  # Binary targets jako float\n",
    "\n",
    "            #outputs = model(ids, mask, token_type_ids if 'token_type_ids' in data else None)\n",
    "            outputs = model(ids, mask if 'token_type_ids' in data else None)\n",
    "\n",
    "            outputs = outputs.squeeze(-1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds = torch.sigmoid(outputs) >= 0.5  # Sigmoid + progowanie przy 0.5\n",
    "            correct_predictions += torch.sum(preds == targets).item()\n",
    "            num_samples += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    writer.add_scalar('Loss/validation', avg_loss, epoch)\n",
    "    writer.add_scalar('F1-Score/validation', val_f1, epoch)\n",
    "\n",
    "    return correct_predictions / num_samples, avg_loss, val_f1\n",
    "def count_trainable_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T13:53:11.194421Z",
     "start_time": "2025-06-12T13:53:11.184138Z"
    }
   },
   "id": "5b40517357c888be",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "478aef9202b247c8bee0bfec0d3ce2bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6948 | Train accuracy 0.5050 | Train F1 0.4835\n",
      "Val loss 0.6925 | Val accuracy 0.5520 | Val F1 0.6356\n",
      "Saved new best model.\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20322fadc5144f8fa88a7ea3e2a9527c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6914 | Train accuracy 0.5181 | Train F1 0.5293\n",
      "Val loss 0.6931 | Val accuracy 0.5161 | Val F1 0.0000\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11931feb534345369cf75fc8f120e6c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6894 | Train accuracy 0.5404 | Train F1 0.5733\n",
      "Val loss 0.6925 | Val accuracy 0.5305 | Val F1 0.5868\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dbc761e4f9d4be496f3b744772c1ec6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6722 | Train accuracy 0.5920 | Train F1 0.6051\n",
      "Val loss 0.7061 | Val accuracy 0.5412 | Val F1 0.6559\n",
      "Saved new best model.\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5656c9e4ba6341188b3c73aab7e9b62e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6635 | Train accuracy 0.6135 | Train F1 0.6494\n",
      "Val loss 0.6983 | Val accuracy 0.5090 | Val F1 0.5018\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aaea8cfb433a4820968795fccd3fef77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6485 | Train accuracy 0.6259 | Train F1 0.6173\n",
      "Val loss 0.6908 | Val accuracy 0.5484 | Val F1 0.5909\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f11c5f9c55c541ceb1dc5be02f7bed38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6204 | Train accuracy 0.6774 | Train F1 0.7014\n",
      "Val loss 0.6998 | Val accuracy 0.5663 | Val F1 0.6159\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4592cf041d649008545655aac162743"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5975 | Train accuracy 0.7090 | Train F1 0.7183\n",
      "Val loss 0.6967 | Val accuracy 0.5806 | Val F1 0.6113\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b586ee0c25a47eabf70126285d09dbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5818 | Train accuracy 0.7413 | Train F1 0.7477\n",
      "Val loss 0.6889 | Val accuracy 0.5878 | Val F1 0.6128\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4849167078547ec9eb7630cd91006bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5763 | Train accuracy 0.7352 | Train F1 0.7235\n",
      "Val loss 0.6951 | Val accuracy 0.5771 | Val F1 0.6144\n"
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "\n",
    "    model, train_acc, train_loss, train_f1 = train_model(train_data_loader, model, optimizer)\n",
    "    print(f'Train loss {train_loss:.4f} | Train accuracy {train_acc:.4f} | Train F1 {train_f1:.4f}')\n",
    "\n",
    "    val_acc, val_loss, val_f1 = eval_model(val_data_loader, model, epoch)\n",
    "    print(f'Val loss {val_loss:.4f} | Val accuracy {val_acc:.4f} | Val F1 {val_f1:.4f}')\n",
    "\n",
    "    # Logowanie metryk do TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('F1-Score/train', train_f1, epoch)\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        model.bert.save_pretrained(output_dir)\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, \"best_binary_model_state.bin\"))\n",
    "        #torch.save(model.state_dict(), \"best_binary_model_state.bin\")\n",
    "        best_f1 = val_f1\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    #scheduler.step(val_loss)  # Tuning LR\n",
    "    scheduler.step()\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T16:23:16.553344Z",
     "start_time": "2025-06-12T13:53:11.227826Z"
    }
   },
   "id": "134691e19bd73c9a",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e1afc6cd3268ae0f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction on Adolescence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec16ff152b1bd75a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False, # BERT-base-uncased zazwyczaj nie potrzebuje token_type_ids dla pojedynczych zdań\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:06:45.795494Z",
     "start_time": "2025-06-12T17:06:45.660741Z"
    }
   },
   "id": "beff316161fd86a5",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from saved model path...\n",
      "Loading pre-trained BERT model from saved model path...\n",
      "Initializing BERTLSTMClassifier...\n"
     ]
    }
   ],
   "source": [
    "# Ścieżka do zapisanego modelu\n",
    "saved_model_path = './fine_tuned_bert_lstm_model'\n",
    "model_weights_path = os.path.join(saved_model_path, \"best_binary_model_state.bin\")\n",
    "\n",
    "# Wczytaj tokenizer z folderu\n",
    "print(\"Loading tokenizer from saved model path...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(saved_model_path)\n",
    "\n",
    "# Wczytaj bazowy model BERT z folderu (to zapewni zgodność słownika)\n",
    "print(\"Loading pre-trained BERT model from saved model path...\")\n",
    "bert_model = BertModel.from_pretrained(saved_model_path)\n",
    "\n",
    "# Zainicjuj Twój customowy klasyfikator\n",
    "print(\"Initializing BERTLSTMClassifier...\")\n",
    "model = BERTLSTMClassifier(bert_model, lstm_layers=LSTM_LAYERS, lstm_hidden_dim=LSTM_HIDDEN_DIM)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:06:57.376502Z",
     "start_time": "2025-06-12T17:06:49.510392Z"
    }
   },
   "id": "3f9d6193447c2144",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from ./fine_tuned_bert_lstm_model\\best_binary_model_state.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juwieczo\\AppData\\Local\\Temp\\ipykernel_30176\\2232047830.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Loading model weights from {model_weights_path}...\")\n",
    "    model.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_weights_path}' not found. Please ensure the path and filename are correct.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model loading: {e}\")\n",
    "    exit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:07:00.065454Z",
     "start_time": "2025-06-12T17:06:57.379122Z"
    }
   },
   "id": "f18a7e61d33b16c8",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "BERTLSTMClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (lstm): LSTM(768, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:07:00.155425Z",
     "start_time": "2025-06-12T17:07:00.083061Z"
    }
   },
   "id": "118409751fb02179",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:07:04.005968Z",
     "start_time": "2025-06-12T17:07:02.539085Z"
    }
   },
   "id": "b1e70b55778a44d8",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new dataset: adolescence_s01_e03_subtitles_eng.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading new dataset: adolescence_s01_e03_subtitles_eng.csv\")\n",
    "df_adolescence = pd.read_csv('C:/Users/juwieczo/DataspellProjects/meisd_project/data/adolescence_s01_e03_subtitles_eng.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:07:04.061500Z",
     "start_time": "2025-06-12T17:07:04.007043Z"
    }
   },
   "id": "f3ea942b1711f09b",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if 'text' not in df_adolescence.columns:\n",
    "    print(\"Warning: 'text' column not found in df_adolescence. Assuming first column contains the text.\")\n",
    "    texts_for_inference = df_adolescence.iloc[:, 0].tolist()\n",
    "else:\n",
    "    texts_for_inference = df_adolescence['text'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:07:05.011088Z",
     "start_time": "2025-06-12T17:07:05.003551Z"
    }
   },
   "id": "1aeca95a650c9a06",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inference_dataset = InferenceDataset(\n",
    "    texts=texts_for_inference,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "inference_data_loader = torch.utils.data.DataLoader(\n",
    "    inference_dataset,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:07:05.851608Z",
     "start_time": "2025-06-12T17:07:05.835805Z"
    }
   },
   "id": "74cf697c7e4bde37",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference on the new dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting:   0%|          | 0/26 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fde120cdd4df49e787dc296b466d51f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting inference on the new dataset...\")\n",
    "predictions = []\n",
    "original_texts = []\n",
    "\n",
    "with torch.no_grad(): # Wyłączamy obliczanie gradientów dla przewidywań\n",
    "    for data in tq.tqdm(inference_data_loader, total=len(inference_data_loader), desc=\"Predicting\"):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        original_texts.extend(data['text'])\n",
    "\n",
    "        # token_type_ids nie jest zwracane przez InferenceDataset, więc nie przekazujemy\n",
    "        # outputs = model(input_ids, attention_mask, token_type_ids=None) # Jeśli model wymaga token_type_ids\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Logity są zwracane przez model, przekształć je na prawdopodobieństwa za pomocą sigmoid\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "\n",
    "        # Próg 0.5 do binarnej klasyfikacji\n",
    "        batch_predictions = (probs >= 0.5).astype(int)\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "print(\"Inference complete.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:09:54.957783Z",
     "start_time": "2025-06-12T17:07:06.863496Z"
    }
   },
   "id": "c3fd29b41ba5abed",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to adolescence_s01_e03_subtitles_predictions.csv\n",
      "\n",
      "Sample predictions:\n",
      "                                                text  predicted_label\n",
      "0                                                Oh.                0\n",
      "1                                  Do you work here?                1\n",
      "2                      - Um, I'm a visitor. - Right.                0\n",
      "3  Right. Do you wanna just come through for a se...                1\n",
      "4                             Sorry. I'm a bit late.                0\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'text': original_texts,\n",
    "    'predicted_label': predictions\n",
    "})\n",
    "\n",
    "output_filename = 'adolescence_s01_e03_subtitles_predictions.csv'\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_filename}\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(results_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:27.111453Z",
     "start_time": "2025-06-12T17:13:27.039887Z"
    }
   },
   "id": "941c46e943b282c4",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transfer learning to ESConv dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f952fdde39a28c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "#dataset = load_data(\"D:/julixus/MEISD/meisd_project/data/ESConv.json\")\n",
    "dataset = load_data(\"C:/Users/juwieczo/DataspellProjects/meisd_project/data/ESConv.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:43.692353Z",
     "start_time": "2025-06-12T17:13:43.480148Z"
    }
   },
   "id": "e74cfffe2519a329",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  initial_emotion_intensity                                             dialog\n0                         5  [Hello, I am having a lot of anxiety about qui...\n1                         5  [hello im looking for someone to talk to, im f...\n2                         4  [Hello, I'm concerned about my job. I have bee...\n3                         4  [I am dong good. You?, I have been staying hom...\n4                         5  [Infinitely complicated., Too many decisions. ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>initial_emotion_intensity</th>\n      <th>dialog</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>[Hello, I am having a lot of anxiety about qui...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>[hello im looking for someone to talk to, im f...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>[Hello, I'm concerned about my job. I have bee...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[I am dong good. You?, I have been staying hom...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[Infinitely complicated., Too many decisions. ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_seeker_data(data, key):\n",
    "    result = []\n",
    "\n",
    "    for entry in data:\n",
    "        dialog = entry['dialog']\n",
    "        seeker_dialog = [item['content'].strip() for item in dialog if item['speaker'] == 'seeker']\n",
    "\n",
    "        quarter_length = max(1, len(seeker_dialog) // 4)\n",
    "\n",
    "        if key == 'initial_emotion_intensity':\n",
    "            selected_dialog = seeker_dialog[:quarter_length]\n",
    "        elif key == 'final_emotion_intensity':\n",
    "            selected_dialog = seeker_dialog[-quarter_length:]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        result.append({\n",
    "            key: entry['survey_score']['seeker'][key],\n",
    "            'dialog': selected_dialog\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n",
    "first_25_percent = extract_seeker_data(dataset, 'initial_emotion_intensity')\n",
    "#last_25_percent = extract_seeker_data(dataset, 'final_emotion_intensity')\n",
    "\n",
    "first_25_df = pd.DataFrame(first_25_percent)\n",
    "#last_25_df = pd.DataFrame(last_25_percent)\n",
    "\n",
    "first_25_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:43.998390Z",
     "start_time": "2025-06-12T17:13:43.899233Z"
    }
   },
   "id": "a2a3e0a1d7d37ee0",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_counts = first_25_df['initial_emotion_intensity'].value_counts()\n",
    "least_common_label = label_counts.idxmin()\n",
    "first_25_df = first_25_df[first_25_df['initial_emotion_intensity'] != least_common_label]\n",
    "first_25_df['initial_emotion_intensity'] = pd.to_numeric(first_25_df['initial_emotion_intensity'], errors='coerce')\n",
    "first_25_df['initial_emotion_intensity'] = first_25_df['initial_emotion_intensity'] - 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:44.662794Z",
     "start_time": "2025-06-12T17:13:44.638591Z"
    }
   },
   "id": "5fb5f7b4ca2c53a3",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      label                                         Utterances\n0         3  [Hello, I am having a lot of anxiety about qui...\n1         3  [hello im looking for someone to talk to, im f...\n2         2  [Hello, I'm concerned about my job. I have bee...\n3         2  [I am dong good. You?, I have been staying hom...\n4         3  [Infinitely complicated., Too many decisions. ...\n...     ...                                                ...\n1295      3  [I feel sleepy but can not sleep, It has alway...\n1296      2  [I am fine. thanks. how about you ?, I lost my...\n1297      1        [HI how are you today, Doing well, thanks.]\n1298      1  [Hello, I am a little down today.  How are you...\n1299      1                                  [hi, i'm nereida]\n\n[1298 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>Utterances</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>[Hello, I am having a lot of anxiety about qui...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>[hello im looking for someone to talk to, im f...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[Hello, I'm concerned about my job. I have bee...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>[I am dong good. You?, I have been staying hom...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>[Infinitely complicated., Too many decisions. ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1295</th>\n      <td>3</td>\n      <td>[I feel sleepy but can not sleep, It has alway...</td>\n    </tr>\n    <tr>\n      <th>1296</th>\n      <td>2</td>\n      <td>[I am fine. thanks. how about you ?, I lost my...</td>\n    </tr>\n    <tr>\n      <th>1297</th>\n      <td>1</td>\n      <td>[HI how are you today, Doing well, thanks.]</td>\n    </tr>\n    <tr>\n      <th>1298</th>\n      <td>1</td>\n      <td>[Hello, I am a little down today.  How are you...</td>\n    </tr>\n    <tr>\n      <th>1299</th>\n      <td>1</td>\n      <td>[hi, i'm nereida]</td>\n    </tr>\n  </tbody>\n</table>\n<p>1298 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_25_df.rename(columns={\n",
    "    'dialog': 'Utterances',\n",
    "    'initial_emotion_intensity': 'label'\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:45.303765Z",
     "start_time": "2025-06-12T17:13:45.285549Z"
    }
   },
   "id": "a5038c8d54db5f67",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data['label'] = (df_data['max_intensity'] == 2).astype(int)\n",
    "columns = ['Utterances', 'label']\n",
    "df = df_data[columns].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:45.898278Z",
     "start_time": "2025-06-12T17:13:45.886690Z"
    }
   },
   "id": "c3d3ba2d4b569246",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv: Train (759, 2), Valid (163, 2), Test (163, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train_esconv, df_temp_esconv = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "df_valid_esconv, df_test_esconv = train_test_split(df_temp_esconv, random_state=88, test_size=0.50, shuffle=True)\n",
    "print(f\"ESConv: Train {df_train_esconv.shape}, Valid {df_valid_esconv.shape}, Test {df_test_esconv.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:46.331137Z",
     "start_time": "2025-06-12T17:13:46.305528Z"
    }
   },
   "id": "1a61879bc5efc024",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset_esconv = CustomDataset(df_train_esconv, tokenizer, MAX_LEN)\n",
    "valid_dataset_esconv = CustomDataset(df_valid_esconv, tokenizer, MAX_LEN)\n",
    "train_data_loader_esconv = torch.utils.data.DataLoader(train_dataset_esconv, batch_size=BATCH, shuffle=True)\n",
    "valid_data_loader_esconv = torch.utils.data.DataLoader(valid_dataset_esconv, batch_size=BATCH, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:46.915771Z",
     "start_time": "2025-06-12T17:13:46.908675Z"
    }
   },
   "id": "6476534e8c7bbcd1",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juwieczo\\AppData\\Local\\Temp\\ipykernel_30176\\2044755011.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_binary_model_state.bin\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": "BERTLSTMClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (lstm): LSTM(768, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_binary_model_state.bin\"))\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:48.089921Z",
     "start_time": "2025-06-12T17:13:47.211319Z"
    }
   },
   "id": "4a86f720c348d6b3",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before freezing on ESConv: Total params: 110797313, Trainable params: 110797313\n"
     ]
    }
   ],
   "source": [
    "total_params, trainable_params = count_trainable_parameters(model)\n",
    "print(f\"Before freezing on ESConv: Total params: {total_params}, Trainable params: {trainable_params}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:48.109411Z",
     "start_time": "2025-06-12T17:13:48.094184Z"
    }
   },
   "id": "7b62b214b225c503",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing parameter: bert.encoder.layer.0.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.0.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.0.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.0.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.0.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.1.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.1.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.1.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.1.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.2.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.2.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.2.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.2.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.3.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.3.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.3.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.3.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.4.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.4.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.4.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.4.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.5.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.5.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.5.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.5.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "Frozen BERT layers: 0-5\n",
      "After freezing on ESConv: Total params: 110797313, Trainable params: 68270081\n"
     ]
    }
   ],
   "source": [
    "freeze_bert_layers(model, freeze_until_layer=6)\n",
    "total_params, trainable_params = count_trainable_parameters(model)\n",
    "print(f\"After freezing on ESConv: Total params: {total_params}, Trainable params: {trainable_params}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:48.129061Z",
     "start_time": "2025-06-12T17:13:48.112403Z"
    }
   },
   "id": "c9de44483af3d0b6",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juwieczo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer_esconv = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE_FINE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler_esconv = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_esconv, T_max=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T17:13:48.602576Z",
     "start_time": "2025-06-12T17:13:48.544658Z"
    }
   },
   "id": "611128487641b380",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning on ESConv...\n",
      "Fine-tuning Epoch 1/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1142b798cfaa44718fff1115f4d07bbb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6884 | Train accuracy 0.5441 | Train F1 0.6239\n",
      "ESConv Val loss 0.6855 | Val accuracy 0.5890 | Val F1 0.6215\n",
      "Saved new best ESConv model.\n",
      "Fine-tuning Epoch 2/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b55a2df961934a008fadcf4f4b0e4df5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6879 | Train accuracy 0.5468 | Train F1 0.5567\n",
      "ESConv Val loss 0.6823 | Val accuracy 0.5890 | Val F1 0.6378\n",
      "Saved new best ESConv model.\n",
      "Fine-tuning Epoch 3/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "773aa83152a4409d9bc5c138f75fc9d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6834 | Train accuracy 0.5586 | Train F1 0.5315\n",
      "ESConv Val loss 0.6788 | Val accuracy 0.6074 | Val F1 0.6559\n",
      "Saved new best ESConv model.\n",
      "Fine-tuning Epoch 4/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d16ec57288b4638a492e6ac8c19352d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6779 | Train accuracy 0.5929 | Train F1 0.6161\n",
      "ESConv Val loss 0.6756 | Val accuracy 0.5890 | Val F1 0.6633\n",
      "Saved new best ESConv model.\n",
      "Fine-tuning Epoch 5/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79b99765a55d48359d6b33da572d003c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6746 | Train accuracy 0.5982 | Train F1 0.6408\n",
      "ESConv Val loss 0.6726 | Val accuracy 0.5828 | Val F1 0.6566\n",
      "Fine-tuning Epoch 6/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d431ba67df704b3d9b57732a3a479936"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6738 | Train accuracy 0.5916 | Train F1 0.6220\n",
      "ESConv Val loss 0.6706 | Val accuracy 0.5706 | Val F1 0.6429\n",
      "Fine-tuning Epoch 7/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82c9dfd457664a26b8d19de4057eaad0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6746 | Train accuracy 0.5797 | Train F1 0.6057\n",
      "ESConv Val loss 0.6689 | Val accuracy 0.5583 | Val F1 0.6289\n",
      "Fine-tuning Epoch 8/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f84c4141be664d778d2f4e5be4140510"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6708 | Train accuracy 0.6047 | Train F1 0.6394\n",
      "ESConv Val loss 0.6691 | Val accuracy 0.5828 | Val F1 0.6566\n",
      "Fine-tuning Epoch 9/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf428b9d046d49cb8182e881d1fa67c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6691 | Train accuracy 0.5863 | Train F1 0.6323\n",
      "ESConv Val loss 0.6684 | Val accuracy 0.5767 | Val F1 0.6497\n",
      "Fine-tuning Epoch 10/10 (ESConv)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f5290d2073549cb8b672058565f4921"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Train loss 0.6705 | Train accuracy 0.6008 | Train F1 0.6406\n",
      "ESConv Val loss 0.6683 | Val accuracy 0.5767 | Val F1 0.6497\n",
      "Fine-tuning on ESConv complete!\n"
     ]
    }
   ],
   "source": [
    "history_esconv = defaultdict(list)\n",
    "best_f1_esconv = 0\n",
    "\n",
    "print(\"Fine-tuning on ESConv...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Fine-tuning Epoch {epoch}/{EPOCHS} (ESConv)')\n",
    "    model, train_acc, train_loss, train_f1 = train_model(train_data_loader_esconv, model, optimizer_esconv)\n",
    "    print(f'ESConv Train loss {train_loss:.4f} | Train accuracy {train_acc:.4f} | Train F1 {train_f1:.4f}')\n",
    "    val_acc, val_loss, val_f1 = eval_model(valid_data_loader_esconv, model, epoch)\n",
    "    print(f'ESConv Val loss {val_loss:.4f} | Val accuracy {val_acc:.4f} | Val F1 {val_f1:.4f}')\n",
    "    history_esconv['train_acc'].append(train_acc)\n",
    "    history_esconv['train_loss'].append(train_loss)\n",
    "    history_esconv['train_f1'].append(train_f1)\n",
    "    history_esconv['val_acc'].append(val_acc)\n",
    "    history_esconv['val_loss'].append(val_loss)\n",
    "    history_esconv['val_f1'].append(val_f1)\n",
    "    if val_f1 > best_f1_esconv:\n",
    "        torch.save(model.state_dict(), \"best_fine_tuned_model_state.bin\")\n",
    "        best_f1_esconv = val_f1\n",
    "        print(\"Saved new best ESConv model.\")\n",
    "    scheduler_esconv.step()\n",
    "print(\"Fine-tuning on ESConv complete!\")\n",
    "# Zamknij TensorBoard writer\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T18:28:18.538689Z",
     "start_time": "2025-06-12T17:13:49.288437Z"
    }
   },
   "id": "91f78b45ae0631b7",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESConv Test Set Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.03      0.06        89\n",
      "           1       0.46      0.99      0.63        74\n",
      "\n",
      "    accuracy                           0.47       163\n",
      "   macro avg       0.60      0.51      0.35       163\n",
      "weighted avg       0.62      0.47      0.32       163\n"
     ]
    }
   ],
   "source": [
    "test_dataset_esconv = CustomDataset(df_test_esconv, tokenizer, MAX_LEN)\n",
    "test_data_loader_esconv = torch.utils.data.DataLoader(test_dataset_esconv, batch_size=BATCH, shuffle=False)\n",
    "predictions, true_labels = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader_esconv:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['targets'].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        preds = torch.sigmoid(outputs) >= 0.5\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"ESConv Test Set Report:\")\n",
    "print(classification_report(true_labels, predictions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T18:29:19.882435Z",
     "start_time": "2025-06-12T18:28:48.948337Z"
    }
   },
   "id": "6aa2f7642758f3d9",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51088840e0073d0d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b6b4a28e7cc68f69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fb5ff8ddd8529238"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_model(data_loader, model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            #outputs = model(input_ids=input_ids)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, true_labels\n",
    "\n",
    "# Run validation\n",
    "predictions, true_labels = test_model(val_data_loader, model)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91b2b2fe84a323a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dc2acc0f4d5644c0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17fc756d71b62fbb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7f4d5391be3c082",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
