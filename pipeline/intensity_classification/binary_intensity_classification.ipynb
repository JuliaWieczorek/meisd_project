{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "#from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AdamW\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from deep_translator import GoogleTranslator\n",
    "from googletrans import Translator\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "BATCH = 32\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased' #\"roberta-base\" #'bert-base-cased'\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.00001\n",
    "THRESHOLD = 0.3 #prog decyzyjny\n",
    "DROPOUT_RATE = 0.3\n",
    "WEIGHT_DECAY = 0.001\n",
    "MODE='min'\n",
    "PATIENCE=2\n",
    "FACTOR=0.5\n",
    "VERBOSE=True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a7d5ffe51d43f10",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('C:/Users/juwieczo/DataspellProjects/meisd_project/pipeline/max_first_25_intensity.csv')\n",
    "#df_data = pd.read_csv('C:/Users/juwieczo/DataspellProjects/meisd_project/pipeline/balanced_augmented_data_primary_intensity.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7511ba9431cb25f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85b5b15638a903f5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_frequencies = df_data['max_intensity'].value_counts()\n",
    "label_frequencies_percent = df_data['max_intensity'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "132893087c52c8a6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data['label'] = (df_data['max_intensity'] == 2).astype(int)\n",
    "#df_data['label'] = (df_data['label'] == 2).astype(int)\n",
    "\n",
    "columns = ['Utterances', 'label']\n",
    "df = df_data[columns].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27346da902b31a4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1. Synonym Replacement\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words[:]\n",
    "    num_replacements = max(1, len(words) // 5)  # Replace about 20% of words\n",
    "    random_words = random.sample(words, num_replacements)\n",
    "\n",
    "    for word in random_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            #synonym = synonyms[0].lemmas()[0].name()  # Take first synonym\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            if synonym != word:  # Avoid replacement if the synonym is identical\n",
    "                new_words = [synonym if w == word else w for w in new_words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "# 2. Random Insertion\n",
    "def random_insertion(text, n=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        new_word = random.choice(words)\n",
    "        insert_pos = random.randint(0, len(words))\n",
    "        words.insert(insert_pos, new_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# 3. Random Deletion\n",
    "def random_deletion(text, p=0.3):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text  # Avoid deleting single-word text\n",
    "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    if not new_words:\n",
    "        return random.choice(words)  # Return one word if all words are deleted\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# 4. Back Translation\n",
    "def back_translation(text, src_lang='en', mid_lang='fr', max_retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=src_lang, target=mid_lang).translate(text)\n",
    "            back_translated = GoogleTranslator(source=mid_lang, target=src_lang).translate(translated)\n",
    "            return back_translated\n",
    "        except Exception as e:\n",
    "            print(f\"Back translation error on attempt {attempt + 1}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(1)\n",
    "    raise ValueError(\"Back translation failed\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcc085f5564eeb6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_text(text, num_augments=2):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augments):\n",
    "        augmentation_choice = random.choice(['synonym', 'insertion', 'deletion', 'back_translation'])\n",
    "        if augmentation_choice == 'synonym':\n",
    "            augmented_texts.append(synonym_replacement(text))\n",
    "        elif augmentation_choice == 'insertion':\n",
    "            augmented_texts.append(random_insertion(text))\n",
    "        elif augmentation_choice == 'deletion':\n",
    "            augmented_texts.append(random_deletion(text))\n",
    "        elif augmentation_choice == 'back_translation':\n",
    "            augmented_texts.append(back_translation(text))\n",
    "    return augmented_texts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507e1faa1a6e9329",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def augment_binary_data(df, label_column, augment_text, num_augments=2):\n",
    "    \"\"\"\n",
    "    Augments binary classification data to balance class distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'Utterances' column and a binary label column.\n",
    "    - label_column (str): Column name of the binary target label (0 or 1).\n",
    "    - augment_text (callable): Function to augment text. Should take a string and return a list of augmented strings.\n",
    "    - num_augments (int): Number of augmented samples to generate per original sample.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Augmented DataFrame with balanced class distribution.\n",
    "    \"\"\"\n",
    "    # Oblicz liczność klas\n",
    "    class_counts = df[label_column].value_counts()\n",
    "    min_class, max_class = class_counts.idxmin(), class_counts.idxmax()\n",
    "    num_min, num_max = class_counts[min_class], class_counts[max_class]\n",
    "\n",
    "    print(f\"Liczność klas przed augmentacją: {class_counts.to_dict()}\")\n",
    "\n",
    "    # Pobierz próbki z mniejszej klasy\n",
    "    class_subset = df[df[label_column] == min_class].copy()\n",
    "\n",
    "    # Oblicz ile dodatkowych próbek potrzebujemy\n",
    "    num_to_add = num_max - num_min\n",
    "\n",
    "    # Inicjalizacja nowego zbioru danych\n",
    "    augmented_data = {'Utterances': [], label_column: []}\n",
    "\n",
    "    # Augmentuj dane, ale tylko do momentu wyrównania liczby próbek\n",
    "    augment_per_sample = max(1, num_to_add // len(class_subset))  # Ile augmentacji na 1 próbkę\n",
    "    remaining = num_to_add  # Ile jeszcze próbek musimy dodać\n",
    "\n",
    "    for _, row in tqdm(class_subset.iterrows(), total=len(class_subset), desc=f\"Augmenting class {min_class}\"):\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "\n",
    "        # Wykonaj augmentację tekstu\n",
    "        new_texts = augment_text(row['Utterances'], num_augments=min(augment_per_sample, remaining))\n",
    "\n",
    "        for new_text in new_texts:\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            augmented_data['Utterances'].append(new_text)\n",
    "            augmented_data[label_column].append(min_class)\n",
    "            remaining -= 1  # Zmniejsz licznik brakujących próbek\n",
    "\n",
    "    # Tworzenie DataFrame z nowymi próbkami\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "    # Połączenie oryginalnych danych z nowymi danymi\n",
    "    final_df = pd.concat([df, augmented_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Sprawdź finalny rozkład klas\n",
    "    final_counts = final_df[label_column].value_counts()\n",
    "    print(f\"Liczność klas po augmentacji: {final_counts.to_dict()}\")\n",
    "\n",
    "    return final_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf74b5425e90d5a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def augment_binary_data_percent(df, label_column, augment_text, augment_percent=25):\n",
    "    \"\"\"\n",
    "    Augments binary classification data by a specified percentage.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'Utterances' column and a binary label column.\n",
    "    - label_column (str): Column name of the binary target label (0 or 1).\n",
    "    - augment_text (callable): Function to augment text. Should take a string and return a list of augmented strings.\n",
    "    - augment_percent (int): Percentage increase for each class (e.g., 25 means adding 25% more samples per class).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Augmented DataFrame with increased class distributions.\n",
    "    \"\"\"\n",
    "    # Oblicz liczność klas\n",
    "    class_counts = df[label_column].value_counts()\n",
    "    print(f\"Liczność klas przed augmentacją: {class_counts.to_dict()}\")\n",
    "\n",
    "    # Inicjalizacja nowego zbioru danych\n",
    "    augmented_data = {'Utterances': [], label_column: []}\n",
    "\n",
    "    for label in class_counts.index:\n",
    "        class_subset = df[df[label_column] == label].copy()\n",
    "        num_to_add = int(class_counts[label] * (augment_percent / 100))\n",
    "\n",
    "        augment_per_sample = max(1, num_to_add // len(class_subset))\n",
    "        remaining = num_to_add\n",
    "\n",
    "        for _, row in tqdm(class_subset.iterrows(), total=len(class_subset), desc=f\"Augmenting class {label}\"):\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "\n",
    "            new_texts = augment_text(row['Utterances'], num_augments=min(augment_per_sample, remaining))\n",
    "\n",
    "            for new_text in new_texts:\n",
    "                if remaining <= 0:\n",
    "                    break\n",
    "                augmented_data['Utterances'].append(new_text)\n",
    "                augmented_data[label_column].append(label)\n",
    "                remaining -= 1\n",
    "\n",
    "    # Tworzenie DataFrame z nowymi próbkami\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "    # Połączenie oryginalnych danych z nowymi danymi\n",
    "    final_df = pd.concat([df, augmented_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Sprawdź finalny rozkład klas\n",
    "    final_counts = final_df[label_column].value_counts()\n",
    "    print(f\"Liczność klas po augmentacji: {final_counts.to_dict()}\")\n",
    "\n",
    "    return final_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "120d3b0d5958947f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Augmentation\n",
    "augmented_df = augment_binary_data(\n",
    "    df=df,\n",
    "    label_column='label',\n",
    "    augment_text=augment_text,\n",
    "    num_augments=2\n",
    ")\n",
    "\n",
    "print(augmented_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "230e9a2bfc05843e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = augment_binary_data_percent(\n",
    "    df=augmented_df,\n",
    "    label_column='label',\n",
    "    augment_text=augment_text,\n",
    "    augment_percent=70\n",
    ")\n",
    "\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc800c4ee0aa2eb0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_frequencies = df['label'].value_counts()\n",
    "label_frequencies_percent = df['label'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "656eec943b8ae25b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c053cc2ea6cbe64f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Original train size: {df_data.shape}\")\n",
    "print(f\"Validation size: {df_valid.shape}, Test size: {df_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d69bb5cab8227c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6313a5510286cb08",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_frequencies = df['label'].value_counts()\n",
    "label_frequencies_percent = df['label'].value_counts(normalize=True) * 100\n",
    "print(label_frequencies_percent)\n",
    "print(label_frequencies)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7478b534ccd10119",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_distribution = df['label'].value_counts(normalize=True)\n",
    "print(class_distribution)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7064823e84ebfa8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Sentiment Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73c13e7f14210a32",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target_list = list(df.columns)\n",
    "target_list = target_list[1:]\n",
    "target_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "981cb9e10ae1a9a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        self.targets = self.df['label'].astype(int).values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            #'token_type_ids': inputs[\"token_type_ids\"].flatten(), -> nie potrzebne przy RoBERTa\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),\n",
    "            'utterances': utterances\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e8c28009bdd96a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class BERTBinarySentimentClassificationClass(nn.Module):\n",
    "#     def __init__(self, bert_model):\n",
    "#         super(BERTBinarySentimentClassificationClass, self).__init__()\n",
    "#         self.bert = bert_model\n",
    "#         self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "#         self.out = nn.Linear(self.bert.config.hidden_size, 1)  # Binary classification (1 output)\n",
    "# \n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "#         # Forward pass przez BERT\n",
    "#         outputs = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "#         pooled_output = outputs.pooler_output\n",
    "#         dropout_output = self.dropout(pooled_output)\n",
    "#         return self.out(dropout_output)\n",
    "#         #return torch.sigmoid(self.out(dropout_output))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e94d6131ac088b5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class RoBERTaBinarySentimentClassificationClass(nn.Module):\n",
    "#     def __init__(self, roberta_model):\n",
    "#         super(RoBERTaBinarySentimentClassificationClass, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "#         self.out = nn.Linear(self.roberta.config.hidden_size, 1)  # Binary classification\n",
    "# \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Forward pass przez RoBERTa\n",
    "#         outputs = self.roberta(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         # Sprawdzenie, czy pooler_output jest dostępne\n",
    "#         if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "#             pooled_output = outputs.pooler_output  # Preferowane, jeśli dostępne\n",
    "#         else:\n",
    "#             pooled_output = outputs.last_hidden_state[:, 0, :]  # Wykorzystanie tokena [CLS]\n",
    "# \n",
    "#         dropout_output = self.dropout(pooled_output)\n",
    "#         return self.out(dropout_output)\n",
    "#         # return torch.sigmoid(self.out(dropout_output))  # Jeśli używasz BCEWithLogitsLoss, sigmoid nie jest potrzebny"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0a40b7814203672",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BERTLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, lstm_hidden_dim=128, lstm_layers=1, dropout_rate=DROPOUT_RATE, bidirectional=True):\n",
    "        super(BERTLSTMClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.bert.config.hidden_size,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if lstm_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # Jeśli LSTM jest dwukierunkowe, rozmiar wejścia do fc to lstm_hidden_dim*2, w przeciwnym wypadku lstm_hidden_dim\n",
    "        fc_input_dim = lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # Pobranie reprezentacji z BERTa (cała sekwencja)\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # last_hidden_state: tensor o wymiarach (batch_size, seq_length, hidden_size)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "\n",
    "        # Przepuszczenie przez LSTM\n",
    "        lstm_output, (hidden, _) = self.lstm(sequence_output)\n",
    "        # Jeśli LSTM jest dwukierunkowe, ukryty stan z ostatniej warstwy ma wymiar (num_layers*2, batch, hidden_dim)\n",
    "        if self.lstm.bidirectional:\n",
    "            # Pobieramy ostatnie stany z obu kierunków i je łączymy\n",
    "            hidden_forward = hidden[-2, :, :]  # ostatnia warstwa, kierunek \"forward\"\n",
    "            hidden_backward = hidden[-1, :, :]  # ostatnia warstwa, kierunek \"backward\"\n",
    "            hidden = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "\n",
    "        dropout_output = self.dropout(hidden)\n",
    "        logits = self.fc(dropout_output)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2082ab52f98981f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#model_path = 'best_model_state.bin'\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#model = BERTBinarySentimentClassificationClass(bert_model)\n",
    "model = BERTLSTMClassifier(bert_model)\n",
    "\n",
    "# roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "# model = RoBERTaBinarySentimentClassificationClass(roberta_model)\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5256b71691e1c46",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aadf862ae0047947",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH, shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de1b1918cc1209d9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "outputs = model(data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e2ccb1ee90e217c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                  max_length = 50,\n",
    "                                  truncation = True,\n",
    "                                  padding = \"max_length\",\n",
    "                                  return_attention_mask = True,\n",
    "                                  return_tensors = \"pt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1b009a33dc77303",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#bert_model = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encodings['input_ids'],\n",
    "    attention_mask=encodings['attention_mask']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bec695a35b6a3f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def loss_fn(outputs, targets):\n",
    "#     # jesli uzywamy BCEWithLogitsLoss, to w modelu nie musimy dodawać sigmoid, bo ta funkcja już zawiera operację sigmoid.\n",
    "#     return torch.nn.BCEWithLogitsLoss()(outputs.squeeze(-1), targets.float())\n",
    "\n",
    "# def loss_fn(outputs, targets):\n",
    "#     return torch.nn.BCELoss()(outputs.squeeze(), targets.float())\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs powinny być wyjściem z modelu (logity)\n",
    "        bce_loss = self.bce(inputs, targets.float())\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        # Obliczenie p_t: dla próbki z target=1 mamy probas, dla target=0 mamy 1 - probas\n",
    "        p_t = targets * probas + (1 - targets) * (1 - probas)\n",
    "        loss = self.alpha * (1 - p_t) ** self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "loss_fn = FocalLoss(alpha=4, gamma=2, reduction='mean')\n",
    "#gamma\n",
    "#Odpowiada za redukowanie wagi łatwo sklasyfikowanych przykładów. Wyższa wartość gamma sprawia, że model skupia się bardziej na trudnych przypadkach.\n",
    "#Jeśli gamma jest zbyt wysoka (np. 2 lub więcej), może to powodować, że model zaniedbuje uczenie się na przykładach, które są łatwiejsze do sklasyfikowania, co może negatywnie wpływać na ogólną stabilność treningu.\n",
    "#W twoim przypadku, skoro klasa 1 nie jest przewidywana, warto spróbować obniżyć wartość gamma (np. do 1), aby nie tłumić gradientów zbyt mocno.\n",
    "\n",
    "#alpha\n",
    "#Umożliwia balansowanie klas poprzez przypisanie większej wagi przykładom z klasy, która jest niedoreprezentowana lub trudniejsza do nauki.\n",
    "# W twoim kodzie alpha=1 oznacza, że wszystkie próbki są traktowane jednakowo. Jeśli obserwujesz, że jedna klasa (tu klasa 1) jest pomijana, możesz spróbować nadać jej większą wagę.\n",
    "# Możesz rozważyć modyfikację alpha na wektor, np. alpha = [0.25, 0.75] lub inne wartości, w zależności od nierównowagi między klasami. To pozwoli nadać przykładom z klasy 1 większy wpływ na stratę.\n",
    "\n",
    "\n",
    "#Zmodyfikuj implementację, aby alpha było wektorem wag (np. alpha = [waga_klasy0, waga_klasy1]), co pozwoli na precyzyjne balansowanie strat między klasami.\n",
    "#Przeprowadź serię eksperymentów (np. grid search) z różnymi wartościami gamma i alpha, aby znaleźć najlepszą kombinację, która poprawi metryki takie jak F1-score, szczególnie dla klasy 1.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77a7cac8c06c5ae1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "# learning rate scheduler\n",
    "# ReduceLROnPlateau może obniżać lr do bardzo małych wartości, co czasem prowadzi do problemów. Możesz dodać min_lr, aby ograniczyć ten efekt:\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=MODE, patience=PATIENCE, factor=FACTOR, verbose=VERBOSE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea368685bd23d290",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(training_loader, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trenuje model na danych treningowych i zwraca model, dokładność, średni loss oraz F1-score.\n",
    "\n",
    "    Args:\n",
    "        training_loader (DataLoader): DataLoader z danymi treningowymi.\n",
    "        model (torch.nn.Module): Model do trenowania.\n",
    "        optimizer (torch.optim.Optimizer): Optymalizator do aktualizacji wag modelu.\n",
    "        loss_fn (callable): Funkcja strat, np. nn.BCEWithLogitsLoss.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): Wytrenowany model.\n",
    "        train_accuracy (float): Dokładność modelu na zbiorze treningowym.\n",
    "        avg_loss (float): Średnia wartość funkcji strat.\n",
    "        train_f1 (float): F1-score (binary) na zbiorze treningowym.\n",
    "    \"\"\"\n",
    "    # Inicjalizacja zmiennych do śledzenia wyników\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), leave=True, colour='steelblue')\n",
    "\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        #token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.float)  # Binary targets jako float\n",
    "\n",
    "        #outputs = model(ids, mask, token_type_ids if 'token_type_ids' in data else None)\n",
    "        outputs = model(ids, mask if 'token_type_ids' in data else None)\n",
    "\n",
    "        outputs = outputs.squeeze(-1)  # Dopasowanie wymiarów do binary classification (1D)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        preds = torch.sigmoid(outputs) >= 0.5  # Sigmoid + progowanie przy 0.5\n",
    "        correct_predictions += torch.sum(preds == targets).item()\n",
    "        num_samples += targets.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    train_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return model, correct_predictions / num_samples, np.mean(losses), train_f1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b856ca62d79ec79c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, epoch):\n",
    "    \"\"\"\n",
    "    Ewaluacja modelu na danych walidacyjnych.\n",
    "\n",
    "    Args:\n",
    "        validation_loader (DataLoader): DataLoader z danymi walidacyjnymi.\n",
    "        model (torch.nn.Module): Model do oceny.\n",
    "        loss_fn (callable): Funkcja strat, np. nn.BCEWithLogitsLoss.\n",
    "        epoch (int): Aktualny numer epoki do logowania w TensorBoard.\n",
    "\n",
    "    Returns:\n",
    "        val_accuracy (float): Dokładność modelu na zbiorze walidacyjnym.\n",
    "        avg_loss (float): Średnia wartość funkcji strat na zbiorze walidacyjnym.\n",
    "        val_f1 (float): F1-score (binary) na zbiorze walidacyjnym.\n",
    "    \"\"\"\n",
    "    # Inicjalizacja zmiennych do śledzenia wyników\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Ustaw model w tryb ewaluacyjny\n",
    "    model.eval()\n",
    "\n",
    "    # Wyłącz gradienty dla ewaluacji\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            #token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.float)  # Binary targets jako float\n",
    "\n",
    "            #outputs = model(ids, mask, token_type_ids if 'token_type_ids' in data else None)\n",
    "            outputs = model(ids, mask if 'token_type_ids' in data else None)\n",
    "\n",
    "            outputs = outputs.squeeze(-1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds = torch.sigmoid(outputs) >= 0.5  # Sigmoid + progowanie przy 0.5\n",
    "            correct_predictions += torch.sum(preds == targets).item()\n",
    "            num_samples += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    writer.add_scalar('Loss/validation', avg_loss, epoch)\n",
    "    writer.add_scalar('F1-Score/validation', val_f1, epoch)\n",
    "\n",
    "    return correct_predictions / num_samples, avg_loss, val_f1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85ae992f90f3f334",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "\n",
    "    model, train_acc, train_loss, train_f1 = train_model(train_data_loader, model, optimizer)\n",
    "    print(f'Train loss {train_loss:.4f} | Train accuracy {train_acc:.4f} | Train F1 {train_f1:.4f}')\n",
    "\n",
    "    val_acc, val_loss, val_f1 = eval_model(val_data_loader, model, epoch)\n",
    "    print(f'Val loss {val_loss:.4f} | Val accuracy {val_acc:.4f} | Val F1 {val_f1:.4f}')\n",
    "\n",
    "    # Logowanie metryk do TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('F1-Score/train', train_f1, epoch)\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    # Sprawdzenie najlepszej F1 i zapisanie modelu\n",
    "    if val_f1 > best_f1:\n",
    "        torch.save(model.state_dict(), \"best_binary_model_state.bin\")\n",
    "        best_f1 = val_f1\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    #scheduler.step(val_loss)  # Tuning LR\n",
    "    scheduler.step()\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "134691e19bd73c9a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e1afc6cd3268ae0f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transfer learning to ESConv dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f952fdde39a28c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "#dataset = load_data(\"D:/julixus/MEISD/meisd_project/data/ESConv.json\")\n",
    "dataset = load_data(\"C:/Users/juwieczo/DataspellProjects/meisd_project/data/ESConv.json\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e74cfffe2519a329",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_seeker_data(data, key):\n",
    "    result = []\n",
    "\n",
    "    for entry in data:\n",
    "        dialog = entry['dialog']\n",
    "        seeker_dialog = [item['content'].strip() for item in dialog if item['speaker'] == 'seeker']\n",
    "\n",
    "        quarter_length = max(1, len(seeker_dialog) // 4)\n",
    "\n",
    "        if key == 'initial_emotion_intensity':\n",
    "            selected_dialog = seeker_dialog[:quarter_length]\n",
    "        elif key == 'final_emotion_intensity':\n",
    "            selected_dialog = seeker_dialog[-quarter_length:]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        result.append({\n",
    "            key: entry['survey_score']['seeker'][key],\n",
    "            'dialog': selected_dialog\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n",
    "first_25_percent = extract_seeker_data(dataset, 'initial_emotion_intensity')\n",
    "#last_25_percent = extract_seeker_data(dataset, 'final_emotion_intensity')\n",
    "\n",
    "first_25_df = pd.DataFrame(first_25_percent)\n",
    "#last_25_df = pd.DataFrame(last_25_percent)\n",
    "\n",
    "first_25_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2a3e0a1d7d37ee0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_counts = first_25_df['initial_emotion_intensity'].value_counts()\n",
    "least_common_label = label_counts.idxmin()\n",
    "first_25_df = first_25_df[first_25_df['initial_emotion_intensity'] != least_common_label]\n",
    "first_25_df['initial_emotion_intensity'] = pd.to_numeric(first_25_df['initial_emotion_intensity'], errors='coerce')\n",
    "first_25_df['initial_emotion_intensity'] = first_25_df['initial_emotion_intensity'] - 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fb5f7b4ca2c53a3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "first_25_df.rename(columns={\n",
    "    'dialog': 'Utterances',\n",
    "    'initial_emotion_intensity': 'label'\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5038c8d54db5f67",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data['label'] = (df_data['max_intensity'] == 2).astype(int)\n",
    "columns = ['Utterances', 'label']\n",
    "df = df_data[columns].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3d3ba2d4b569246",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH, shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6476534e8c7bbcd1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_model(data_loader, model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            #outputs = model(input_ids=input_ids)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, true_labels\n",
    "\n",
    "# Run validation\n",
    "predictions, true_labels = test_model(val_data_loader, model)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91b2b2fe84a323a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dc2acc0f4d5644c0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17fc756d71b62fbb",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
