{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:31:29.732586Z",
     "start_time": "2024-11-06T15:31:24.294447Z"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 30  #50 #128  # wiekszosc tokenow zdaje sie byc ponizej 40, klasycznie wklada sie tu 256, my przystaniemy na 30\n",
    "TRAIN_BATCH_SIZE = 32  #8 #16 #32 \n",
    "#Czasami, przy bardzo niskim tempie uczenia i zbyt dużych batchach, model może wolniej konwergować. Spróbuj zmniejszyć wielkość batcha, np. z 16 do 8.\n",
    "VALID_BATCH_SIZE = 32  #8 #16 #32\n",
    "TEST_BATCH_SIZE = 32  #8 #16 #32\n",
    "EPOCHS = 10\n",
    "#LEARNING_RATE = 1e-05  #1e-05\n",
    "THRESHOLD = 0.5  # threshold for the sigmoid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:31:32.867886Z",
     "start_time": "2024-11-06T15:31:32.865116Z"
    }
   },
   "id": "cff131745b17f957",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('multi_label_binarizer_MEISD.csv')\n",
    "df_data.head()\n",
    "# For the multilabel classification we use:\n",
    "columns = ['Utterances', 'sentiment_0', 'sentiment_1', 'sentiment_2']\n",
    "multi_columns = df_data[columns].copy()\n",
    "multi_columns\n",
    "df_data['label'] = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].idxmax(axis=1)\n",
    "df_data['label'] = df_data['label'].apply(lambda x: int(x.split('_')[1]))\n",
    "df_data = df_data[['Utterances', 'label']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:31:35.003774Z",
     "start_time": "2024-11-06T15:31:34.963033Z"
    }
   },
   "id": "8eee9b8144b4fc18",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                                  max_length=50,\n",
    "                                  truncation=True,\n",
    "                                  padding=\"max_length\",\n",
    "                                  return_attention_mask=True,\n",
    "                                  return_tensors=\"pt\")\n",
    "token_lens = []\n",
    "\n",
    "for txt in df_data['Utterances']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:31:38.522671Z",
     "start_time": "2024-11-06T15:31:35.962977Z"
    }
   },
   "id": "51989e954638f219",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_data, random_state=77, test_size=0.30, shuffle=True)\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)\n",
    "\n",
    "columns = multi_columns.columns\n",
    "\n",
    "categor_freq = multi_columns[columns[1:]].sum() / multi_columns.shape[0]\n",
    "categor_freq\n",
    "class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        # Upewnij się, że etykiety są typu całkowitego (int)\n",
    "        self.targets = self.df['label'].astype(int).values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterances = str(self.utterances[index])  # 'index' jest prawidłowe\n",
    "        #utterances = \" \".join(utterances.split())  # Usuwa niepotrzebne białe znaki\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            utterances,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.tensor(self.targets[index], dtype=torch.long)  # Zapewnij typ long\n",
    "        # print(f\"Target dtype: {target.dtype}\")  # Debugging\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),  # Zapewnij typ long\n",
    "            'utterances': utterances\n",
    "        }\n",
    "\n",
    "\n",
    "target_list = list(df_data.columns)\n",
    "target_list = target_list[1:]\n",
    "\n",
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # 0 means no parallel loading\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,  # Validation data should not be shuffled\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "data = next(iter(train_data_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:31:39.680286Z",
     "start_time": "2024-11-06T15:31:39.651248Z"
    }
   },
   "id": "a33d5328af4cea03",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encodings['input_ids'],\n",
    "    attention_mask=encodings['attention_mask']\n",
    ")\n",
    "bert_model.config.hidden_size\n",
    "\n",
    "class BERTSentimentClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSentimentClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        self.linear = torch.nn.Linear(self.bert_model.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = output.pooler_output  # Corrected here\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n",
    "\n",
    "model = BERTSentimentClass()\n",
    "\n",
    "# # Freezing BERT layers:\n",
    "#for name, param in model.bert_model.named_parameters():\n",
    "#    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "#        param.requires_grad = True\n",
    "#    else:\n",
    "#        param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:32:08.400494Z",
     "start_time": "2024-11-06T15:32:06.771072Z"
    }
   },
   "id": "8fe81a783afb6479",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "total_samples = sum(class_distribution)\n",
    "class_weights = [total_samples / count for count in class_distribution]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:32:11.913288Z",
     "start_time": "2024-11-06T15:32:11.467269Z"
    }
   },
   "id": "38f832cc7bf8e009",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 16:32:16,646\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "2024-11-06 16:32:20,110\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-11-06 16:32:20,112\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ray.cloudpickle.dumps(<class 'ray.tune.trainable.function_trainable.wrap_function.<locals>.ImplicitFunc'>) failed.\nTo check which non-serializable variables are captured in scope, re-run the ray script with 'RAY_PICKLE_VERBOSE_DEBUG=1'. Other options: \n-Try reproducing the issue by calling `pickle.dumps(trainable)`. \n-If the error is typing-related, try removing the type annotations and try again.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 78\u001B[0m\n\u001B[0;32m     70\u001B[0m         writer\u001B[38;5;241m.\u001B[39madd_scalar(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy/val\u001B[39m\u001B[38;5;124m'\u001B[39m, val_accuracy, epoch)\n\u001B[0;32m     73\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m: tune\u001B[38;5;241m.\u001B[39mloguniform(\u001B[38;5;241m1e-5\u001B[39m, \u001B[38;5;241m1e-1\u001B[39m),\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: tune\u001B[38;5;241m.\u001B[39mchoice([\u001B[38;5;241m16\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m64\u001B[39m]),\n\u001B[0;32m     76\u001B[0m }\n\u001B[1;32m---> 78\u001B[0m analysis \u001B[38;5;241m=\u001B[39m tune\u001B[38;5;241m.\u001B[39mrun(train_model_with_validation, config\u001B[38;5;241m=\u001B[39mconfig)\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\ray\\tune\\tune.py:758\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001B[0m\n\u001B[0;32m    756\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, exp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(experiments):\n\u001B[0;32m    757\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exp, Experiment):\n\u001B[1;32m--> 758\u001B[0m         experiments[i] \u001B[38;5;241m=\u001B[39m Experiment(\n\u001B[0;32m    759\u001B[0m             name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m    760\u001B[0m             run\u001B[38;5;241m=\u001B[39mexp,\n\u001B[0;32m    761\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    762\u001B[0m             time_budget_s\u001B[38;5;241m=\u001B[39mtime_budget_s,\n\u001B[0;32m    763\u001B[0m             config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[0;32m    764\u001B[0m             resources_per_trial\u001B[38;5;241m=\u001B[39mresources_per_trial,\n\u001B[0;32m    765\u001B[0m             num_samples\u001B[38;5;241m=\u001B[39mnum_samples,\n\u001B[0;32m    766\u001B[0m             storage_path\u001B[38;5;241m=\u001B[39mstorage_path,\n\u001B[0;32m    767\u001B[0m             storage_filesystem\u001B[38;5;241m=\u001B[39mstorage_filesystem,\n\u001B[0;32m    768\u001B[0m             sync_config\u001B[38;5;241m=\u001B[39msync_config,\n\u001B[0;32m    769\u001B[0m             checkpoint_config\u001B[38;5;241m=\u001B[39mcheckpoint_config,\n\u001B[0;32m    770\u001B[0m             trial_name_creator\u001B[38;5;241m=\u001B[39mtrial_name_creator,\n\u001B[0;32m    771\u001B[0m             trial_dirname_creator\u001B[38;5;241m=\u001B[39mtrial_dirname_creator,\n\u001B[0;32m    772\u001B[0m             log_to_file\u001B[38;5;241m=\u001B[39mlog_to_file,\n\u001B[0;32m    773\u001B[0m             export_formats\u001B[38;5;241m=\u001B[39mexport_formats,\n\u001B[0;32m    774\u001B[0m             max_failures\u001B[38;5;241m=\u001B[39mmax_failures,\n\u001B[0;32m    775\u001B[0m             restore\u001B[38;5;241m=\u001B[39mrestore,\n\u001B[0;32m    776\u001B[0m         )\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fail_fast \u001B[38;5;129;01mand\u001B[39;00m max_failures \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_failures must be 0 if fail_fast=True.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\ray\\tune\\experiment\\experiment.py:149\u001B[0m, in \u001B[0;36mExperiment.__init__\u001B[1;34m(self, name, run, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, sync_config, checkpoint_config, trial_name_creator, trial_dirname_creator, log_to_file, export_formats, max_failures, restore, local_dir)\u001B[0m\n\u001B[0;32m    141\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    142\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcheckpoint_frequency\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m cannot be set for a function trainable. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    143\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou will need to report a checkpoint every \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    146\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto get this behavior.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    147\u001B[0m         )\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_identifier \u001B[38;5;241m=\u001B[39m Experiment\u001B[38;5;241m.\u001B[39mregister_if_needed(run)\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RpcError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mrpc_code \u001B[38;5;241m==\u001B[39m ray\u001B[38;5;241m.\u001B[39m_raylet\u001B[38;5;241m.\u001B[39mGRPC_STATUS_CODE_RESOURCE_EXHAUSTED:\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\ray\\tune\\experiment\\experiment.py:360\u001B[0m, in \u001B[0;36mExperiment.register_if_needed\u001B[1;34m(cls, run_object)\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, PicklingError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    353\u001B[0m     extra_msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    354\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOther options: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    355\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-Try reproducing the issue by calling \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    358\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe type annotations and try again.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    359\u001B[0m     )\n\u001B[1;32m--> 360\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(e)(\u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m extra_msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m name\n",
      "\u001B[1;31mTypeError\u001B[0m: ray.cloudpickle.dumps(<class 'ray.tune.trainable.function_trainable.wrap_function.<locals>.ImplicitFunc'>) failed.\nTo check which non-serializable variables are captured in scope, re-run the ray script with 'RAY_PICKLE_VERBOSE_DEBUG=1'. Other options: \n-Try reproducing the issue by calling `pickle.dumps(trainable)`. \n-If the error is typing-related, try removing the type annotations and try again."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_model(model, val_data_loader):\n",
    "    model.eval()  # Przełącz model w tryb oceny\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Wyłącz gradienty podczas oceny\n",
    "        for batch in val_data_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(inputs, attn_mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Uzyskaj przewidywania\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_val_loss, accuracy\n",
    "\n",
    "# Funkcja treningowa z dodaną walidacją\n",
    "def train_model_with_validation(config):\n",
    "    learning_rate = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    model = BERTSentimentClass()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attn_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Ocena na danych walidacyjnych\n",
    "        val_loss, val_accuracy = eval_model(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Raportowanie do Ray Tune\n",
    "        tune.report(loss=running_loss/len(train_loader), val_loss=val_loss, val_accuracy=val_accuracy)\n",
    "\n",
    "        # Zapis wyników do TensorBoard\n",
    "        writer.add_scalar('Loss/train', running_loss/len(train_loader), epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "}\n",
    "\n",
    "analysis = tune.run(train_model_with_validation, config=config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:32:20.409635Z",
     "start_time": "2024-11-06T15:32:14.388093Z"
    }
   },
   "id": "1bd991665d234b1f",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)  # Uruchom Ray\n",
    "analysis = tune.run(\n",
    "    train_model,\n",
    "    config=config,\n",
    "    num_samples=10,  # Liczba eksperymentów\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d51437c3cec2bef0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Best config: \", analysis.get_best_config(metric=\"loss\", mode=\"min\"))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fad262c85466249d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Best trial final validation loss: {analysis.best_trial.last_result['loss']}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cb85f83e742b8d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66162e052d9e8ef3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
