{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:15.966123Z",
     "start_time": "2024-11-06T17:29:10.534779Z"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 30  #50 #128  # wiekszosc tokenow zdaje sie byc ponizej 40, klasycznie wklada sie tu 256, my przystaniemy na 30\n",
    "TRAIN_BATCH_SIZE = 32  #8 #16 #32 \n",
    "#Czasami, przy bardzo niskim tempie uczenia i zbyt dużych batchach, model może wolniej konwergować. Spróbuj zmniejszyć wielkość batcha, np. z 16 do 8.\n",
    "VALID_BATCH_SIZE = 32  #8 #16 #32\n",
    "TEST_BATCH_SIZE = 32  #8 #16 #32\n",
    "EPOCHS = 10\n",
    "#LEARNING_RATE = 1e-05  #1e-05\n",
    "THRESHOLD = 0.5  # threshold for the sigmoid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:17.754057Z",
     "start_time": "2024-11-06T17:29:17.751267Z"
    }
   },
   "id": "cff131745b17f957",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('multi_label_binarizer_MEISD.csv')\n",
    "df_data.head()\n",
    "# For the multilabel classification we use:\n",
    "columns = ['Utterances', 'sentiment_0', 'sentiment_1', 'sentiment_2']\n",
    "multi_columns = df_data[columns].copy()\n",
    "multi_columns\n",
    "df_data['label'] = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].idxmax(axis=1)\n",
    "df_data['label'] = df_data['label'].apply(lambda x: int(x.split('_')[1]))\n",
    "df_data = df_data[['Utterances', 'label']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:18.272270Z",
     "start_time": "2024-11-06T17:29:18.233030Z"
    }
   },
   "id": "8eee9b8144b4fc18",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterances    0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_data.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:18.687396Z",
     "start_time": "2024-11-06T17:29:18.682982Z"
    }
   },
   "id": "60ec74242c2b5a33",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "if tokenizer is None:\n",
    "    raise ValueError(\"Failed to load tokenizer. Ensure the model name is correct and Hugging Face's transformers library is properly installed.\")\n",
    "\n",
    "\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "encodings = tokenizer.encode_plus(test_text,\n",
    "                                  add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                                  max_length=50,\n",
    "                                  truncation=True,\n",
    "                                  padding=\"max_length\",\n",
    "                                  return_attention_mask=True,\n",
    "                                  return_tensors=\"pt\")\n",
    "token_lens = []\n",
    "\n",
    "for txt in df_data['Utterances']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:22.207617Z",
     "start_time": "2024-11-06T17:29:19.151051Z"
    }
   },
   "id": "51989e954638f219",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_data, random_state=77, test_size=0.30, shuffle=True)\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)\n",
    "\n",
    "columns = multi_columns.columns\n",
    "\n",
    "categor_freq = multi_columns[columns[1:]].sum() / multi_columns.shape[0]\n",
    "categor_freq\n",
    "class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.utterances = list(df['Utterances'])\n",
    "        self.targets = self.df['label'].astype(int).values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterance = self.utterances[index]\n",
    "\n",
    "        # Convert the utterance to a string to avoid any non-string types\n",
    "        utterance = str(utterance)\n",
    "\n",
    "        try:\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                utterance,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization error at index {index} for utterance: '{utterance}'\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            raise\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "target_list = list(df_data.columns)\n",
    "target_list = target_list[1:]\n",
    "\n",
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # 0 means no parallel loading\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,  # Validation data should not be shuffled\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "data = next(iter(train_data_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:22.227468Z",
     "start_time": "2024-11-06T17:29:22.208623Z"
    }
   },
   "id": "a33d5328af4cea03",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encodings['input_ids'],\n",
    "    attention_mask=encodings['attention_mask']\n",
    ")\n",
    "bert_model.config.hidden_size\n",
    "\n",
    "class BERTSentimentClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSentimentClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        self.linear = torch.nn.Linear(self.bert_model.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = output.pooler_output  # Corrected here\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n",
    "\n",
    "model = BERTSentimentClass()\n",
    "\n",
    "# # Freezing BERT layers:\n",
    "#for name, param in model.bert_model.named_parameters():\n",
    "#    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "#        param.requires_grad = True\n",
    "#    else:\n",
    "#        param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:23.507415Z",
     "start_time": "2024-11-06T17:29:22.228475Z"
    }
   },
   "id": "8fe81a783afb6479",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class_distribution = multi_columns[['sentiment_0', 'sentiment_1', 'sentiment_2']].sum()\n",
    "total_samples = sum(class_distribution)\n",
    "class_weights = [total_samples / count for count in class_distribution]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:24.289336Z",
     "start_time": "2024-11-06T17:29:23.508448Z"
    }
   },
   "id": "38f832cc7bf8e009",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(model, val_data_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_data_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(inputs, attn_mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_val_loss, accuracy\n",
    "\n",
    "@ray.remote\n",
    "def train_model_with_validation(config):\n",
    "    global train_loader, val_loader  # Use the pre-defined loaders\n",
    "    learning_rate = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    model = BERTSentimentClass().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attn_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Evaluate on validation data\n",
    "        val_loss, val_accuracy = eval_model(model, val_loader)\n",
    "\n",
    "        # Log using Ray Tune\n",
    "        tune.report(loss=running_loss/len(train_loader), val_loss=val_loss, val_accuracy=val_accuracy)\n",
    "\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:29:24.296343Z",
     "start_time": "2024-11-06T17:29:24.290344Z"
    }
   },
   "id": "1753e82b8a2c0f1c",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 18:30:55,284\tINFO worker.py:1816 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "DeprecationWarning",
     "evalue": "The `local_dir` argument is deprecated. You should set the `storage_path` instead. See the docs: https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#setting-the-local-staging-directory",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mDeprecationWarning\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m ray\u001B[38;5;241m.\u001B[39minit(ignore_reinit_error\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, log_to_driver\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, _temp_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:/julixus/MEISD/meisd_project/ray_temp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m analysis \u001B[38;5;241m=\u001B[39m tune\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m      4\u001B[0m     tune\u001B[38;5;241m.\u001B[39mwith_parameters(train_model_with_validation, train_loader\u001B[38;5;241m=\u001B[39mtrain_data_loader, val_loader\u001B[38;5;241m=\u001B[39mval_data_loader),\n\u001B[0;32m      5\u001B[0m     config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[0;32m      6\u001B[0m     num_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,  \u001B[38;5;66;03m# Liczba eksperymentów\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     resources_per_trial\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m},\n\u001B[0;32m      8\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBERT_Sentiment_Experiment\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      9\u001B[0m     local_dir\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD:/julixus/MEISD/meisd_project/ray_tune\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     10\u001B[0m )\n",
      "File \u001B[1;32mD:\\conda\\Lib\\site-packages\\ray\\tune\\tune.py:587\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001B[0m\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m(\n\u001B[0;32m    583\u001B[0m         ENV_VAR_DEPRECATION_MESSAGE\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRAY_AIR_LOCAL_CACHE_DIR\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    584\u001B[0m     )\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m local_dir \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 587\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m(\n\u001B[0;32m    588\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe `local_dir` argument is deprecated. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    589\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou should set the `storage_path` instead. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    590\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee the docs: https://docs.ray.io/en/latest/train/user-guides/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    591\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpersistent-storage.html#setting-the-local-staging-directory\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    592\u001B[0m     )\n\u001B[0;32m    594\u001B[0m ray\u001B[38;5;241m.\u001B[39m_private\u001B[38;5;241m.\u001B[39musage\u001B[38;5;241m.\u001B[39musage_lib\u001B[38;5;241m.\u001B[39mrecord_library_usage(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtune\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    596\u001B[0m \u001B[38;5;66;03m# Tracking environment variable usage here will also catch:\u001B[39;00m\n\u001B[0;32m    597\u001B[0m \u001B[38;5;66;03m# 1.) Tuner.fit() usage\u001B[39;00m\n\u001B[0;32m    598\u001B[0m \u001B[38;5;66;03m# 2.) Trainer.fit() usage\u001B[39;00m\n\u001B[0;32m    599\u001B[0m \u001B[38;5;66;03m# 3.) Ray client usage (env variables are inherited by the Ray runtime env)\u001B[39;00m\n",
      "\u001B[1;31mDeprecationWarning\u001B[0m: The `local_dir` argument is deprecated. You should set the `storage_path` instead. See the docs: https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#setting-the-local-staging-directory"
     ]
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True, log_to_driver=True, _temp_dir=\"D:/julixus/MEISD/meisd_project/ray_temp\")\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(train_model_with_validation, train_loader=train_data_loader, val_loader=val_data_loader),\n",
    "    config=config,\n",
    "    num_samples=1,  # Liczba eksperymentów\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    name=\"BERT_Sentiment_Experiment\",\n",
    "    local_dir= 'D:/julixus/MEISD/meisd_project/ray_tune'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:30:57.970257Z",
     "start_time": "2024-11-06T17:30:53.170186Z"
    }
   },
   "id": "d51437c3cec2bef0",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest config: \u001B[39m\u001B[38;5;124m\"\u001B[39m, analysis\u001B[38;5;241m.\u001B[39mget_best_config(metric\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Best config: \", analysis.get_best_config(metric=\"loss\", mode=\"min\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:30:23.576773Z",
     "start_time": "2024-11-06T17:30:23.562438Z"
    }
   },
   "id": "fad262c85466249d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Best trial final validation loss: {analysis.best_trial.last_result['loss']}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cb85f83e742b8d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66162e052d9e8ef3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
